{
    "docs": [
        {
            "location": "/", 
            "text": "Optim.jl\n\n\nUnivariate and multivariate optimization in Julia.\n\n\nOptim.jl is part of the \nJuliaNLSolvers\n family.\n\n\n\n\n\n\n\n\nSource\n\n\nPackageEvaluator\n\n\nBuild Status\n\n\nSocial\n\n\nReferences to cite\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat\n\n\nOptim is a Julia package for optimizing functions of various kinds. While there is some support for box constrained and Riemannian optimization, most of the solvers try to find an $x$ that minimizes a function $f(x)$ without any constraints. Thus, the main focus is on unconstrained optimization. The provided solvers, under certain conditions, will converge to a local minimum. In the case where a global minimum is desired, global optimization techniques should be employed instead (see e.g. \nBlackBoxOptim\n).\n\n\n\n\nWhy\n\n\nThere are many solvers available from both free and commercial sources, and many of them are accessible from Julia. Few of them are written in Julia. Performance-wise this is rarely a problem, as they are often written in either Fortran or C. However, solvers written directly in Julia does come with some advantages.\n\n\nWhen writing Julia software (packages) that require something to be optimized, the programmer can either choose to write their own optimization routine, or use one of the many available solvers. For example, this could be something from the \nNLOpt\n suite. This means adding a dependency which is not written in Julia, and more assumptions have to be made as to the environment the user is in. Does the user have the proper compilers? Is it possible to use GPL'ed code in the project? Optim is released under the MIT license, and installation is a simple \nPkg.add\n, so it really doesn't get much freer, easier, and lightweight than that.\n\n\nIt is also true, that using a solver written in C or Fortran makes it impossible to leverage one of the main benefits of Julia: multiple dispatch. Since Optim is entirely written in Julia, we can currently use the dispatch system to ease the use of custom preconditioners. A planned feature along these lines is to allow for user controlled choice of solvers for various steps in the algorithm, entirely based on dispatch, and not predefined possibilities chosen by the developers of Optim.\n\n\nBeing a Julia package also means that Optim has access to the automatic differentiation features through the packages in \nJuliaDiff\n.\n\n\n\n\nHow\n\n\nOptim is registered in \nMETADATA.jl\n. This means that all you need to do to install Optim, is to run\n\n\nPkg.add(\nOptim\n)", 
            "title": "Home"
        }, 
        {
            "location": "/#optimjl", 
            "text": "Univariate and multivariate optimization in Julia.  Optim.jl is part of the  JuliaNLSolvers  family.     Source  PackageEvaluator  Build Status  Social  References to cite", 
            "title": "Optim.jl"
        }, 
        {
            "location": "/#what", 
            "text": "Optim is a Julia package for optimizing functions of various kinds. While there is some support for box constrained and Riemannian optimization, most of the solvers try to find an $x$ that minimizes a function $f(x)$ without any constraints. Thus, the main focus is on unconstrained optimization. The provided solvers, under certain conditions, will converge to a local minimum. In the case where a global minimum is desired, global optimization techniques should be employed instead (see e.g.  BlackBoxOptim ).", 
            "title": "What"
        }, 
        {
            "location": "/#why", 
            "text": "There are many solvers available from both free and commercial sources, and many of them are accessible from Julia. Few of them are written in Julia. Performance-wise this is rarely a problem, as they are often written in either Fortran or C. However, solvers written directly in Julia does come with some advantages.  When writing Julia software (packages) that require something to be optimized, the programmer can either choose to write their own optimization routine, or use one of the many available solvers. For example, this could be something from the  NLOpt  suite. This means adding a dependency which is not written in Julia, and more assumptions have to be made as to the environment the user is in. Does the user have the proper compilers? Is it possible to use GPL'ed code in the project? Optim is released under the MIT license, and installation is a simple  Pkg.add , so it really doesn't get much freer, easier, and lightweight than that.  It is also true, that using a solver written in C or Fortran makes it impossible to leverage one of the main benefits of Julia: multiple dispatch. Since Optim is entirely written in Julia, we can currently use the dispatch system to ease the use of custom preconditioners. A planned feature along these lines is to allow for user controlled choice of solvers for various steps in the algorithm, entirely based on dispatch, and not predefined possibilities chosen by the developers of Optim.  Being a Julia package also means that Optim has access to the automatic differentiation features through the packages in  JuliaDiff .", 
            "title": "Why"
        }, 
        {
            "location": "/#how", 
            "text": "Optim is registered in  METADATA.jl . This means that all you need to do to install Optim, is to run  Pkg.add( Optim )", 
            "title": "How"
        }, 
        {
            "location": "/user/minimization/", 
            "text": "Unconstrained Optimization\n\n\nTo show how the Optim package can be used, we minimize the \nRosenbrock function\n, a classical test problem for numerical optimization. We'll assume that you've already installed the Optim package using Julia's package manager. First, we load Optim and define the Rosenbrock function:\n\n\nusing Optim\nf(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n\n\n\n\nOnce we've defined this function, we can find the minimizer (the input that minimizes the objective) and the minimum (the value of the objective at the minimizer) using any of our favorite optimization algorithms. With a function defined, we just specify an initial point \nx\n and call \noptimize\n with a starting point \nx0\n:\n\n\nx0 = [0.0, 0.0]\noptimize(f, x0)\n\n\n\n\nNote\n: it is important to pass \ninitial_x\n as an array. If your problem is one-dimensional, you have to wrap it in an array. An easy way to do so is to write \noptimize(x-\nf(first(x)), [initial_x])\n which make sure the input is an array, but the anonymous function automatically passes the first (and only) element onto your given \nf\n.\n\n\nOptim will default to using the Nelder-Mead method in the multivariate case, as we did not provide a gradient. This can also be explicitly specified using:\n\n\noptimize(f, x0, NelderMead())\n\n\n\n\nOther solvers are available. Below, we use L-BFGS, a quasi-Newton method that requires a gradient. If we pass \nf\n alone, Optim will construct an approximate gradient for us using central finite differencing:\n\n\noptimize(f, x0, LBFGS())\n\n\n\n\nFor better performance and greater precision, you can pass your own gradient function. If your objective is written in all Julia code with no special calls to external (that is non-Julia) libraries, you can also use automatic differentiation, by using the \nautodiff\n keyword and setting it to \n:forward\n:\n\n\noptimize(f, x0, LBFGS(); autodiff = :forward)\n\n\n\n\nFor the Rosenbrock example, the analytical gradient can be shown to be:\n\n\nfunction g!(G, x)\nG[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\nG[2] = 200.0 * (x[2] - x[1]^2)\nend\n\n\n\n\nNote, that the functions we're using to calculate the gradient (and later the Hessian \nh!\n) of the Rosenbrock function mutate a fixed-sized storage array, which is passed as an additional argument called \nG\n (or \nH\n for the Hessian) in these examples. By mutating a single array over many iterations, this style of function definition removes the sometimes considerable costs associated with allocating a new array during each call to the \ng!\n or \nh!\n functions. If you prefer to have your gradients simply accept an \nx\n, you can still use \noptimize\n by setting the \ninplace\n keyword to \nfalse\n:\n\n\noptimize(f, g, x0; inplace = false)\n\n\n\n\nwhere \ng\n is a function of \nx\n only.\n\n\nReturning to our in-place version, you simply pass \ng!\n together with \nf\n from before to use the gradient:\n\n\noptimize(f, g!, x0, LBFGS())\n\n\n\n\nFor some methods, like simulated annealing, the gradient will be ignored:\n\n\noptimize(f, g!, x0, SimulatedAnnealing())\n\n\n\n\nIn addition to providing gradients, you can provide a Hessian function \nh!\n as well. In our current case this is:\n\n\nfunction h!(H, x)\n    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n    H[1, 2] = -400.0 * x[1]\n    H[2, 1] = -400.0 * x[1]\n    H[2, 2] = 200.0\nend\n\n\n\n\nNow we can use Newton's method for optimization by running:\n\n\noptimize(f, g!, h!, x0)\n\n\n\n\nWhich defaults to \nNewton()\n since a Hessian function was provided. Like gradients, the Hessian function will be ignored if you use a method that does not require it:\n\n\noptimize(f, g!, h!, x0, LBFGS())\n\n\n\n\nNote that Optim will not generate approximate Hessians using finite differencing because of the potentially low accuracy of approximations to the Hessians. Other than Newton's method, none of the algorithms provided by the Optim package employ exact Hessians.\n\n\n\n\nBox Constrained Optimization\n\n\nA primal interior-point algorithm for simple \"box\" constraints (lower and upper bounds) is available. Reusing our Rosenbrock example from above, boxed minimization is performed as follows:\n\n\nlower = [1.25, -2.1]\nupper = [Inf, Inf]\ninitial_x = [2.0, 2.0]\ninner_optimizer = GradientDescent()\nresults = optimize(f, g!, lower, upper, initial_x, Fminbox(inner_optimizer))\n\n\n\n\nThis performs optimization with a barrier penalty, successively scaling down the barrier coefficient and using the chosen \ninner_optimizer\n (\nGradientDescent()\n above) for convergence at each step. To change algorithm specific options, such as the line search algorithm, specify it directly in the \ninner_optimizer\n constructor:\n\n\nlower = [1.25, -2.1]\nupper = [Inf, Inf]\ninitial_x = [2.0, 2.0]\n# requires using LineSearches\ninner_optimizer = GradientDescent(linesearch=LineSearches.BackTracking(order=3))\nresults = optimize(f, g!, lower, upper, initial_x, Fminbox(inner_optimizer))\n\n\n\n\nThis algorithm uses diagonal preconditioning to improve the accuracy, and hence is a good example of how to use \nConjugateGradient\n or \nLBFGS\n with preconditioning. Other methods will currently not use preconditioning. Only the box constraints are used. If you can analytically compute the diagonal of the Hessian of your objective function, you may want to consider writing your own preconditioner.\n\n\nThere are two iterations parameters: an outer iterations parameter used to control \nFminbox\n and an inner iterations parameter used to control the inner optimizer. For example, the following restricts the optimization to 2 major iterations\n\n\nresults = optimize(f, g!, lower, upper, initial_x, Fminbox(GradientDescent()), Optim.Options(outer_iterations = 2))\n\n\n\n\nIn contrast, the following sets the maximum number of iterations for each \nConjugateGradient()\n optimization to 2\n\n\nresults = optimize(f, g!, lower, upper, initial_x, Fminbox(GradientDescent()), Optim.Options(iterations = 2))\n\n\n\n\n\n\nMinimizing a univariate function on a bounded interval\n\n\nMinimization of univariate functions without derivatives is available through the \noptimize\n interface:\n\n\noptimize(f, lower, upper, method; kwargs...)\n\n\n\n\nNotice the lack of initial \nx\n. A specific example is the following quadratic function.\n\n\njulia\n f_univariate(x) = 2x^2+3x+1\nf_univariate (generic function with 1 method)\n\njulia\n optimize(f_univariate, -2.0, 1.0)\nResults of Optimization Algorithm\n * Algorithm: Brent's Method\n * Search Interval: [-2.000000, 1.000000]\n * Minimizer: -7.500000e-01\n * Minimum: -1.250000e-01\n * Iterations: 7\n * Convergence: max(|x - x_upper|, |x - x_lower|) \n= 2*(1.5e-08*|x|+2.2e-16): true\n * Objective Function Calls: 8\n\n\n\n\nThe output shows that we provided an initial lower and upper bound, that there is a final minimizer and minimum, and that it used seven major iterations. Importantly, we also see that convergence was declared. The default method is Brent's method, which is one out of two available methods:\n\n\n\n\nBrent's method, the default (can be explicitly selected with \nBrent()\n).\n\n\nGolden section search, available with \nGoldenSection()\n.\n\n\n\n\nIf we want to manually specify this method, we use the usual syntax as for multivariate optimization.\n\n\n    optimize(f, lower, upper, Brent(); kwargs...)\n    optimize(f, lower, upper, GoldenSection(); kwargs...)\n\n\n\n\nKeywords are used to set options for this special type of optimization. In addition to the \niterations\n, \nstore_trace\n, \nshow_trace\n and \nextended_trace\n options, the following options are also available:\n\n\n\n\nrel_tol\n: The relative tolerance used for determining convergence. Defaults to \nsqrt(eps(T))\n.\n\n\nabs_tol\n: The absolute tolerance used for determining convergence. Defaults to \neps(T)\n.\n\n\n\n\n\n\nObtaining results\n\n\nAfter we have our results in \nres\n, we can use the API for getting optimization results. This consists of a collection of functions. They are not exported, so they have to be prefixed by \nOptim.\n. Say we do the following optimization:\n\n\nres = optimize(x-\ndot(x,[1 0. 0; 0 3 0; 0 0 1]*x), zeros(3))\n\n\n\n\nIf we can't remember what method we used, we simply use\n\n\nsummary(res)\n\n\n\n\nwhich will return \n\"Nelder Mead\"\n. A bit more useful information is the minimizer and minimum of the objective functions, which can be found using\n\n\njulia\n Optim.minimizer(res)\n3-element Array{Float64,1}:\n -0.499921\n -0.3333\n -1.49994\n\njulia\n Optim.minimum(res)\n -2.8333333205768865\n\n\n\n\n\n\nComplete list of functions\n\n\nA complete list of functions can be found below.\n\n\nDefined for all methods:\n\n\n\n\nsummary(res)\n\n\nminimizer(res)\n\n\nminimum(res)\n\n\niterations(res)\n\n\niteration_limit_reached(res)\n\n\ntrace(res)\n\n\nx_trace(res)\n\n\nf_trace(res)\n\n\nf_calls(res)\n\n\nconverged(res)\n\n\n\n\nDefined for univariate optimization:\n\n\n\n\nlower_bound(res)\n\n\nupper_bound(res)\n\n\nx_lower_trace(res)\n\n\nx_upper_trace(res)\n\n\nrel_tol(res)\n\n\nabs_tol(res)\n\n\n\n\nDefined for multivariate optimization:\n\n\n\n\ng_norm_trace(res)\n\n\ng_calls(res)\n\n\nx_converged(res)\n\n\nf_converged(res)\n\n\ng_converged(res)\n\n\ninitial_state(res)\n\n\n\n\n\n\nInput types\n\n\nMost users will input \nVector\n's as their \ninitial_x\n's, and get an \nOptim.minimizer(res)\n out that is also a vector. For zeroth and first order methods, it is also possible to pass in matrices, or even higher dimensional arrays. The only restriction imposed by leaving the \nVector\n case is, that it is no longer possible to use finite difference approximations or automatic differentiation. Second order methods (variants of Newton's method) do not support this more general input type.\n\n\n\n\nNotes on convergence flags and checks\n\n\nCurrently, it is possible to access a minimizer using \nOptim.minimizer(result)\n even if all convergence flags are \nfalse\n. This means that the user has to be a bit careful when using the output from the solvers. It is advised to include checks for convergence if the minimizer or minimum is used to carry out further calculations.\n\n\nA related note is that first and second order methods makes a convergence check on the gradient before entering the optimization loop. This is done to prevent line search errors if \ninitial_x\n is a stationary point. Notice, that this is only a first order check. If \ninitial_x\n is any type of stationary point, \ng_converged\n will be true. This includes local minima, saddle points, and local maxima. If \niterations\n is \n0\n and \ng_converged\n is \ntrue\n, the user needs to keep this point in mind.", 
            "title": "Minimizing a function"
        }, 
        {
            "location": "/user/minimization/#unconstrained-optimization", 
            "text": "To show how the Optim package can be used, we minimize the  Rosenbrock function , a classical test problem for numerical optimization. We'll assume that you've already installed the Optim package using Julia's package manager. First, we load Optim and define the Rosenbrock function:  using Optim\nf(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2  Once we've defined this function, we can find the minimizer (the input that minimizes the objective) and the minimum (the value of the objective at the minimizer) using any of our favorite optimization algorithms. With a function defined, we just specify an initial point  x  and call  optimize  with a starting point  x0 :  x0 = [0.0, 0.0]\noptimize(f, x0)  Note : it is important to pass  initial_x  as an array. If your problem is one-dimensional, you have to wrap it in an array. An easy way to do so is to write  optimize(x- f(first(x)), [initial_x])  which make sure the input is an array, but the anonymous function automatically passes the first (and only) element onto your given  f .  Optim will default to using the Nelder-Mead method in the multivariate case, as we did not provide a gradient. This can also be explicitly specified using:  optimize(f, x0, NelderMead())  Other solvers are available. Below, we use L-BFGS, a quasi-Newton method that requires a gradient. If we pass  f  alone, Optim will construct an approximate gradient for us using central finite differencing:  optimize(f, x0, LBFGS())  For better performance and greater precision, you can pass your own gradient function. If your objective is written in all Julia code with no special calls to external (that is non-Julia) libraries, you can also use automatic differentiation, by using the  autodiff  keyword and setting it to  :forward :  optimize(f, x0, LBFGS(); autodiff = :forward)  For the Rosenbrock example, the analytical gradient can be shown to be:  function g!(G, x)\nG[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\nG[2] = 200.0 * (x[2] - x[1]^2)\nend  Note, that the functions we're using to calculate the gradient (and later the Hessian  h! ) of the Rosenbrock function mutate a fixed-sized storage array, which is passed as an additional argument called  G  (or  H  for the Hessian) in these examples. By mutating a single array over many iterations, this style of function definition removes the sometimes considerable costs associated with allocating a new array during each call to the  g!  or  h!  functions. If you prefer to have your gradients simply accept an  x , you can still use  optimize  by setting the  inplace  keyword to  false :  optimize(f, g, x0; inplace = false)  where  g  is a function of  x  only.  Returning to our in-place version, you simply pass  g!  together with  f  from before to use the gradient:  optimize(f, g!, x0, LBFGS())  For some methods, like simulated annealing, the gradient will be ignored:  optimize(f, g!, x0, SimulatedAnnealing())  In addition to providing gradients, you can provide a Hessian function  h!  as well. In our current case this is:  function h!(H, x)\n    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n    H[1, 2] = -400.0 * x[1]\n    H[2, 1] = -400.0 * x[1]\n    H[2, 2] = 200.0\nend  Now we can use Newton's method for optimization by running:  optimize(f, g!, h!, x0)  Which defaults to  Newton()  since a Hessian function was provided. Like gradients, the Hessian function will be ignored if you use a method that does not require it:  optimize(f, g!, h!, x0, LBFGS())  Note that Optim will not generate approximate Hessians using finite differencing because of the potentially low accuracy of approximations to the Hessians. Other than Newton's method, none of the algorithms provided by the Optim package employ exact Hessians.", 
            "title": "Unconstrained Optimization"
        }, 
        {
            "location": "/user/minimization/#box-constrained-optimization", 
            "text": "A primal interior-point algorithm for simple \"box\" constraints (lower and upper bounds) is available. Reusing our Rosenbrock example from above, boxed minimization is performed as follows:  lower = [1.25, -2.1]\nupper = [Inf, Inf]\ninitial_x = [2.0, 2.0]\ninner_optimizer = GradientDescent()\nresults = optimize(f, g!, lower, upper, initial_x, Fminbox(inner_optimizer))  This performs optimization with a barrier penalty, successively scaling down the barrier coefficient and using the chosen  inner_optimizer  ( GradientDescent()  above) for convergence at each step. To change algorithm specific options, such as the line search algorithm, specify it directly in the  inner_optimizer  constructor:  lower = [1.25, -2.1]\nupper = [Inf, Inf]\ninitial_x = [2.0, 2.0]\n# requires using LineSearches\ninner_optimizer = GradientDescent(linesearch=LineSearches.BackTracking(order=3))\nresults = optimize(f, g!, lower, upper, initial_x, Fminbox(inner_optimizer))  This algorithm uses diagonal preconditioning to improve the accuracy, and hence is a good example of how to use  ConjugateGradient  or  LBFGS  with preconditioning. Other methods will currently not use preconditioning. Only the box constraints are used. If you can analytically compute the diagonal of the Hessian of your objective function, you may want to consider writing your own preconditioner.  There are two iterations parameters: an outer iterations parameter used to control  Fminbox  and an inner iterations parameter used to control the inner optimizer. For example, the following restricts the optimization to 2 major iterations  results = optimize(f, g!, lower, upper, initial_x, Fminbox(GradientDescent()), Optim.Options(outer_iterations = 2))  In contrast, the following sets the maximum number of iterations for each  ConjugateGradient()  optimization to 2  results = optimize(f, g!, lower, upper, initial_x, Fminbox(GradientDescent()), Optim.Options(iterations = 2))", 
            "title": "Box Constrained Optimization"
        }, 
        {
            "location": "/user/minimization/#minimizing-a-univariate-function-on-a-bounded-interval", 
            "text": "Minimization of univariate functions without derivatives is available through the  optimize  interface:  optimize(f, lower, upper, method; kwargs...)  Notice the lack of initial  x . A specific example is the following quadratic function.  julia  f_univariate(x) = 2x^2+3x+1\nf_univariate (generic function with 1 method)\n\njulia  optimize(f_univariate, -2.0, 1.0)\nResults of Optimization Algorithm\n * Algorithm: Brent's Method\n * Search Interval: [-2.000000, 1.000000]\n * Minimizer: -7.500000e-01\n * Minimum: -1.250000e-01\n * Iterations: 7\n * Convergence: max(|x - x_upper|, |x - x_lower|)  = 2*(1.5e-08*|x|+2.2e-16): true\n * Objective Function Calls: 8  The output shows that we provided an initial lower and upper bound, that there is a final minimizer and minimum, and that it used seven major iterations. Importantly, we also see that convergence was declared. The default method is Brent's method, which is one out of two available methods:   Brent's method, the default (can be explicitly selected with  Brent() ).  Golden section search, available with  GoldenSection() .   If we want to manually specify this method, we use the usual syntax as for multivariate optimization.      optimize(f, lower, upper, Brent(); kwargs...)\n    optimize(f, lower, upper, GoldenSection(); kwargs...)  Keywords are used to set options for this special type of optimization. In addition to the  iterations ,  store_trace ,  show_trace  and  extended_trace  options, the following options are also available:   rel_tol : The relative tolerance used for determining convergence. Defaults to  sqrt(eps(T)) .  abs_tol : The absolute tolerance used for determining convergence. Defaults to  eps(T) .", 
            "title": "Minimizing a univariate function on a bounded interval"
        }, 
        {
            "location": "/user/minimization/#obtaining-results", 
            "text": "After we have our results in  res , we can use the API for getting optimization results. This consists of a collection of functions. They are not exported, so they have to be prefixed by  Optim. . Say we do the following optimization:  res = optimize(x- dot(x,[1 0. 0; 0 3 0; 0 0 1]*x), zeros(3))  If we can't remember what method we used, we simply use  summary(res)  which will return  \"Nelder Mead\" . A bit more useful information is the minimizer and minimum of the objective functions, which can be found using  julia  Optim.minimizer(res)\n3-element Array{Float64,1}:\n -0.499921\n -0.3333\n -1.49994\n\njulia  Optim.minimum(res)\n -2.8333333205768865", 
            "title": "Obtaining results"
        }, 
        {
            "location": "/user/minimization/#complete-list-of-functions", 
            "text": "A complete list of functions can be found below.  Defined for all methods:   summary(res)  minimizer(res)  minimum(res)  iterations(res)  iteration_limit_reached(res)  trace(res)  x_trace(res)  f_trace(res)  f_calls(res)  converged(res)   Defined for univariate optimization:   lower_bound(res)  upper_bound(res)  x_lower_trace(res)  x_upper_trace(res)  rel_tol(res)  abs_tol(res)   Defined for multivariate optimization:   g_norm_trace(res)  g_calls(res)  x_converged(res)  f_converged(res)  g_converged(res)  initial_state(res)", 
            "title": "Complete list of functions"
        }, 
        {
            "location": "/user/minimization/#input-types", 
            "text": "Most users will input  Vector 's as their  initial_x 's, and get an  Optim.minimizer(res)  out that is also a vector. For zeroth and first order methods, it is also possible to pass in matrices, or even higher dimensional arrays. The only restriction imposed by leaving the  Vector  case is, that it is no longer possible to use finite difference approximations or automatic differentiation. Second order methods (variants of Newton's method) do not support this more general input type.", 
            "title": "Input types"
        }, 
        {
            "location": "/user/minimization/#notes-on-convergence-flags-and-checks", 
            "text": "Currently, it is possible to access a minimizer using  Optim.minimizer(result)  even if all convergence flags are  false . This means that the user has to be a bit careful when using the output from the solvers. It is advised to include checks for convergence if the minimizer or minimum is used to carry out further calculations.  A related note is that first and second order methods makes a convergence check on the gradient before entering the optimization loop. This is done to prevent line search errors if  initial_x  is a stationary point. Notice, that this is only a first order check. If  initial_x  is any type of stationary point,  g_converged  will be true. This includes local minima, saddle points, and local maxima. If  iterations  is  0  and  g_converged  is  true , the user needs to keep this point in mind.", 
            "title": "Notes on convergence flags and checks"
        }, 
        {
            "location": "/user/gradientsandhessians/", 
            "text": "Gradients and Hessians\n\n\nTo use first- and second-order methods, you need to provide gradients and Hessians, either in-place or out-of-place. There are three main ways of specifying derivatives: analytic, finite-difference and automatic differentiation.\n\n\n\n\nAnalytic\n\n\nThis results in the fastest run times, but requires the user to perform the often tedious task of computing the derivatives by hand. The gradient of complicated objective functions (e.g. involving the solution of algebraic equations, differential equations, eigendecompositions, etc.) can be computed efficiently using the adjoint method (see e.g. \nthese lecture notes\n). In particular, assuming infinite memory, the gradient of a $\\mathbb{R}^N \\to \\mathbb{R}$ function $f$ can always be computed with a runtime comparable with only one evaluation of $f$, no matter how large $N$.\n\n\nTo use analytic derivatives, simply pass \ng!\n and \nh!\n functions to \noptimize\n.\n\n\n\n\nFinite differences\n\n\nThis uses the functionality in \nDiffEqDiffTools.jl\n to compute gradients and Hessians through central finite differences: $f'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h}$. For a $\\mathbb{R}^N \\to \\mathbb{R}$ objective function $f$, this requires $2N$ evaluations of $f$. It is therefore efficient in low dimensions but slow when $N$ is large. It is also inaccurate: $h$ is chosen equal to $\\epsilon^{1/3}$ where $\\epsilon$ is the machine epsilon (about $10^{-16}$ for \nFloat64\n) to balance the truncation and rounding errors, resulting in an error of $\\epsilon^{2/3}$ (about $10^{-11}$ for \nFloat64\n) for the derivative.\n\n\nFinite differences are on by default if gradients and Hessians are not supplied to the \noptimize\n call.\n\n\n\n\nAutomatic differentiation\n\n\nAutomatic differentiation techniques are a middle ground between finite differences and analytic computations. They are exact up to machine precision, and do not require intervention from the user. They come in two main flavors: \nforward and reverse mode\n. Forward-mode automatic differentiation is relatively straightforward to implement by propagating the sensitivities of the input variables, and is often faster than finite differences. The disadvantage is that the objective function has to be written using only Julia code. Forward-mode automatic differentiation still requires a runtime comparable to $N$ evaluations of $f$, and is therefore costly in large dimensions, like finite differences.\n\n\nReverse-mode automatic differentiation can be seen as an automatic implementation of the adjoint method mentioned above, and requires a runtime comparable to only one evaluation of $f$. It is however considerably more complex to implement, requiring to record the execution of the program to then run it backwards, and incurs a larger overhead.\n\n\nForward-mode automatic differentiation is supported through the \nForwardDiff.jl\n package by providing the \nautodiff=:forward\n keyword to \noptimize\n. Reverse-mode automatic differentiation is not supported explicitly yet (although you can use it by writing your own \ng!\n function). There are a number of implementations in Julia, such as \nReverseDiff.jl\n.\n\n\n\n\nExample\n\n\nLet us consider the Rosenbrock example again.\n\n\nfunction f(x)\n    return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\nend\n\nfunction g!(G, x)\n    G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n    G[2] = 200.0 * (x[2] - x[1]^2)\nend\n\nfunction h!(H, x)\n    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n    H[1, 2] = -400.0 * x[1]\n    H[2, 1] = -400.0 * x[1]\n    H[2, 2] = 200.0\nend\n\ninitial_x = zeros(2)\n\n\n\n\nLet us see if BFGS and Newton's Method can solve this problem with the functions provided.\n\n\njulia\n Optim.minimizer(optimize(f, g!, h!, initial_x, BFGS()))\n2-element Array{Float64,1}:\n 1.0\n 1.0\n\njulia\n Optim.minimizer(optimize(f, g!, h!, initial_x, Newton()))\n\n2-element Array{Float64,1}:\n 1.0\n 1.0\n\n\n\n\nThis is indeed the case. Now let us use finite differences for BFGS.\n\n\njulia\n Optim.minimizer(optimize(f, initial_x, BFGS()))\n2-element Array{Float64,1}:\n 1.0\n 1.0\n\n\n\n\nStill looks good. Returning to automatic differentiation, let us try both solvers using this method.  We enable \nforward mode\n automatic differentiation by using the \nautodiff = :forward\n keyword.\n\n\njulia\n Optim.minimizer(optimize(f, initial_x, BFGS(); autodiff = :forward))\n2-element Array{Float64,1}:\n 1.0\n 1.0\n\njulia\n Optim.minimizer(optimize(f, initial_x, Newton(); autodiff = :forward))\n2-element Array{Float64,1}:\n 1.0\n 1.0\n\n\n\n\nIndeed, the minimizer was found, without providing any gradients or Hessians.", 
            "title": "Gradients and Hessians"
        }, 
        {
            "location": "/user/gradientsandhessians/#gradients-and-hessians", 
            "text": "To use first- and second-order methods, you need to provide gradients and Hessians, either in-place or out-of-place. There are three main ways of specifying derivatives: analytic, finite-difference and automatic differentiation.", 
            "title": "Gradients and Hessians"
        }, 
        {
            "location": "/user/gradientsandhessians/#analytic", 
            "text": "This results in the fastest run times, but requires the user to perform the often tedious task of computing the derivatives by hand. The gradient of complicated objective functions (e.g. involving the solution of algebraic equations, differential equations, eigendecompositions, etc.) can be computed efficiently using the adjoint method (see e.g.  these lecture notes ). In particular, assuming infinite memory, the gradient of a $\\mathbb{R}^N \\to \\mathbb{R}$ function $f$ can always be computed with a runtime comparable with only one evaluation of $f$, no matter how large $N$.  To use analytic derivatives, simply pass  g!  and  h!  functions to  optimize .", 
            "title": "Analytic"
        }, 
        {
            "location": "/user/gradientsandhessians/#finite-differences", 
            "text": "This uses the functionality in  DiffEqDiffTools.jl  to compute gradients and Hessians through central finite differences: $f'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h}$. For a $\\mathbb{R}^N \\to \\mathbb{R}$ objective function $f$, this requires $2N$ evaluations of $f$. It is therefore efficient in low dimensions but slow when $N$ is large. It is also inaccurate: $h$ is chosen equal to $\\epsilon^{1/3}$ where $\\epsilon$ is the machine epsilon (about $10^{-16}$ for  Float64 ) to balance the truncation and rounding errors, resulting in an error of $\\epsilon^{2/3}$ (about $10^{-11}$ for  Float64 ) for the derivative.  Finite differences are on by default if gradients and Hessians are not supplied to the  optimize  call.", 
            "title": "Finite differences"
        }, 
        {
            "location": "/user/gradientsandhessians/#automatic-differentiation", 
            "text": "Automatic differentiation techniques are a middle ground between finite differences and analytic computations. They are exact up to machine precision, and do not require intervention from the user. They come in two main flavors:  forward and reverse mode . Forward-mode automatic differentiation is relatively straightforward to implement by propagating the sensitivities of the input variables, and is often faster than finite differences. The disadvantage is that the objective function has to be written using only Julia code. Forward-mode automatic differentiation still requires a runtime comparable to $N$ evaluations of $f$, and is therefore costly in large dimensions, like finite differences.  Reverse-mode automatic differentiation can be seen as an automatic implementation of the adjoint method mentioned above, and requires a runtime comparable to only one evaluation of $f$. It is however considerably more complex to implement, requiring to record the execution of the program to then run it backwards, and incurs a larger overhead.  Forward-mode automatic differentiation is supported through the  ForwardDiff.jl  package by providing the  autodiff=:forward  keyword to  optimize . Reverse-mode automatic differentiation is not supported explicitly yet (although you can use it by writing your own  g!  function). There are a number of implementations in Julia, such as  ReverseDiff.jl .", 
            "title": "Automatic differentiation"
        }, 
        {
            "location": "/user/gradientsandhessians/#example", 
            "text": "Let us consider the Rosenbrock example again.  function f(x)\n    return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\nend\n\nfunction g!(G, x)\n    G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n    G[2] = 200.0 * (x[2] - x[1]^2)\nend\n\nfunction h!(H, x)\n    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\n    H[1, 2] = -400.0 * x[1]\n    H[2, 1] = -400.0 * x[1]\n    H[2, 2] = 200.0\nend\n\ninitial_x = zeros(2)  Let us see if BFGS and Newton's Method can solve this problem with the functions provided.  julia  Optim.minimizer(optimize(f, g!, h!, initial_x, BFGS()))\n2-element Array{Float64,1}:\n 1.0\n 1.0\n\njulia  Optim.minimizer(optimize(f, g!, h!, initial_x, Newton()))\n\n2-element Array{Float64,1}:\n 1.0\n 1.0  This is indeed the case. Now let us use finite differences for BFGS.  julia  Optim.minimizer(optimize(f, initial_x, BFGS()))\n2-element Array{Float64,1}:\n 1.0\n 1.0  Still looks good. Returning to automatic differentiation, let us try both solvers using this method.  We enable  forward mode  automatic differentiation by using the  autodiff = :forward  keyword.  julia  Optim.minimizer(optimize(f, initial_x, BFGS(); autodiff = :forward))\n2-element Array{Float64,1}:\n 1.0\n 1.0\n\njulia  Optim.minimizer(optimize(f, initial_x, Newton(); autodiff = :forward))\n2-element Array{Float64,1}:\n 1.0\n 1.0  Indeed, the minimizer was found, without providing any gradients or Hessians.", 
            "title": "Example"
        }, 
        {
            "location": "/user/config/", 
            "text": "Configurable options\n\n\nThere are several options that simply take on some default values if the user doensn't supply anything else than a function (and gradient) and a starting point.\n\n\n\n\nSolver options\n\n\nThere quite a few different solvers available in Optim, and they are all listed below. Notice that the constructors are written without input here, but they generally take keywords to tweak the way they work. See the pages describing each solver for more detail.\n\n\nRequires only a function handle:\n\n\n\n\nNelderMead()\n\n\nSimulatedAnnealing()\n\n\n\n\nRequires a function and gradient (will be approximated if omitted):\n\n\n\n\nBFGS()\n\n\nLBFGS()\n\n\nConjugateGradient()\n\n\nGradientDescent()\n\n\nMomentumGradientDescent()\n\n\nAcceleratedGradientDescent()\n\n\n\n\nRequires a function, a gradient, and a Hessian (cannot be omitted):\n\n\n\n\nNewton()\n\n\nNewtonTrustRegion()\n\n\n\n\nBox constrained minimization:\n\n\n\n\nFminbox()\n\n\n\n\nSpecial methods for bounded univariate optimization:\n\n\n\n\nBrent()\n\n\nGoldenSection()\n\n\n\n\n\n\nGeneral Options\n\n\nIn addition to the solver, you can alter the behavior of the Optim package by using the following keywords:\n\n\n\n\nx_tol\n: Absolute tolerance in changes of the input vector \nx\n, in infinity norm. Defaults to \n0.0\n.\n\n\nf_tol\n: Relative tolerance in changes of the objective value. Defaults to \n0.0\n.\n\n\ng_tol\n: Absolute tolerance in the gradient, in infinity norm. Defaults to \n1e-8\n. For gradient free methods, this will control the main convergence tolerance, which is solver specific.\n\n\nf_calls_limit\n: A soft upper limit on the number of objective calls. Defaults to \n0\n (unlimited).\n\n\ng_calls_limit\n: A soft upper limit on the number of gradient calls. Defaults to \n0\n (unlimited).\n\n\nh_calls_limit\n: A soft upper limit on the number of Hessian calls. Defaults to \n0\n (unlimited).\n\n\nallow_f_increases\n: Allow steps that increase the objective value. Defaults to \nfalse\n. Note that, when setting this to \ntrue\n, the last iterate will be returned as the minimizer even if the objective increased.\n\n\niterations\n: How many iterations will run before the algorithm gives up? Defaults to \n1_000\n.\n\n\nstore_trace\n: Should a trace of the optimization algorithm's state be stored? Defaults to \nfalse\n.\n\n\nshow_trace\n: Should a trace of the optimization algorithm's state be shown on \nstdout\n? Defaults to \nfalse\n.\n\n\nextended_trace\n: Save additional information. Solver dependent. Defaults to \nfalse\n.\n\n\nshow_every\n: Trace output is printed every \nshow_every\nth iteration.\n\n\ncallback\n: A function to be called during tracing. A return value of \ntrue\n stops the \noptimize\n call.\n\n\ntime_limit\n: A soft upper limit on the total run time. Defaults to \nNaN\n (unlimited).\n\n\n\n\nWe currently recommend the statically dispatched interface by using the \nOptim.Options\n constructor:\n\n\nres = optimize(f, g!,\n               [0.0, 0.0],\n               GradientDescent(),\n               Optim.Options(g_tol = 1e-12,\n                             iterations = 10,\n                             store_trace = true,\n                             show_trace = false))\n\n\n\n\nAnother interface is also available, based directly on keywords:\n\n\nres = optimize(f, g!,\n               [0.0, 0.0],\n               method = GradientDescent(),\n               g_tol = 1e-12,\n               iterations = 10,\n               store_trace = true,\n               show_trace = false)\n\n\n\n\nNotice the need to specify the method using a keyword if this syntax is used. This approach might be deprecated in the future, and as a result we recommend writing code that has to maintained using the \nOptim.Options\n approach.", 
            "title": "Configurable Options"
        }, 
        {
            "location": "/user/config/#configurable-options", 
            "text": "There are several options that simply take on some default values if the user doensn't supply anything else than a function (and gradient) and a starting point.", 
            "title": "Configurable options"
        }, 
        {
            "location": "/user/config/#solver-options", 
            "text": "There quite a few different solvers available in Optim, and they are all listed below. Notice that the constructors are written without input here, but they generally take keywords to tweak the way they work. See the pages describing each solver for more detail.  Requires only a function handle:   NelderMead()  SimulatedAnnealing()   Requires a function and gradient (will be approximated if omitted):   BFGS()  LBFGS()  ConjugateGradient()  GradientDescent()  MomentumGradientDescent()  AcceleratedGradientDescent()   Requires a function, a gradient, and a Hessian (cannot be omitted):   Newton()  NewtonTrustRegion()   Box constrained minimization:   Fminbox()   Special methods for bounded univariate optimization:   Brent()  GoldenSection()", 
            "title": "Solver options"
        }, 
        {
            "location": "/user/config/#general-options", 
            "text": "In addition to the solver, you can alter the behavior of the Optim package by using the following keywords:   x_tol : Absolute tolerance in changes of the input vector  x , in infinity norm. Defaults to  0.0 .  f_tol : Relative tolerance in changes of the objective value. Defaults to  0.0 .  g_tol : Absolute tolerance in the gradient, in infinity norm. Defaults to  1e-8 . For gradient free methods, this will control the main convergence tolerance, which is solver specific.  f_calls_limit : A soft upper limit on the number of objective calls. Defaults to  0  (unlimited).  g_calls_limit : A soft upper limit on the number of gradient calls. Defaults to  0  (unlimited).  h_calls_limit : A soft upper limit on the number of Hessian calls. Defaults to  0  (unlimited).  allow_f_increases : Allow steps that increase the objective value. Defaults to  false . Note that, when setting this to  true , the last iterate will be returned as the minimizer even if the objective increased.  iterations : How many iterations will run before the algorithm gives up? Defaults to  1_000 .  store_trace : Should a trace of the optimization algorithm's state be stored? Defaults to  false .  show_trace : Should a trace of the optimization algorithm's state be shown on  stdout ? Defaults to  false .  extended_trace : Save additional information. Solver dependent. Defaults to  false .  show_every : Trace output is printed every  show_every th iteration.  callback : A function to be called during tracing. A return value of  true  stops the  optimize  call.  time_limit : A soft upper limit on the total run time. Defaults to  NaN  (unlimited).   We currently recommend the statically dispatched interface by using the  Optim.Options  constructor:  res = optimize(f, g!,\n               [0.0, 0.0],\n               GradientDescent(),\n               Optim.Options(g_tol = 1e-12,\n                             iterations = 10,\n                             store_trace = true,\n                             show_trace = false))  Another interface is also available, based directly on keywords:  res = optimize(f, g!,\n               [0.0, 0.0],\n               method = GradientDescent(),\n               g_tol = 1e-12,\n               iterations = 10,\n               store_trace = true,\n               show_trace = false)  Notice the need to specify the method using a keyword if this syntax is used. This approach might be deprecated in the future, and as a result we recommend writing code that has to maintained using the  Optim.Options  approach.", 
            "title": "General Options"
        }, 
        {
            "location": "/algo/linesearch/", 
            "text": "Line search\n\n\n\n\nDescription\n\n\nThe line search functionality has been moved to \nLineSearches.jl\n.\n\n\nLine search is used to decide the step length along the direction computed by an optimization algorithm.\n\n\nThe following \nOptim\n algorithms use line search:\n\n\n\n\nAccelerated Gradient Descent\n\n\n(L-)BFGS\n\n\nConjugate Gradient\n\n\nGradient Descent\n\n\nMomentum Gradient Descent\n\n\nNewton\n\n\n\n\nBy default \nOptim\n calls the line search algorithm \nHagerZhang()\n provided by \nLineSearches\n. Different line search algorithms can be assigned with the \nlinesearch\n keyword argument to the given algorithm.\n\n\nLineSearches\n also allows the user to decide how the initial step length for the line search algorithm is chosen. This is set with the \nalphaguess\n keyword argument for the \nOptim\n algorithm. The default procedure varies.\n\n\n\n\nExample\n\n\nThis example compares two different line search algorithms on the Rosenbrock problem.\n\n\nFirst, run \nNewton\n with the default line search algorithm:\n\n\nusing Optim, LineSearches\nprob = Optim.UnconstrainedProblems.examples[\nRosenbrock\n]\n\nalgo_hz = Newton(;alphaguess = LineSearches.InitialStatic(), linesearch = LineSearches.HagerZhang())\nres_hz = Optim.optimize(prob.f, prob.g!, prob.h!, prob.initial_x, method=algo_hz)\n\n\n\n\nThis gives the result\n\n\n * Algorithm: Newton's Method\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.9999999999999994,0.9999999999999989]\n * Minimum: 3.081488e-31\n * Iterations: 14\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.06e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 2.94e+13 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 1.11e-15\n   * stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 44\n * Gradient Calls: 44\n * Hessian Calls: 14\n\n\n\n\nNow we can try \nNewton\n with the More-Thuente line search:\n\n\nalgo_mt = Newton(;alphaguess = LineSearches.InitialStatic(), linesearch = LineSearches.MoreThuente())\nres_mt = Optim.optimize(prob.f, prob.g!, prob.h!, prob.initial_x, method=algo_mt)\n\n\n\n\nThis gives the following result, reducing the number of function and gradient calls:\n\n\nResults of Optimization Algorithm\n * Algorithm: Newton's Method\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.9999999999999992,0.999999999999998]\n * Minimum: 2.032549e-29\n * Iterations: 14\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.67e-08\n   * |f(x) - f(x')| \u2264 0.0e00 |f(x)|: false\n     |f(x) - f(x')| = 1.66e+13 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 1.76e-13\n   * stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 17\n * Gradient Calls: 17\n * Hessian Calls: 14\n\n\n\n\n\n\nReferences", 
            "title": "Linesearch"
        }, 
        {
            "location": "/algo/linesearch/#line-search", 
            "text": "", 
            "title": "Line search"
        }, 
        {
            "location": "/algo/linesearch/#description", 
            "text": "The line search functionality has been moved to  LineSearches.jl .  Line search is used to decide the step length along the direction computed by an optimization algorithm.  The following  Optim  algorithms use line search:   Accelerated Gradient Descent  (L-)BFGS  Conjugate Gradient  Gradient Descent  Momentum Gradient Descent  Newton   By default  Optim  calls the line search algorithm  HagerZhang()  provided by  LineSearches . Different line search algorithms can be assigned with the  linesearch  keyword argument to the given algorithm.  LineSearches  also allows the user to decide how the initial step length for the line search algorithm is chosen. This is set with the  alphaguess  keyword argument for the  Optim  algorithm. The default procedure varies.", 
            "title": "Description"
        }, 
        {
            "location": "/algo/linesearch/#example", 
            "text": "This example compares two different line search algorithms on the Rosenbrock problem.  First, run  Newton  with the default line search algorithm:  using Optim, LineSearches\nprob = Optim.UnconstrainedProblems.examples[ Rosenbrock ]\n\nalgo_hz = Newton(;alphaguess = LineSearches.InitialStatic(), linesearch = LineSearches.HagerZhang())\nres_hz = Optim.optimize(prob.f, prob.g!, prob.h!, prob.initial_x, method=algo_hz)  This gives the result   * Algorithm: Newton's Method\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.9999999999999994,0.9999999999999989]\n * Minimum: 3.081488e-31\n * Iterations: 14\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.06e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 2.94e+13 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 1.11e-15\n   * stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 44\n * Gradient Calls: 44\n * Hessian Calls: 14  Now we can try  Newton  with the More-Thuente line search:  algo_mt = Newton(;alphaguess = LineSearches.InitialStatic(), linesearch = LineSearches.MoreThuente())\nres_mt = Optim.optimize(prob.f, prob.g!, prob.h!, prob.initial_x, method=algo_mt)  This gives the following result, reducing the number of function and gradient calls:  Results of Optimization Algorithm\n * Algorithm: Newton's Method\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.9999999999999992,0.999999999999998]\n * Minimum: 2.032549e-29\n * Iterations: 14\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.67e-08\n   * |f(x) - f(x')| \u2264 0.0e00 |f(x)|: false\n     |f(x) - f(x')| = 1.66e+13 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 1.76e-13\n   * stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 17\n * Gradient Calls: 17\n * Hessian Calls: 14", 
            "title": "Example"
        }, 
        {
            "location": "/algo/linesearch/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/user/algochoice/", 
            "text": "Algorithm choice\n\n\nThere are two main settings you must choose in Optim: the algorithm and the linesearch.\n\n\n\n\nAlgorithms\n\n\nThe first choice to be made is that of the order of the method. Zeroth-order methods do not have gradient information, and are very slow to converge, especially in high dimension. First-order methods do not have access to curvature information and can take a large number of iterations to converge for badly conditioned problems. Second-order methods can converge very quickly once in the vicinity of a minimizer. Of course, this enhanced performance comes at a cost: the objective function has to be differentiable, you have to supply gradients and Hessians, and, for second order methods, a linear system has to be solved at each step.\n\n\nIf you can provide analytic gradients and Hessians, and the dimension of the problem is not too large, then second order methods are very efficient. The Newton method with trust region is the method of choice. \n\n\nWhen you do not have an explicit Hessian or when the dimension becomes large enough that the linear solve in the Newton method becomes the bottleneck, first order methods should be preferred. BFGS is a very efficient method, but also requires a linear system solve. LBFGS usually has a performance very close to that of BFGS, and avoids linear system solves (the parameter \nm\n can be tweaked: increasing it can improve the convergence, at the expense of memory and time spent in linear algebra operations). The conjugate gradient method usually converges less quickly than LBFGS, but requires less memory. Gradient descent should only be used for testing. Acceleration methods are experimental.\n\n\nWhen the objective function is non-differentiable or you do not want to use gradients, use zeroth-order methods. Nelder-Mead is currently the most robust.\n\n\n\n\nLinesearches\n\n\nLinesearches are used in every first- and second-order method except for the trust-region Newton method. Linesearch routines attempt to locate quickly an approximate minimizer of the univariate function $\\alpha \\to f(x+ \\alpha d)$, where $d$ is the descent direction computed by the algorithm. They vary in how accurate this minimization is. Two good linesearches are BackTracking and HagerZhang, the former being less stringent than the latter. For well-conditioned objective functions and methods where the step is usually well-scaled (such as LBFGS or Newton), a rough linesearch such as BackTracking is usually the most performant. For badly behaved problems or when extreme accuracy is needed (gradients below the square root of the machine epsilon, about $10^{-8}$ with \nFloat64\n), the HagerZhang method proves more robust. An exception is the conjugate gradient method which requires an accurate linesearch to be efficient, and should be used with the HagerZhang linesearch.\n\n\n\n\nSummary\n\n\nAs a very crude heuristic:\n\n\nFor a low-dimensional problem with analytic gradients and Hessians, use the Newton method with trust region. For larger problems or when there is no analytic Hessian, use LBFGS, and tweak the parameter \nm\n if needed. If the function is non-differentiable, use Nelder-Mead. Use the HagerZhang linesearch for robustness and BackTracking for speed.", 
            "title": "Algorithm choice"
        }, 
        {
            "location": "/user/algochoice/#algorithm-choice", 
            "text": "There are two main settings you must choose in Optim: the algorithm and the linesearch.", 
            "title": "Algorithm choice"
        }, 
        {
            "location": "/user/algochoice/#algorithms", 
            "text": "The first choice to be made is that of the order of the method. Zeroth-order methods do not have gradient information, and are very slow to converge, especially in high dimension. First-order methods do not have access to curvature information and can take a large number of iterations to converge for badly conditioned problems. Second-order methods can converge very quickly once in the vicinity of a minimizer. Of course, this enhanced performance comes at a cost: the objective function has to be differentiable, you have to supply gradients and Hessians, and, for second order methods, a linear system has to be solved at each step.  If you can provide analytic gradients and Hessians, and the dimension of the problem is not too large, then second order methods are very efficient. The Newton method with trust region is the method of choice.   When you do not have an explicit Hessian or when the dimension becomes large enough that the linear solve in the Newton method becomes the bottleneck, first order methods should be preferred. BFGS is a very efficient method, but also requires a linear system solve. LBFGS usually has a performance very close to that of BFGS, and avoids linear system solves (the parameter  m  can be tweaked: increasing it can improve the convergence, at the expense of memory and time spent in linear algebra operations). The conjugate gradient method usually converges less quickly than LBFGS, but requires less memory. Gradient descent should only be used for testing. Acceleration methods are experimental.  When the objective function is non-differentiable or you do not want to use gradients, use zeroth-order methods. Nelder-Mead is currently the most robust.", 
            "title": "Algorithms"
        }, 
        {
            "location": "/user/algochoice/#linesearches", 
            "text": "Linesearches are used in every first- and second-order method except for the trust-region Newton method. Linesearch routines attempt to locate quickly an approximate minimizer of the univariate function $\\alpha \\to f(x+ \\alpha d)$, where $d$ is the descent direction computed by the algorithm. They vary in how accurate this minimization is. Two good linesearches are BackTracking and HagerZhang, the former being less stringent than the latter. For well-conditioned objective functions and methods where the step is usually well-scaled (such as LBFGS or Newton), a rough linesearch such as BackTracking is usually the most performant. For badly behaved problems or when extreme accuracy is needed (gradients below the square root of the machine epsilon, about $10^{-8}$ with  Float64 ), the HagerZhang method proves more robust. An exception is the conjugate gradient method which requires an accurate linesearch to be efficient, and should be used with the HagerZhang linesearch.", 
            "title": "Linesearches"
        }, 
        {
            "location": "/user/algochoice/#summary", 
            "text": "As a very crude heuristic:  For a low-dimensional problem with analytic gradients and Hessians, use the Newton method with trust region. For larger problems or when there is no analytic Hessian, use LBFGS, and tweak the parameter  m  if needed. If the function is non-differentiable, use Nelder-Mead. Use the HagerZhang linesearch for robustness and BackTracking for speed.", 
            "title": "Summary"
        }, 
        {
            "location": "/algo/precondition/", 
            "text": "Preconditioning\n\n\nThe \nGradientDescent\n, \nConjugateGradient\n and \nLBFGS\n methods support preconditioning. A preconditioner can be thought of as a change of coordinates under which the Hessian is better conditioned. With a good preconditioner substantially improved convergence is possible.\n\n\nA preconditioner \nP\ncan be of any type as long as the following two methods are implemented:\n\n\n\n\nA_ldiv_B!(pgr, P, gr)\n : apply \nP\n to a vector \ngr\n and store in \npgr\n     (intuitively, \npgr = P \\ gr\n)\n\n\ndot(x, P, y)\n : the inner product induced by \nP\n     (intuitively, \ndot(x, P * y)\n)\n\n\n\n\nPrecisely what these operations mean, depends on how \nP\n is stored. Commonly, we store a matrix \nP\n which approximates the Hessian in some vague sense. In this case,\n\n\n\n\nA_ldiv_B!(pgr, P, gr) = copyto!(pgr, P \\ A)\n\n\ndot(x, P, y) = dot(x, P * y)\n\n\n\n\nFinally, it is possible to update the preconditioner as the state variable \nx\n changes. This is done through  \nprecondprep!\n which is passed to the optimizers as kw-argument, e.g.,\n\n\n   method=ConjugateGradient(P = precond(100), precondprep! = precond(100))\n\n\n\n\nthough in this case it would always return the same matrix. (See \nfminbox.jl\n for a more natural example.)\n\n\nApart from preconditioning with matrices, \nOptim.jl\n provides a type \nInverseDiagonal\n, which represents a diagonal matrix by its inverse elements.\n\n\n\n\nExample\n\n\nBelow, we see an example where a function is minimized without and with a preconditioner applied.\n\n\nusing ForwardDiff, Optim, SparseArrays\ninitial_x = zeros(100)\nplap(U; n = length(U)) = (n-1)*sum((0.1 .+ diff(U).^2).^2 ) - sum(U) / (n-1)\nplap1(x) = ForwardDiff.gradient(plap,x)\nprecond(n) = spdiagm(-1 =\n -ones(n-1), 0 =\n 2ones(n), 1 =\n -ones(n-1)) * (n+1)\nf(x) = plap([0; x; 0])\ng!(G, x) = copyto!(G, (plap1([0; x; 0]))[2:end-1])\nresult = Optim.optimize(f, g!, initial_x, method = ConjugateGradient(P = nothing))\nresult = Optim.optimize(f, g!, initial_x, method = ConjugateGradient(P = precond(100)))\n\n\n\n\nThe former optimize call converges at a slower rate than the latter. Looking at a  plot of the 2D version of the function shows the problem.\n\n\n\n\nThe contours are shaped like ellipsoids, but we would rather want them to be circles. Using the preconditioner effectively changes the coordinates such that the contours becomes less ellipsoid-like. Benchmarking shows that using preconditioning provides  an approximate speed-up factor of 15 in this 100 dimensional case.\n\n\n\n\nReferences", 
            "title": "Preconditioners"
        }, 
        {
            "location": "/algo/precondition/#preconditioning", 
            "text": "The  GradientDescent ,  ConjugateGradient  and  LBFGS  methods support preconditioning. A preconditioner can be thought of as a change of coordinates under which the Hessian is better conditioned. With a good preconditioner substantially improved convergence is possible.  A preconditioner  P can be of any type as long as the following two methods are implemented:   A_ldiv_B!(pgr, P, gr)  : apply  P  to a vector  gr  and store in  pgr      (intuitively,  pgr = P \\ gr )  dot(x, P, y)  : the inner product induced by  P      (intuitively,  dot(x, P * y) )   Precisely what these operations mean, depends on how  P  is stored. Commonly, we store a matrix  P  which approximates the Hessian in some vague sense. In this case,   A_ldiv_B!(pgr, P, gr) = copyto!(pgr, P \\ A)  dot(x, P, y) = dot(x, P * y)   Finally, it is possible to update the preconditioner as the state variable  x  changes. This is done through   precondprep!  which is passed to the optimizers as kw-argument, e.g.,     method=ConjugateGradient(P = precond(100), precondprep! = precond(100))  though in this case it would always return the same matrix. (See  fminbox.jl  for a more natural example.)  Apart from preconditioning with matrices,  Optim.jl  provides a type  InverseDiagonal , which represents a diagonal matrix by its inverse elements.", 
            "title": "Preconditioning"
        }, 
        {
            "location": "/algo/precondition/#example", 
            "text": "Below, we see an example where a function is minimized without and with a preconditioner applied.  using ForwardDiff, Optim, SparseArrays\ninitial_x = zeros(100)\nplap(U; n = length(U)) = (n-1)*sum((0.1 .+ diff(U).^2).^2 ) - sum(U) / (n-1)\nplap1(x) = ForwardDiff.gradient(plap,x)\nprecond(n) = spdiagm(-1 =  -ones(n-1), 0 =  2ones(n), 1 =  -ones(n-1)) * (n+1)\nf(x) = plap([0; x; 0])\ng!(G, x) = copyto!(G, (plap1([0; x; 0]))[2:end-1])\nresult = Optim.optimize(f, g!, initial_x, method = ConjugateGradient(P = nothing))\nresult = Optim.optimize(f, g!, initial_x, method = ConjugateGradient(P = precond(100)))  The former optimize call converges at a slower rate than the latter. Looking at a  plot of the 2D version of the function shows the problem.   The contours are shaped like ellipsoids, but we would rather want them to be circles. Using the preconditioner effectively changes the coordinates such that the contours becomes less ellipsoid-like. Benchmarking shows that using preconditioning provides  an approximate speed-up factor of 15 in this 100 dimensional case.", 
            "title": "Example"
        }, 
        {
            "location": "/algo/precondition/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/algo/complex/", 
            "text": "Complex optimization\n\n\nOptimization of functions defined on complex inputs ($\\mathbb{C}^n \\to \\mathbb{R}$) is supported by simply passing a complex $x$ as input. The algorithms supported are all those which can naturally be extended to work with complex numbers: simulated annealing and all the first-order methods.\n\n\nThe gradient of a complex-to-real function is defined as the only vector $g$ such that\n\n\n\n\n\nf(x+h) = f(x) + \\mbox{Re}(g' * h) + \\mathcal{O}(h^2).\n\n\n\n\n\nThis is sometimes written\n\n\n\n\n\ng = \\frac{df}{d(z*)} = \\frac{df}{d(\\mbox{Re}(z))} + i \\frac{df}{d(\\mbox{Im(z)})}.\n\n\n\n\n\nThe gradient of a $\\mathbb{C}^n \\to \\mathbb{R}$ function is a $\\mathbb{C}^n \\to \\mathbb{C}^n$ map. Even if it is differentiable when seen as a function of $\\mathbb{R}^{2n}$ to $\\mathbb{R}^{2n}$, it might not be complex-differentiable. For instance, take $f(z) = \\mbox{Re}(z)^2$. Then $g(z) = 2 \\mbox{Re}(z)$, which is not complex-differentiable (holomorphic). Therefore, the Hessian of a $\\mathbb{C}^n \\to \\mathbb{R}$ function is in general not well-defined as a $n \\times n$ complex matrix (only as a $2n \\times 2n$ real matrix), and therefore second-order optimization algorithms are not applicable directly. To use second-order optimization, convert to real variables.\n\n\n\n\nExamples\n\n\nWe show how to minimize a quadratic plus quartic function with the \nLBFGS\n optimization algorithm.\n\n\nusing Random\nRandom.seed!(0) # Set the seed for reproducibility\n# \u03bc is the strength of the quartic. \u03bc = 0 is just a quadratic problem\nn = 4\nA = randn(n,n) + im*randn(n,n)\nA = A'A + I\nb = randn(n) + im*randn(n)\n\u03bc = 1.0\n\nfcomplex(x) = real(dot(x,A*x)/2 - dot(b,x)) + \u03bc*sum(abs.(x).^4)\ngcomplex(x) = A*x-b + 4\u03bc*(abs.(x).^2).*x\ngcomplex!(stor,x) = copyto!(stor,gcomplex(x))\n\nx0 = randn(n)+im*randn(n)\n\nres = optimize(fcomplex, gcomplex!, x0, LBFGS())\n\n\n\n\nThe output of the optimization is\n\n\nResults of Optimization Algorithm\n * Algorithm: L-BFGS\n * Starting Point: [0.48155603952425174 - 1.477880724921868im,-0.3219431528959694 - 0.18542418173298963im, ...]\n * Minimizer: [0.14163543901272568 - 0.034929496785515886im,-0.1208600058040362 - 0.6125620908171383im, ...]\n * Minimum: -1.568997e+00\n * Iterations: 16\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.28e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = -4.25e-16 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 6.33e-11\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 48\n * Gradient Calls: 48\n\n\n\n\nSimilarly, with \nConjugateGradient\n.\n\n\nres = optimize(fcomplex, gcomplex!, x0, ConjugateGradient())\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: Conjugate Gradient\n * Starting Point: [0.48155603952425174 - 1.477880724921868im,-0.3219431528959694 - 0.18542418173298963im, ...]\n * Minimizer: [0.1416354378490425 - 0.034929499492595516im,-0.12086000949769983 - 0.6125620892675705im, ...]\n * Minimum: -1.568997e+00\n * Iterations: 23\n * Convergence: false\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 8.54e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = -4.25e-16 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 3.72e-08\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 51\n * Gradient Calls: 29\n\n\n\n\n\n\nDifferentation\n\n\nThe finite difference methods used by \nOptim\n support real functions with complex inputs.\n\n\nres = optimize(fcomplex, x0, LBFGS())\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: L-BFGS\n * Starting Point: [0.48155603952425174 - 1.477880724921868im,-0.3219431528959694 - 0.18542418173298963im, ...]\n * Minimizer: [0.1416354390108624 - 0.034929496786122484im,-0.12086000580073922 - 0.6125620908025359im, ...]\n * Minimum: -1.568997e+00\n * Iterations: 16\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.28e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: true\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 1.04e-10\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 48\n * Gradient Calls: 48\n\n\n\n\nAutomatic differentiation support for complex inputs may come when \nCassete.jl\n is ready.\n\n\n\n\nReferences\n\n\n\n\nSorber, L., Barel, M. V., \n Lathauwer, L. D. (2012). Unconstrained optimization of real functions in complex variables. SIAM Journal on Optimization, 22(3), 879-898.\n\n\nKreutz-Delgado, K. (2009). The complex gradient operator and the CR-calculus. arXiv preprint arXiv:0906.4835.", 
            "title": "Complex optimization"
        }, 
        {
            "location": "/algo/complex/#complex-optimization", 
            "text": "Optimization of functions defined on complex inputs ($\\mathbb{C}^n \\to \\mathbb{R}$) is supported by simply passing a complex $x$ as input. The algorithms supported are all those which can naturally be extended to work with complex numbers: simulated annealing and all the first-order methods.  The gradient of a complex-to-real function is defined as the only vector $g$ such that   \nf(x+h) = f(x) + \\mbox{Re}(g' * h) + \\mathcal{O}(h^2).   This is sometimes written   \ng = \\frac{df}{d(z*)} = \\frac{df}{d(\\mbox{Re}(z))} + i \\frac{df}{d(\\mbox{Im(z)})}.   The gradient of a $\\mathbb{C}^n \\to \\mathbb{R}$ function is a $\\mathbb{C}^n \\to \\mathbb{C}^n$ map. Even if it is differentiable when seen as a function of $\\mathbb{R}^{2n}$ to $\\mathbb{R}^{2n}$, it might not be complex-differentiable. For instance, take $f(z) = \\mbox{Re}(z)^2$. Then $g(z) = 2 \\mbox{Re}(z)$, which is not complex-differentiable (holomorphic). Therefore, the Hessian of a $\\mathbb{C}^n \\to \\mathbb{R}$ function is in general not well-defined as a $n \\times n$ complex matrix (only as a $2n \\times 2n$ real matrix), and therefore second-order optimization algorithms are not applicable directly. To use second-order optimization, convert to real variables.", 
            "title": "Complex optimization"
        }, 
        {
            "location": "/algo/complex/#examples", 
            "text": "We show how to minimize a quadratic plus quartic function with the  LBFGS  optimization algorithm.  using Random\nRandom.seed!(0) # Set the seed for reproducibility\n# \u03bc is the strength of the quartic. \u03bc = 0 is just a quadratic problem\nn = 4\nA = randn(n,n) + im*randn(n,n)\nA = A'A + I\nb = randn(n) + im*randn(n)\n\u03bc = 1.0\n\nfcomplex(x) = real(dot(x,A*x)/2 - dot(b,x)) + \u03bc*sum(abs.(x).^4)\ngcomplex(x) = A*x-b + 4\u03bc*(abs.(x).^2).*x\ngcomplex!(stor,x) = copyto!(stor,gcomplex(x))\n\nx0 = randn(n)+im*randn(n)\n\nres = optimize(fcomplex, gcomplex!, x0, LBFGS())  The output of the optimization is  Results of Optimization Algorithm\n * Algorithm: L-BFGS\n * Starting Point: [0.48155603952425174 - 1.477880724921868im,-0.3219431528959694 - 0.18542418173298963im, ...]\n * Minimizer: [0.14163543901272568 - 0.034929496785515886im,-0.1208600058040362 - 0.6125620908171383im, ...]\n * Minimum: -1.568997e+00\n * Iterations: 16\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.28e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = -4.25e-16 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 6.33e-11\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 48\n * Gradient Calls: 48  Similarly, with  ConjugateGradient .  res = optimize(fcomplex, gcomplex!, x0, ConjugateGradient())  Results of Optimization Algorithm\n * Algorithm: Conjugate Gradient\n * Starting Point: [0.48155603952425174 - 1.477880724921868im,-0.3219431528959694 - 0.18542418173298963im, ...]\n * Minimizer: [0.1416354378490425 - 0.034929499492595516im,-0.12086000949769983 - 0.6125620892675705im, ...]\n * Minimum: -1.568997e+00\n * Iterations: 23\n * Convergence: false\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 8.54e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = -4.25e-16 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 3.72e-08\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 51\n * Gradient Calls: 29", 
            "title": "Examples"
        }, 
        {
            "location": "/algo/complex/#differentation", 
            "text": "The finite difference methods used by  Optim  support real functions with complex inputs.  res = optimize(fcomplex, x0, LBFGS())  Results of Optimization Algorithm\n * Algorithm: L-BFGS\n * Starting Point: [0.48155603952425174 - 1.477880724921868im,-0.3219431528959694 - 0.18542418173298963im, ...]\n * Minimizer: [0.1416354390108624 - 0.034929496786122484im,-0.12086000580073922 - 0.6125620908025359im, ...]\n * Minimum: -1.568997e+00\n * Iterations: 16\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.28e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: true\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 1.04e-10\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 48\n * Gradient Calls: 48  Automatic differentiation support for complex inputs may come when  Cassete.jl  is ready.", 
            "title": "Differentation"
        }, 
        {
            "location": "/algo/complex/#references", 
            "text": "Sorber, L., Barel, M. V.,   Lathauwer, L. D. (2012). Unconstrained optimization of real functions in complex variables. SIAM Journal on Optimization, 22(3), 879-898.  Kreutz-Delgado, K. (2009). The complex gradient operator and the CR-calculus. arXiv preprint arXiv:0906.4835.", 
            "title": "References"
        }, 
        {
            "location": "/algo/manifolds/", 
            "text": "Manifold optimization\n\n\nOptim.jl supports the minimization of functions defined on Riemannian manifolds, i.e. with simple constraints such as normalization and orthogonality. The basic idea of such algorithms is to project back (\"retract\") each iterate of an unconstrained minimization method onto the manifold. This is used by passing a \nmanifold\n keyword argument to the optimizer.\n\n\n\n\nHowto\n\n\nHere is a simple test case where we minimize the Rayleigh quotient \nx, A x\n of a symmetric matrix \nA\n under the constraint \n||x|| = 1\n, finding an eigenvector associated with the lowest eigenvalue of \nA\n.\n\n\nn = 10\nA = Diagonal(range(1, stop=2, length=n))\nf(x) = dot(x,A*x)/2\ng(x) = A*x\ng!(stor,x) = copyto!(stor,g(x))\nx0 = randn(n)\n\nmanif = Optim.Sphere()\nOptim.optimize(f, g!, x0, Optim.ConjugateGradient(manifold=manif))\n\n\n\n\n\n\nSupported solvers and manifolds\n\n\nAll first-order optimization methods are supported.\n\n\nThe following manifolds are currently supported:\n\n\n\n\nFlat: Euclidean space, default. Standard unconstrained optimization.\n\n\nSphere: spherical constraint \n||x|| = 1\n\n\nStiefel: Stiefel manifold of N by n matrices with orthogonal columns, i.e. \nX'*X = I\n\n\n\n\nThe following meta-manifolds construct manifolds out of pre-existing ones:\n\n\n\n\nPowerManifold: identical copies of a specified manifold\n\n\nProductManifold: product of two (potentially different) manifolds\n\n\n\n\nSee \ntest/multivariate/manifolds.jl\n for usage examples.\n\n\nImplementing new manifolds is as simple as adding methods \nproject_tangent!(M::YourManifold,x)\n and \nretract!(M::YourManifold,g,x)\n. If you implement another manifold or optimization method, please contribute a PR!\n\n\n\n\nReferences\n\n\nThe Geometry of Algorithms with Orthogonality Constraints, Alan Edelman, Tom\u00e1s A. Arias, Steven T. Smith, SIAM. J. Matrix Anal. \n Appl., 20(2), 303\u2013353\n\n\nOptimization Algorithms on Matrix Manifolds, P.-A. Absil, R. Mahony, R. Sepulchre, Princeton University Press, 2008", 
            "title": "Manifolds"
        }, 
        {
            "location": "/algo/manifolds/#manifold-optimization", 
            "text": "Optim.jl supports the minimization of functions defined on Riemannian manifolds, i.e. with simple constraints such as normalization and orthogonality. The basic idea of such algorithms is to project back (\"retract\") each iterate of an unconstrained minimization method onto the manifold. This is used by passing a  manifold  keyword argument to the optimizer.", 
            "title": "Manifold optimization"
        }, 
        {
            "location": "/algo/manifolds/#howto", 
            "text": "Here is a simple test case where we minimize the Rayleigh quotient  x, A x  of a symmetric matrix  A  under the constraint  ||x|| = 1 , finding an eigenvector associated with the lowest eigenvalue of  A .  n = 10\nA = Diagonal(range(1, stop=2, length=n))\nf(x) = dot(x,A*x)/2\ng(x) = A*x\ng!(stor,x) = copyto!(stor,g(x))\nx0 = randn(n)\n\nmanif = Optim.Sphere()\nOptim.optimize(f, g!, x0, Optim.ConjugateGradient(manifold=manif))", 
            "title": "Howto"
        }, 
        {
            "location": "/algo/manifolds/#supported-solvers-and-manifolds", 
            "text": "All first-order optimization methods are supported.  The following manifolds are currently supported:   Flat: Euclidean space, default. Standard unconstrained optimization.  Sphere: spherical constraint  ||x|| = 1  Stiefel: Stiefel manifold of N by n matrices with orthogonal columns, i.e.  X'*X = I   The following meta-manifolds construct manifolds out of pre-existing ones:   PowerManifold: identical copies of a specified manifold  ProductManifold: product of two (potentially different) manifolds   See  test/multivariate/manifolds.jl  for usage examples.  Implementing new manifolds is as simple as adding methods  project_tangent!(M::YourManifold,x)  and  retract!(M::YourManifold,g,x) . If you implement another manifold or optimization method, please contribute a PR!", 
            "title": "Supported solvers and manifolds"
        }, 
        {
            "location": "/algo/manifolds/#references", 
            "text": "The Geometry of Algorithms with Orthogonality Constraints, Alan Edelman, Tom\u00e1s A. Arias, Steven T. Smith, SIAM. J. Matrix Anal.   Appl., 20(2), 303\u2013353  Optimization Algorithms on Matrix Manifolds, P.-A. Absil, R. Mahony, R. Sepulchre, Princeton University Press, 2008", 
            "title": "References"
        }, 
        {
            "location": "/user/tipsandtricks/", 
            "text": "Dealing with constant parameters\n\n\nIn many applications, there may be factors that are relevant to the function evaluations, but are fixed throughout the optimization. An obvious example is using data in a likelihood function, but it could also be parameters we wish to hold constant.\n\n\nConsider a squared error loss function that depends on some data \nx\n and \ny\n, and parameters \nbetas\n. As far as the solver is concerned, there should only be one input argument to the function we want to minimize, call it \nsqerror\n.\n\n\nThe problem is that we want to optimize a function \nsqerror\n that really depends on three inputs, and two of them are constant throught the optimization procedure. To do this, we need to define the variables \nx\n and \ny\n\n\nx = [1.0, 2.0, 3.0]\ny = 1.0 + 2.0 * x + [-0.3, 0.3, -0.1]\n\n\n\n\nWe then simply define a function in three variables\n\n\nfunction sqerror(betas, X, Y)\n    err = 0.0\n    for i in 1:length(X)\n        pred_i = betas[1] + betas[2] * X[i]\n        err += (Y[i] - pred_i)^2\n    end\n    return err\nend\n\n\n\n\nand then optimize the following anonymous function\n\n\nres = optimize(b -\n sqerror(b, x, y), [0.0, 0.0])\n\n\n\n\nAlternatively, we can define a closure \nsqerror(betas)\n that is aware of the variables we just defined\n\n\nfunction sqerror(betas)\n    err = 0.0\n    for i in 1:length(x)\n        pred_i = betas[1] + betas[2] * x[i]\n        err += (y[i] - pred_i)^2\n    end\n    return err\nend\n\n\n\n\nWe can then optimize the \nsqerror\n function just like any other function\n\n\nres = optimize(sqerror, [0.0, 0.0])\n\n\n\n\n\n\nAvoid repeating computations\n\n\nSay you are optimizing a function\n\n\nf(x) = x[1]^2+x[2]^2\ng!(storage, x) = copyto!(storage, [2x[1], 2x[2]])\n\n\n\n\nIn this situation, no calculations from \nf\n could be reused in \ng!\n. However, sometimes there is a substantial similarity between the objective function, and gradient, and some calculations can be reused.\n\n\nTo avoid repeating calculations, define functions \nfg!\n or \nfgh!\n that compute the objective function, the gradient and the Hessian (if needed) simultaneously. These functions internally can be written to avoid repeating common calculations.\n\n\nFor example, here we define a function \nfg!\n to compute the objective function and the gradient, as required:\n\n\nfunction fg!(F,G,x)\n  # do common computations here\n  # ...\n  if G != nothing\n    # code to compute gradient here\n    # writing the result to the vector G\n  end\n  if F != nothing\n    # value = ... code to compute objective function\n    return value\n  end\nend\n\n\n\n\nOptim\n will only call this function with an argument \nG\n that is \nnothing\n (if the gradient is not required) or a \nVector\n that should be filled (in-place) with the gradient. This flexibility is convenient for algorithms that only use the gradient in some iterations but not in others.\n\n\nNow we call \noptimize\n with the following syntax:\n\n\nOptim.optimize(Optim.only_fg!(fg!), [0., 0.], Optim.LBFGS())\n\n\n\n\nSimilarly, for a computation that requires the Hessian, we can write:\n\n\nfunction fgh!(F,G,H,x)\n  G == nothing || # compute gradient and store in G\n  H == nothing || # compute Hessian and store in H\n  F == nothing || return f(x)\n  nothing\nend\n\nOptim.optimize(Optim.only_fgh!(fgh!), [0., 0.], Optim.Newton())\n\n\n\n\n\n\nProvide gradients\n\n\nAs mentioned in the general introduction, passing analytical gradients can have an impact on performance. To show an example of this, consider the separable extension of the Rosenbrock function in dimension 5000, see \nSROSENBR\n in CUTEst.\n\n\nBelow, we use the gradients and objective functions from \nmastsif\n through \nCUTEst.jl\n. We only show the first five iterations of an attempt to minimize the function using Gradient Descent.\n\n\njulia\n @time optimize(f, initial_x, GradientDescent(),\n                      Optim.Options(show_trace=true, iterations = 5))\nIter     Function value   Gradient norm\n     0     4.850000e+04     2.116000e+02\n     1     1.018734e+03     2.704951e+01\n     2     3.468449e+00     5.721261e-01\n     3     2.966899e+00     2.638790e-02\n     4     2.511859e+00     5.237768e-01\n     5     2.107853e+00     1.020287e-01\n 21.731129 seconds (1.61 M allocations: 63.434 MB, 0.03% gc time)\nResults of Optimization Algorithm\n * Algorithm: Gradient Descent\n * Starting Point: [1.2,1.0, ...]\n * Minimizer: [1.0287767703731154,1.058769439356144, ...]\n * Minimum: 2.107853e+00\n * Iterations: 5\n * Convergence: false\n   * |x - x'| \n 0.0: false\n   * |f(x) - f(x')| / |f(x)| \n 0.0: false\n   * |g(x)| \n 1.0e-08: false\n   * Reached Maximum Number of Iterations: true\n * Objective Function Calls: 23\n * Gradient Calls: 23\n\njulia\n @time optimize(f, g!, initial_x, GradientDescent(),\n                      Optim.Options(show_trace=true, iterations = 5))\nIter     Function value   Gradient norm\n     0     4.850000e+04     2.116000e+02\n     1     1.018769e+03     2.704998e+01\n     2     3.468488e+00     5.721481e-01\n     3     2.966900e+00     2.638792e-02\n     4     2.511828e+00     5.237919e-01\n     5     2.107802e+00     1.020415e-01\n  0.009889 seconds (915 allocations: 270.266 KB)\nResults of Optimization Algorithm\n * Algorithm: Gradient Descent\n * Starting Point: [1.2,1.0, ...]\n * Minimizer: [1.0287763814102757,1.05876866832087, ...]\n * Minimum: 2.107802e+00\n * Iterations: 5\n * Convergence: false\n   * |x - x'| \n 0.0: false\n   * |f(x) - f(x')| / |f(x)| \n 0.0: false\n   * |g(x)| \n 1.0e-08: false\n   * Reached Maximum Number of Iterations: true\n * Objective Function Calls: 23\n * Gradient Calls: 23\n\n\n\n\nThe objective has obtained a value that is very similar between the two runs, but the run with the analytical gradient is way faster.  It is possible that the finite differences code can be improved, but generally the optimization will be slowed down by all the function evaluations required to do the central finite differences calculations.\n\n\n\n\nSeparating time spent in Optim's code and user provided functions\n\n\nConsider the Rosenbrock problem.\n\n\nusing Optim\nprob = Optim.UnconstrainedProblems.examples[\nRosenbrock\n];\n\n\n\n\nSay we optimize this function, and look at the total run time of \noptimize\n using the Newton Trust Region method, and we are surprised that it takes a long time to run. We then wonder if time is spent in Optim's own code (solving the sub-problem for example) or in evaluating the objective, gradient or hessian that we provided. Then it can be very useful to use the \nTimerOutputs.jl\n package. This package allows us to run an over-all timer for \noptimize\n, and add individual timers for \nf\n, \ng!\n, and \nh!\n. Consider the example below, that is due to the author of the package (Kristoffer Carlsson).\n\n\nusing TimerOutputs\nconst to = TimerOutput()\n\nf(x    ) =  @timeit to \nf\n  prob.f(x)\ng!(x, g) =  @timeit to \ng!\n prob.g!(x, g)\nh!(x, h) =  @timeit to \nh!\n prob.h!(x, h)\n\nbegin\nreset_timer!(to)\n@timeit to \nTrust Region\n begin\n    res = Optim.optimize(f, g!, h!, prob.initial_x, NewtonTrustRegion())\nend\nshow(to; allocations = false)\nend\n\n\n\n\nWe see that the time is actually \nnot\n spent in our provided functions, but most of the time is spent in the code for the trust region method.\n\n\n\n\nEarly stopping\n\n\nSometimes it might be of interest to stop the optimizer early. The simplest way to do this is to set the \niterations\n keyword in \nOptim.Options\n to some number. This will prevent the iteration counter exceeding some limit, with the standard value being 1000. Alternatively, it is possible to put a soft limit on the run time of the optimization procedure by setting the \ntime_limit\n keyword in the \nOptim.Options\n constructor.\n\n\nusing Optim\nproblem = Optim.UnconstrainedProblems.examples[\nRosenbrock\n]\n\nf = problem.f\ninitial_x = problem.initial_x\n\nfunction slow(x)\n    sleep(0.1)\n    f(x)\nend\n\nstart_time = time()\n\noptimize(slow, zeros(2), NelderMead(), Optim.Options(time_limit = 3.0))\n\n\n\n\nThis will stop after about three seconds. If it is more important that we stop before the limit is reached, it is possible to use a callback with a simple model for predicting how much time will have passed when the next iteration is over. Consider the following code\n\n\nusing Optim\nproblem = Optim.UnconstrainedProblems.examples[\nRosenbrock\n]\n\nf = problem.f\ninitial_x = problem.initial_x\n\nfunction very_slow(x)\n    sleep(.5)\n    f(x)\nend\n\nstart_time = time()\ntime_to_setup = zeros(1)\nfunction advanced_time_control(x)\n    println(\n * Iteration:       \n, x.iteration)\n    so_far =  time()-start_time\n    println(\n * Time so far:     \n, so_far)\n    if x.iteration == 0\n        time_to_setup[:] = time()-start_time\n    else\n        expected_next_time = so_far + (time()-start_time-time_to_setup[1])/(x.iteration)\n        println(\n * Next iteration \u2248 \n, expected_next_time)\n        println()\n        return expected_next_time \n 13 ? false : true\n    end\n    println()\n    false\nend\noptimize(very_slow, zeros(2), NelderMead(), Optim.Options(callback = advanced_time_control))\n\n\n\n\nIt will try to predict the elapsed time after the next iteration is over, and stop now if it is expected to exceed the limit of 13 seconds. Running it, we get something like the following output\n\n\njulia\n optimize(very_slow, zeros(2), NelderMead(), Optim.Options(callback = advanced_time_control))\n * Iteration:       0\n * Time so far:     2.219298839569092\n\n * Iteration:       1\n * Time so far:     3.4006409645080566\n * Next iteration \u2248 4.5429909229278564\n\n * Iteration:       2\n * Time so far:     4.403923988342285\n * Next iteration \u2248 5.476739525794983\n\n * Iteration:       3\n * Time so far:     5.407265901565552\n * Next iteration \u2248 6.4569235642751055\n\n * Iteration:       4\n * Time so far:     5.909044027328491\n * Next iteration \u2248 6.821732044219971\n\n * Iteration:       5\n * Time so far:     6.912338972091675\n * Next iteration \u2248 7.843148183822632\n\n * Iteration:       6\n * Time so far:     7.9156060218811035\n * Next iteration \u2248 8.85849153995514\n\n * Iteration:       7\n * Time so far:     8.918903827667236\n * Next iteration \u2248 9.870419979095459\n\n * Iteration:       8\n * Time so far:     9.922197818756104\n * Next iteration \u2248 10.880185931921005\n\n * Iteration:       9\n * Time so far:     10.925468921661377\n * Next iteration \u2248 11.888488478130764\n\n * Iteration:       10\n * Time so far:     11.92870283126831\n * Next iteration \u2248 12.895747828483582\n\n * Iteration:       11\n * Time so far:     12.932114839553833\n * Next iteration \u2248 13.902462200684981\n\nResults of Optimization Algorithm\n * Algorithm: Nelder-Mead\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.23359374999999996,0.042187499999999996, ...]\n * Minimum: 6.291677e-01\n * Iterations: 11\n * Convergence: false\n   *  \u221a(\u03a3(y\u1d62-y\u0304)\u00b2)/n \n 1.0e-08: false\n   * Reached Maximum Number of Iterations: false\n * Objective Function Calls: 24", 
            "title": "Tips and tricks"
        }, 
        {
            "location": "/user/tipsandtricks/#dealing-with-constant-parameters", 
            "text": "In many applications, there may be factors that are relevant to the function evaluations, but are fixed throughout the optimization. An obvious example is using data in a likelihood function, but it could also be parameters we wish to hold constant.  Consider a squared error loss function that depends on some data  x  and  y , and parameters  betas . As far as the solver is concerned, there should only be one input argument to the function we want to minimize, call it  sqerror .  The problem is that we want to optimize a function  sqerror  that really depends on three inputs, and two of them are constant throught the optimization procedure. To do this, we need to define the variables  x  and  y  x = [1.0, 2.0, 3.0]\ny = 1.0 + 2.0 * x + [-0.3, 0.3, -0.1]  We then simply define a function in three variables  function sqerror(betas, X, Y)\n    err = 0.0\n    for i in 1:length(X)\n        pred_i = betas[1] + betas[2] * X[i]\n        err += (Y[i] - pred_i)^2\n    end\n    return err\nend  and then optimize the following anonymous function  res = optimize(b -  sqerror(b, x, y), [0.0, 0.0])  Alternatively, we can define a closure  sqerror(betas)  that is aware of the variables we just defined  function sqerror(betas)\n    err = 0.0\n    for i in 1:length(x)\n        pred_i = betas[1] + betas[2] * x[i]\n        err += (y[i] - pred_i)^2\n    end\n    return err\nend  We can then optimize the  sqerror  function just like any other function  res = optimize(sqerror, [0.0, 0.0])", 
            "title": "Dealing with constant parameters"
        }, 
        {
            "location": "/user/tipsandtricks/#avoid-repeating-computations", 
            "text": "Say you are optimizing a function  f(x) = x[1]^2+x[2]^2\ng!(storage, x) = copyto!(storage, [2x[1], 2x[2]])  In this situation, no calculations from  f  could be reused in  g! . However, sometimes there is a substantial similarity between the objective function, and gradient, and some calculations can be reused.  To avoid repeating calculations, define functions  fg!  or  fgh!  that compute the objective function, the gradient and the Hessian (if needed) simultaneously. These functions internally can be written to avoid repeating common calculations.  For example, here we define a function  fg!  to compute the objective function and the gradient, as required:  function fg!(F,G,x)\n  # do common computations here\n  # ...\n  if G != nothing\n    # code to compute gradient here\n    # writing the result to the vector G\n  end\n  if F != nothing\n    # value = ... code to compute objective function\n    return value\n  end\nend  Optim  will only call this function with an argument  G  that is  nothing  (if the gradient is not required) or a  Vector  that should be filled (in-place) with the gradient. This flexibility is convenient for algorithms that only use the gradient in some iterations but not in others.  Now we call  optimize  with the following syntax:  Optim.optimize(Optim.only_fg!(fg!), [0., 0.], Optim.LBFGS())  Similarly, for a computation that requires the Hessian, we can write:  function fgh!(F,G,H,x)\n  G == nothing || # compute gradient and store in G\n  H == nothing || # compute Hessian and store in H\n  F == nothing || return f(x)\n  nothing\nend\n\nOptim.optimize(Optim.only_fgh!(fgh!), [0., 0.], Optim.Newton())", 
            "title": "Avoid repeating computations"
        }, 
        {
            "location": "/user/tipsandtricks/#provide-gradients", 
            "text": "As mentioned in the general introduction, passing analytical gradients can have an impact on performance. To show an example of this, consider the separable extension of the Rosenbrock function in dimension 5000, see  SROSENBR  in CUTEst.  Below, we use the gradients and objective functions from  mastsif  through  CUTEst.jl . We only show the first five iterations of an attempt to minimize the function using Gradient Descent.  julia  @time optimize(f, initial_x, GradientDescent(),\n                      Optim.Options(show_trace=true, iterations = 5))\nIter     Function value   Gradient norm\n     0     4.850000e+04     2.116000e+02\n     1     1.018734e+03     2.704951e+01\n     2     3.468449e+00     5.721261e-01\n     3     2.966899e+00     2.638790e-02\n     4     2.511859e+00     5.237768e-01\n     5     2.107853e+00     1.020287e-01\n 21.731129 seconds (1.61 M allocations: 63.434 MB, 0.03% gc time)\nResults of Optimization Algorithm\n * Algorithm: Gradient Descent\n * Starting Point: [1.2,1.0, ...]\n * Minimizer: [1.0287767703731154,1.058769439356144, ...]\n * Minimum: 2.107853e+00\n * Iterations: 5\n * Convergence: false\n   * |x - x'|   0.0: false\n   * |f(x) - f(x')| / |f(x)|   0.0: false\n   * |g(x)|   1.0e-08: false\n   * Reached Maximum Number of Iterations: true\n * Objective Function Calls: 23\n * Gradient Calls: 23\n\njulia  @time optimize(f, g!, initial_x, GradientDescent(),\n                      Optim.Options(show_trace=true, iterations = 5))\nIter     Function value   Gradient norm\n     0     4.850000e+04     2.116000e+02\n     1     1.018769e+03     2.704998e+01\n     2     3.468488e+00     5.721481e-01\n     3     2.966900e+00     2.638792e-02\n     4     2.511828e+00     5.237919e-01\n     5     2.107802e+00     1.020415e-01\n  0.009889 seconds (915 allocations: 270.266 KB)\nResults of Optimization Algorithm\n * Algorithm: Gradient Descent\n * Starting Point: [1.2,1.0, ...]\n * Minimizer: [1.0287763814102757,1.05876866832087, ...]\n * Minimum: 2.107802e+00\n * Iterations: 5\n * Convergence: false\n   * |x - x'|   0.0: false\n   * |f(x) - f(x')| / |f(x)|   0.0: false\n   * |g(x)|   1.0e-08: false\n   * Reached Maximum Number of Iterations: true\n * Objective Function Calls: 23\n * Gradient Calls: 23  The objective has obtained a value that is very similar between the two runs, but the run with the analytical gradient is way faster.  It is possible that the finite differences code can be improved, but generally the optimization will be slowed down by all the function evaluations required to do the central finite differences calculations.", 
            "title": "Provide gradients"
        }, 
        {
            "location": "/user/tipsandtricks/#separating-time-spent-in-optims-code-and-user-provided-functions", 
            "text": "Consider the Rosenbrock problem.  using Optim\nprob = Optim.UnconstrainedProblems.examples[ Rosenbrock ];  Say we optimize this function, and look at the total run time of  optimize  using the Newton Trust Region method, and we are surprised that it takes a long time to run. We then wonder if time is spent in Optim's own code (solving the sub-problem for example) or in evaluating the objective, gradient or hessian that we provided. Then it can be very useful to use the  TimerOutputs.jl  package. This package allows us to run an over-all timer for  optimize , and add individual timers for  f ,  g! , and  h! . Consider the example below, that is due to the author of the package (Kristoffer Carlsson).  using TimerOutputs\nconst to = TimerOutput()\n\nf(x    ) =  @timeit to  f   prob.f(x)\ng!(x, g) =  @timeit to  g!  prob.g!(x, g)\nh!(x, h) =  @timeit to  h!  prob.h!(x, h)\n\nbegin\nreset_timer!(to)\n@timeit to  Trust Region  begin\n    res = Optim.optimize(f, g!, h!, prob.initial_x, NewtonTrustRegion())\nend\nshow(to; allocations = false)\nend  We see that the time is actually  not  spent in our provided functions, but most of the time is spent in the code for the trust region method.", 
            "title": "Separating time spent in Optim's code and user provided functions"
        }, 
        {
            "location": "/user/tipsandtricks/#early-stopping", 
            "text": "Sometimes it might be of interest to stop the optimizer early. The simplest way to do this is to set the  iterations  keyword in  Optim.Options  to some number. This will prevent the iteration counter exceeding some limit, with the standard value being 1000. Alternatively, it is possible to put a soft limit on the run time of the optimization procedure by setting the  time_limit  keyword in the  Optim.Options  constructor.  using Optim\nproblem = Optim.UnconstrainedProblems.examples[ Rosenbrock ]\n\nf = problem.f\ninitial_x = problem.initial_x\n\nfunction slow(x)\n    sleep(0.1)\n    f(x)\nend\n\nstart_time = time()\n\noptimize(slow, zeros(2), NelderMead(), Optim.Options(time_limit = 3.0))  This will stop after about three seconds. If it is more important that we stop before the limit is reached, it is possible to use a callback with a simple model for predicting how much time will have passed when the next iteration is over. Consider the following code  using Optim\nproblem = Optim.UnconstrainedProblems.examples[ Rosenbrock ]\n\nf = problem.f\ninitial_x = problem.initial_x\n\nfunction very_slow(x)\n    sleep(.5)\n    f(x)\nend\n\nstart_time = time()\ntime_to_setup = zeros(1)\nfunction advanced_time_control(x)\n    println(  * Iteration:        , x.iteration)\n    so_far =  time()-start_time\n    println(  * Time so far:      , so_far)\n    if x.iteration == 0\n        time_to_setup[:] = time()-start_time\n    else\n        expected_next_time = so_far + (time()-start_time-time_to_setup[1])/(x.iteration)\n        println(  * Next iteration \u2248  , expected_next_time)\n        println()\n        return expected_next_time   13 ? false : true\n    end\n    println()\n    false\nend\noptimize(very_slow, zeros(2), NelderMead(), Optim.Options(callback = advanced_time_control))  It will try to predict the elapsed time after the next iteration is over, and stop now if it is expected to exceed the limit of 13 seconds. Running it, we get something like the following output  julia  optimize(very_slow, zeros(2), NelderMead(), Optim.Options(callback = advanced_time_control))\n * Iteration:       0\n * Time so far:     2.219298839569092\n\n * Iteration:       1\n * Time so far:     3.4006409645080566\n * Next iteration \u2248 4.5429909229278564\n\n * Iteration:       2\n * Time so far:     4.403923988342285\n * Next iteration \u2248 5.476739525794983\n\n * Iteration:       3\n * Time so far:     5.407265901565552\n * Next iteration \u2248 6.4569235642751055\n\n * Iteration:       4\n * Time so far:     5.909044027328491\n * Next iteration \u2248 6.821732044219971\n\n * Iteration:       5\n * Time so far:     6.912338972091675\n * Next iteration \u2248 7.843148183822632\n\n * Iteration:       6\n * Time so far:     7.9156060218811035\n * Next iteration \u2248 8.85849153995514\n\n * Iteration:       7\n * Time so far:     8.918903827667236\n * Next iteration \u2248 9.870419979095459\n\n * Iteration:       8\n * Time so far:     9.922197818756104\n * Next iteration \u2248 10.880185931921005\n\n * Iteration:       9\n * Time so far:     10.925468921661377\n * Next iteration \u2248 11.888488478130764\n\n * Iteration:       10\n * Time so far:     11.92870283126831\n * Next iteration \u2248 12.895747828483582\n\n * Iteration:       11\n * Time so far:     12.932114839553833\n * Next iteration \u2248 13.902462200684981\n\nResults of Optimization Algorithm\n * Algorithm: Nelder-Mead\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.23359374999999996,0.042187499999999996, ...]\n * Minimum: 6.291677e-01\n * Iterations: 11\n * Convergence: false\n   *  \u221a(\u03a3(y\u1d62-y\u0304)\u00b2)/n   1.0e-08: false\n   * Reached Maximum Number of Iterations: false\n * Objective Function Calls: 24", 
            "title": "Early stopping"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/", 
            "text": "Nonlinear constrained optimization\n\n\n\n\nTip\n\n\nThis example is also available as a Jupyter notebook: \nipnewton_basics.ipynb\n\n\n\n\nThe nonlinear constrained optimization interface in \nOptim\n assumes that the user can write the optimization problem in the following way.\n\n\n\n\n\n\\min_{x\\in\\mathbb{R}^n} f(x) \\quad \\text{such that}\\\\\nl_x \\leq \\phantom{c(}x\\phantom{)} \\leq u_x \\\\\nl_c \\leq c(x) \\leq u_c.\n\n\n\n\n\nFor equality constraints on $x_j$ or $c(x)_j$ you set those particular entries of bounds to be equal, $l_j=u_j$. Likewise, setting $l_j=-\\infty$ or $u_j=\\infty$ means that the constraint is unbounded from below or above respectively.\n\n\n\n\nConstrained optimization with \nIPNewton\n\n\nWe will go through examples on how to use the constraints interface with the interior-point Newton optimization algorithm \nIPNewton\n.\n\n\nThroughout these examples we work with the standard Rosenbrock function. The objective and its derivatives are given by\n\n\nfun(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n\nfunction fun_grad!(g, x)\ng[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\ng[2] = 200.0 * (x[2] - x[1]^2)\nend\n\nfunction fun_hess!(h, x)\nh[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\nh[1, 2] = -400.0 * x[1]\nh[2, 1] = -400.0 * x[1]\nh[2, 2] = 200.0\nend;\n\n\n\n\n\n\nOptimization interface\n\n\nTo solve a constrained optimization problem we call the \noptimize\n method\n\n\noptimize(d::AbstractObjective, constraints::AbstractConstraints, initial_x::Tx, method::ConstrainedOptimizer, options::Options)\n\n\n\n\nWe can create instances of \nAbstractObjective\n and \nAbstractConstraints\n using the types \nTwiceDifferentiable\n and \nTwiceDifferentiableConstraints\n from the package \nNLSolversBase.jl\n.\n\n\n\n\nBox minimzation\n\n\nWe want to optimize the Rosenbrock function in the box $-0.5 \\leq x \\leq 0.5$, starting from the point $x_0=(0,0)$. Box constraints are defined using, for example, \nTwiceDifferentiableConstraints(lx, ux)\n.\n\n\nx0 = [0.0, 0.0]\ndf = TwiceDifferentiable(fun, fun_grad!, fun_hess!, x0)\n\nlx = [-0.5, -0.5]; ux = [0.5, 0.5]\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nres = optimize(df, dfc, x0, IPNewton())\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.5,0.2500000000000883]\n * Minimum: 2.500000e-01\n * Iterations: 41\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 8.88e-14\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: true\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 1.00e+00\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 63\n * Gradient Calls: 63\n\n\n\n\nIf we only want to set lower bounds, use \nux = fill(Inf, 2)\n\n\nux = fill(Inf, 2)\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.9999999998342594,0.9999999996456271]\n * Minimum: 7.987239e-20\n * Iterations: 35\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.54e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 3.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 8.83e-09\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 63\n * Gradient Calls: 63\n\n\n\n\n\n\nDefining \"unconstrained\" problems\n\n\nAn unconstrained problem can be defined either by passing \nInf\n bounds or empty arrays. \nNote that we must pass the correct type information to the empty \nlx\n and \nux\n\n\nlx = fill(-Inf, 2); ux = fill(Inf, 2)\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\nlx = Float64[]; ux = Float64[]\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.9999999992619217,0.9999999985003628]\n * Minimum: 5.998937e-19\n * Iterations: 34\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 1.50e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 3.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 7.92e-09\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 63\n * Gradient Calls: 63\n\n\n\n\n\n\nGeneric nonlinear constraints\n\n\nWe now consider the Rosenbrock problem with a constraint on\n\n\n\n\n\n   c(x)_1 = x_1^2 + x_2^2.\n\n\n\n\n\nWe pass the information about the constraints to \noptimize\n by defining a vector function \nc(x)\n and its Jacobian \nJ(x)\n.\n\n\nThe Hessian information is treated differently, by considering the Lagrangian of the corresponding slack-variable transformed optimization problem. This is similar to how the \nCUTEst library\n works. Let $H_j(x)$ represent the Hessian of the $j$th component $c(x)_j$ of the generic constraints. and $\\lambda_j$ the corresponding dual variable in the Lagrangian. Then we want the \nconstraint\n object to add the values of $H_j(x)$ to the Hessian of the objective, weighted by $\\lambda_j$.\n\n\nThe Julian form for the supplied function $c(x)$ and the derivative information is then added in the following way.\n\n\ncon_c!(c, x) = (c[1] = x[1]^2 + x[2]^2; c)\nfunction con_jacobian!(J, x)\n    J[1,1] = 2*x[1]\n    J[1,2] = 2*x[2]\n    J\nend\nfunction con_h!(h, x, \u03bb)\n    h[1,1] += \u03bb[1]*2\n    h[2,2] += \u03bb[1]*2\nend;\n\n\n\n\nNote that \ncon_h!\n adds the \n\u03bb\n-weighted Hessian value of each element of \nc(x)\n to the Hessian of \nfun\n.\n\n\nWe can then optimize the Rosenbrock function inside the ball of radius $0.5$.\n\n\nlx = Float64[]; ux = Float64[]\nlc = [-Inf]; uc = [0.5^2]\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.45564896414551875,0.2058737998704899]\n * Minimum: 2.966216e-01\n * Iterations: 28\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: true\n     |x - x'| = 0.00e+00\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 7.71e-01\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 109\n * Gradient Calls: 109\n\n\n\n\nWe can add a lower bound on the constraint, and thus optimize the objective on the annulus with inner and outer radii $0.1$ and $0.5$ respectively.\n\n\nlc = [0.1^2]\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\n\n\n\n\u250c Warning: Initial guess is not an interior point\n\u2514 @ Optim ~/.julia/packages/Optim/27C9G/src/multivariate/solvers/constrained/ipnewton/ipnewton.jl:111\n\nStacktrace:\n [1] initial_state(::IPNewton{typeof(Optim.backtrack_constrained_grad),Symbol}, ::Optim.Options{Float64,Nothing}, ::TwiceDifferentiable{Float64,Array{Float64,1},Array{Float64,2},Array{Float64,1}}, ::TwiceDifferentiableConstraints{typeof(Main.ex-ipnewton_basics.con_c!),typeof(Main.ex-ipnewton_basics.con_jacobian!),typeof(Main.ex-ipnewton_basics.con_h!),Float64}, ::Array{Float64,1}) at /home/travis/.julia/packages/Optim/27C9G/src/multivariate/solvers/constrained/ipnewton/ipnewton.jl:112\n [2] optimize(::TwiceDifferentiable{Float64,Array{Float64,1},Array{Float64,2},Array{Float64,1}}, ::TwiceDifferentiableConstraints{typeof(Main.ex-ipnewton_basics.con_c!),typeof(Main.ex-ipnewton_basics.con_jacobian!),typeof(Main.ex-ipnewton_basics.con_h!),Float64}, ::Array{Float64,1}, ::IPNewton{typeof(Optim.backtrack_constrained_grad),Symbol}, ::Optim.Options{Float64,Nothing}) at /home/travis/.julia/packages/Optim/27C9G/src/multivariate/solvers/constrained/ipnewton/interior.jl:196 (repeats 2 times)\n [3] top-level scope at none:0\n [4] eval at ./boot.jl:319 [inlined]\n [5] (::getfield(Documenter.Expanders, Symbol(\n##8#10\n)){Module,Expr})() at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Expanders.jl:480\n [6] cd(::getfield(Documenter.Expanders, Symbol(\n##8#10\n)){Module,Expr}, ::String) at ./file.jl:96\n [7] #7 at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Expanders.jl:479 [inlined]\n [8] (::getfield(Documenter.Utilities, Symbol(\n##18#19\n)){getfield(Documenter.Expanders, Symbol(\n##7#9\n)){Documenter.Documents.Page,Module,Expr},Base.PipeEndpoint,Base.PipeEndpoint,Pipe,Array{UInt8,1}})() at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Utilities/Utilities.jl:591\n [9] with_logstate(::getfield(Documenter.Utilities, Symbol(\n##18#19\n)){getfield(Documenter.Expanders, Symbol(\n##7#9\n)){Documenter.Documents.Page,Module,Expr},Base.PipeEndpoint,Base.PipeEndpoint,Pipe,Array{UInt8,1}}, ::Base.CoreLogging.LogState) at ./logging.jl:395\n [10] with_logger(::Function, ::Logging.ConsoleLogger) at ./logging.jl:491\n [11] withoutput at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Utilities/Utilities.jl:589 [inlined]\n [12] runner(::Type{Documenter.Expanders.ExampleBlocks}, ::Markdown.Code, ::Documenter.Documents.Page, ::Documenter.Documents.Document) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Expanders.jl:478\n [13] dispatch(::Type{Documenter.Expanders.ExpanderPipeline}, ::Markdown.Code, ::Vararg{Any,N} where N) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Selectors.jl:168\n [14] expand(::Documenter.Documents.Document) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Expanders.jl:31\n [15] runner(::Type{Documenter.Builder.ExpandTemplates}, ::Documenter.Documents.Document) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Builder.jl:178\n [16] dispatch(::Type{Documenter.Builder.DocumentPipeline}, ::Documenter.Documents.Document) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Selectors.jl:168\n [17] #2 at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Documenter.jl:204 [inlined]\n [18] cd(::getfield(Documenter, Symbol(\n##2#3\n)){Documenter.Documents.Document}, ::String) at ./file.jl:96\n [19] #makedocs#1 at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Documenter.jl:203 [inlined]\n [20] (::getfield(Documenter, Symbol(\n#kw##makedocs\n)))(::NamedTuple{(:doctest,),Tuple{Bool}}, ::typeof(makedocs)) at ./none:0\n [21] top-level scope at none:0\n [22] include at ./boot.jl:317 [inlined]\n [23] include_relative(::Module, ::String) at ./loading.jl:1044\n [24] include(::Module, ::String) at ./sysimg.jl:29\n [25] exec_options(::Base.JLOptions) at ./client.jl:266\n [26] _start() at ./client.jl:425\nResults of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.45564896414551953,0.20587379987049065]\n * Minimum: 2.966216e-01\n * Iterations: 34\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: true\n     |x - x'| = 0.00e+00\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 7.71e-01\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 158\n * Gradient Calls: 158\n\n\n\n\nNote that the algorithm warns that the Initial guess is not an interior point.\n \nIPNewton\n can often handle this, however, if the initial guess is such that \nc(x) = u_c\n, then the algorithm currently fails. We may fix this in the future.\n\n\n\n\nMultiple constraints\n\n\nThe following example illustrates how to add an additional constraint. In particular, we add a constraint function\n\n\n\n\n\n   c(x)_2 = x_2\\sin(x_1)-x_1\n\n\n\n\n\nfunction con2_c!(c, x)\n    c[1] = x[1]^2 + x[2]^2     ## First constraint\n    c[2] = x[2]*sin(x[1])-x[1] ## Second constraint\n    c\nend\nfunction con2_jacobian!(J, x)\n    # First constraint\n    J[1,1] = 2*x[1]\n    J[1,2] = 2*x[2]\n    # Second constraint\n    J[2,1] = x[2]*cos(x[1])-1.0\n    J[2,2] = sin(x[1])\n    J\nend\nfunction con2_h!(h, x, \u03bb)\n    # First constraint\n    h[1,1] += \u03bb[1]*2\n    h[2,2] += \u03bb[1]*2\n    # Second constraint\n    h[1,1] += \u03bb[2]*x[2]*-sin(x[1])\n    h[1,2] += \u03bb[2]*cos(x[1])\n    # Symmetrize h\n    h[2,1]  = h[1,2]\n    h\nend;\n\n\n\n\nWe generate the constraint objects and call \nIPNewton\n with initial guess $x_0 = (0.25,0.25)$.\n\n\nx0 = [0.25, 0.25]\nlc = [-Inf, 0.0]; uc = [0.5^2, 0.0]\ndfc = TwiceDifferentiableConstraints(con2_c!, con2_jacobian!, con2_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.25,0.25]\n * Minimizer: [-1.595442184049669e-19,-1.9465281383022617e-18, ...]\n * Minimum: 1.000000e+00\n * Iterations: 29\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 6.90e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.38e-09 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 2.00e+00\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 215\n * Gradient Calls: 215\n\n\n\n\n\n\nPlain Program\n\n\nBelow follows a version of the program without any comments. The file is also available here: \nipnewton_basics.jl\n\n\nusing Optim, NLSolversBase #hide\nimport NLSolversBase: clear! #hide\n\nfun(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n\nfunction fun_grad!(g, x)\ng[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\ng[2] = 200.0 * (x[2] - x[1]^2)\nend\n\nfunction fun_hess!(h, x)\nh[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\nh[1, 2] = -400.0 * x[1]\nh[2, 1] = -400.0 * x[1]\nh[2, 2] = 200.0\nend;\n\nx0 = [0.0, 0.0]\ndf = TwiceDifferentiable(fun, fun_grad!, fun_hess!, x0)\n\nlx = [-0.5, -0.5]; ux = [0.5, 0.5]\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nres = optimize(df, dfc, x0, IPNewton())\n\nux = fill(Inf, 2)\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\nlx = fill(-Inf, 2); ux = fill(Inf, 2)\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\nlx = Float64[]; ux = Float64[]\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\ncon_c!(c, x) = (c[1] = x[1]^2 + x[2]^2; c)\nfunction con_jacobian!(J, x)\n    J[1,1] = 2*x[1]\n    J[1,2] = 2*x[2]\n    J\nend\nfunction con_h!(h, x, \u03bb)\n    h[1,1] += \u03bb[1]*2\n    h[2,2] += \u03bb[1]*2\nend;\n\nlx = Float64[]; ux = Float64[]\nlc = [-Inf]; uc = [0.5^2]\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\nlc = [0.1^2]\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\nfunction con2_c!(c, x)\n    c[1] = x[1]^2 + x[2]^2     ## First constraint\n    c[2] = x[2]*sin(x[1])-x[1] ## Second constraint\n    c\nend\nfunction con2_jacobian!(J, x)\n    # First constraint\n    J[1,1] = 2*x[1]\n    J[1,2] = 2*x[2]\n    # Second constraint\n    J[2,1] = x[2]*cos(x[1])-1.0\n    J[2,2] = sin(x[1])\n    J\nend\nfunction con2_h!(h, x, \u03bb)\n    # First constraint\n    h[1,1] += \u03bb[1]*2\n    h[2,2] += \u03bb[1]*2\n    # Second constraint\n    h[1,1] += \u03bb[2]*x[2]*-sin(x[1])\n    h[1,2] += \u03bb[2]*cos(x[1])\n    # Symmetrize h\n    h[2,1]  = h[1,2]\n    h\nend;\n\nx0 = [0.25, 0.25]\nlc = [-Inf, 0.0]; uc = [0.5^2, 0.0]\ndfc = TwiceDifferentiableConstraints(con2_c!, con2_jacobian!, con2_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\n# This file was generated using Literate.jl, https://github.com/fredrikekre/Literate.jl\n\n\n\n\nThis page was generated using \nLiterate.jl\n.", 
            "title": "Interior point Newton"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/#nonlinear-constrained-optimization", 
            "text": "Tip  This example is also available as a Jupyter notebook:  ipnewton_basics.ipynb   The nonlinear constrained optimization interface in  Optim  assumes that the user can write the optimization problem in the following way.   \n\\min_{x\\in\\mathbb{R}^n} f(x) \\quad \\text{such that}\\\\\nl_x \\leq \\phantom{c(}x\\phantom{)} \\leq u_x \\\\\nl_c \\leq c(x) \\leq u_c.   For equality constraints on $x_j$ or $c(x)_j$ you set those particular entries of bounds to be equal, $l_j=u_j$. Likewise, setting $l_j=-\\infty$ or $u_j=\\infty$ means that the constraint is unbounded from below or above respectively.", 
            "title": "Nonlinear constrained optimization"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/#constrained-optimization-with-ipnewton", 
            "text": "We will go through examples on how to use the constraints interface with the interior-point Newton optimization algorithm  IPNewton .  Throughout these examples we work with the standard Rosenbrock function. The objective and its derivatives are given by  fun(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n\nfunction fun_grad!(g, x)\ng[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\ng[2] = 200.0 * (x[2] - x[1]^2)\nend\n\nfunction fun_hess!(h, x)\nh[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\nh[1, 2] = -400.0 * x[1]\nh[2, 1] = -400.0 * x[1]\nh[2, 2] = 200.0\nend;", 
            "title": "Constrained optimization with IPNewton"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/#optimization-interface", 
            "text": "To solve a constrained optimization problem we call the  optimize  method  optimize(d::AbstractObjective, constraints::AbstractConstraints, initial_x::Tx, method::ConstrainedOptimizer, options::Options)  We can create instances of  AbstractObjective  and  AbstractConstraints  using the types  TwiceDifferentiable  and  TwiceDifferentiableConstraints  from the package  NLSolversBase.jl .", 
            "title": "Optimization interface"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/#box-minimzation", 
            "text": "We want to optimize the Rosenbrock function in the box $-0.5 \\leq x \\leq 0.5$, starting from the point $x_0=(0,0)$. Box constraints are defined using, for example,  TwiceDifferentiableConstraints(lx, ux) .  x0 = [0.0, 0.0]\ndf = TwiceDifferentiable(fun, fun_grad!, fun_hess!, x0)\n\nlx = [-0.5, -0.5]; ux = [0.5, 0.5]\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nres = optimize(df, dfc, x0, IPNewton())  Results of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.5,0.2500000000000883]\n * Minimum: 2.500000e-01\n * Iterations: 41\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 8.88e-14\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: true\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 1.00e+00\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 63\n * Gradient Calls: 63  If we only want to set lower bounds, use  ux = fill(Inf, 2)  ux = fill(Inf, 2)\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())  Results of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.9999999998342594,0.9999999996456271]\n * Minimum: 7.987239e-20\n * Iterations: 35\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 3.54e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 3.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 8.83e-09\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 63\n * Gradient Calls: 63", 
            "title": "Box minimzation"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/#defining-unconstrained-problems", 
            "text": "An unconstrained problem can be defined either by passing  Inf  bounds or empty arrays.  Note that we must pass the correct type information to the empty  lx  and  ux  lx = fill(-Inf, 2); ux = fill(Inf, 2)\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\nlx = Float64[]; ux = Float64[]\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())  Results of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.9999999992619217,0.9999999985003628]\n * Minimum: 5.998937e-19\n * Iterations: 34\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 1.50e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 3.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 7.92e-09\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 63\n * Gradient Calls: 63", 
            "title": "Defining \"unconstrained\" problems"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/#generic-nonlinear-constraints", 
            "text": "We now consider the Rosenbrock problem with a constraint on   \n   c(x)_1 = x_1^2 + x_2^2.   We pass the information about the constraints to  optimize  by defining a vector function  c(x)  and its Jacobian  J(x) .  The Hessian information is treated differently, by considering the Lagrangian of the corresponding slack-variable transformed optimization problem. This is similar to how the  CUTEst library  works. Let $H_j(x)$ represent the Hessian of the $j$th component $c(x)_j$ of the generic constraints. and $\\lambda_j$ the corresponding dual variable in the Lagrangian. Then we want the  constraint  object to add the values of $H_j(x)$ to the Hessian of the objective, weighted by $\\lambda_j$.  The Julian form for the supplied function $c(x)$ and the derivative information is then added in the following way.  con_c!(c, x) = (c[1] = x[1]^2 + x[2]^2; c)\nfunction con_jacobian!(J, x)\n    J[1,1] = 2*x[1]\n    J[1,2] = 2*x[2]\n    J\nend\nfunction con_h!(h, x, \u03bb)\n    h[1,1] += \u03bb[1]*2\n    h[2,2] += \u03bb[1]*2\nend;  Note that  con_h!  adds the  \u03bb -weighted Hessian value of each element of  c(x)  to the Hessian of  fun .  We can then optimize the Rosenbrock function inside the ball of radius $0.5$.  lx = Float64[]; ux = Float64[]\nlc = [-Inf]; uc = [0.5^2]\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())  Results of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.45564896414551875,0.2058737998704899]\n * Minimum: 2.966216e-01\n * Iterations: 28\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: true\n     |x - x'| = 0.00e+00\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 7.71e-01\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 109\n * Gradient Calls: 109  We can add a lower bound on the constraint, and thus optimize the objective on the annulus with inner and outer radii $0.1$ and $0.5$ respectively.  lc = [0.1^2]\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())  \u250c Warning: Initial guess is not an interior point\n\u2514 @ Optim ~/.julia/packages/Optim/27C9G/src/multivariate/solvers/constrained/ipnewton/ipnewton.jl:111\n\nStacktrace:\n [1] initial_state(::IPNewton{typeof(Optim.backtrack_constrained_grad),Symbol}, ::Optim.Options{Float64,Nothing}, ::TwiceDifferentiable{Float64,Array{Float64,1},Array{Float64,2},Array{Float64,1}}, ::TwiceDifferentiableConstraints{typeof(Main.ex-ipnewton_basics.con_c!),typeof(Main.ex-ipnewton_basics.con_jacobian!),typeof(Main.ex-ipnewton_basics.con_h!),Float64}, ::Array{Float64,1}) at /home/travis/.julia/packages/Optim/27C9G/src/multivariate/solvers/constrained/ipnewton/ipnewton.jl:112\n [2] optimize(::TwiceDifferentiable{Float64,Array{Float64,1},Array{Float64,2},Array{Float64,1}}, ::TwiceDifferentiableConstraints{typeof(Main.ex-ipnewton_basics.con_c!),typeof(Main.ex-ipnewton_basics.con_jacobian!),typeof(Main.ex-ipnewton_basics.con_h!),Float64}, ::Array{Float64,1}, ::IPNewton{typeof(Optim.backtrack_constrained_grad),Symbol}, ::Optim.Options{Float64,Nothing}) at /home/travis/.julia/packages/Optim/27C9G/src/multivariate/solvers/constrained/ipnewton/interior.jl:196 (repeats 2 times)\n [3] top-level scope at none:0\n [4] eval at ./boot.jl:319 [inlined]\n [5] (::getfield(Documenter.Expanders, Symbol( ##8#10 )){Module,Expr})() at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Expanders.jl:480\n [6] cd(::getfield(Documenter.Expanders, Symbol( ##8#10 )){Module,Expr}, ::String) at ./file.jl:96\n [7] #7 at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Expanders.jl:479 [inlined]\n [8] (::getfield(Documenter.Utilities, Symbol( ##18#19 )){getfield(Documenter.Expanders, Symbol( ##7#9 )){Documenter.Documents.Page,Module,Expr},Base.PipeEndpoint,Base.PipeEndpoint,Pipe,Array{UInt8,1}})() at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Utilities/Utilities.jl:591\n [9] with_logstate(::getfield(Documenter.Utilities, Symbol( ##18#19 )){getfield(Documenter.Expanders, Symbol( ##7#9 )){Documenter.Documents.Page,Module,Expr},Base.PipeEndpoint,Base.PipeEndpoint,Pipe,Array{UInt8,1}}, ::Base.CoreLogging.LogState) at ./logging.jl:395\n [10] with_logger(::Function, ::Logging.ConsoleLogger) at ./logging.jl:491\n [11] withoutput at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Utilities/Utilities.jl:589 [inlined]\n [12] runner(::Type{Documenter.Expanders.ExampleBlocks}, ::Markdown.Code, ::Documenter.Documents.Page, ::Documenter.Documents.Document) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Expanders.jl:478\n [13] dispatch(::Type{Documenter.Expanders.ExpanderPipeline}, ::Markdown.Code, ::Vararg{Any,N} where N) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Selectors.jl:168\n [14] expand(::Documenter.Documents.Document) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Expanders.jl:31\n [15] runner(::Type{Documenter.Builder.ExpandTemplates}, ::Documenter.Documents.Document) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Builder.jl:178\n [16] dispatch(::Type{Documenter.Builder.DocumentPipeline}, ::Documenter.Documents.Document) at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Selectors.jl:168\n [17] #2 at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Documenter.jl:204 [inlined]\n [18] cd(::getfield(Documenter, Symbol( ##2#3 )){Documenter.Documents.Document}, ::String) at ./file.jl:96\n [19] #makedocs#1 at /home/travis/.julia/packages/Documenter/Qo3Yk/src/Documenter.jl:203 [inlined]\n [20] (::getfield(Documenter, Symbol( #kw##makedocs )))(::NamedTuple{(:doctest,),Tuple{Bool}}, ::typeof(makedocs)) at ./none:0\n [21] top-level scope at none:0\n [22] include at ./boot.jl:317 [inlined]\n [23] include_relative(::Module, ::String) at ./loading.jl:1044\n [24] include(::Module, ::String) at ./sysimg.jl:29\n [25] exec_options(::Base.JLOptions) at ./client.jl:266\n [26] _start() at ./client.jl:425\nResults of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0]\n * Minimizer: [0.45564896414551953,0.20587379987049065]\n * Minimum: 2.966216e-01\n * Iterations: 34\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: true\n     |x - x'| = 0.00e+00\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 7.71e-01\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 158\n * Gradient Calls: 158  Note that the algorithm warns that the Initial guess is not an interior point.   IPNewton  can often handle this, however, if the initial guess is such that  c(x) = u_c , then the algorithm currently fails. We may fix this in the future.", 
            "title": "Generic nonlinear constraints"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/#multiple-constraints", 
            "text": "The following example illustrates how to add an additional constraint. In particular, we add a constraint function   \n   c(x)_2 = x_2\\sin(x_1)-x_1   function con2_c!(c, x)\n    c[1] = x[1]^2 + x[2]^2     ## First constraint\n    c[2] = x[2]*sin(x[1])-x[1] ## Second constraint\n    c\nend\nfunction con2_jacobian!(J, x)\n    # First constraint\n    J[1,1] = 2*x[1]\n    J[1,2] = 2*x[2]\n    # Second constraint\n    J[2,1] = x[2]*cos(x[1])-1.0\n    J[2,2] = sin(x[1])\n    J\nend\nfunction con2_h!(h, x, \u03bb)\n    # First constraint\n    h[1,1] += \u03bb[1]*2\n    h[2,2] += \u03bb[1]*2\n    # Second constraint\n    h[1,1] += \u03bb[2]*x[2]*-sin(x[1])\n    h[1,2] += \u03bb[2]*cos(x[1])\n    # Symmetrize h\n    h[2,1]  = h[1,2]\n    h\nend;  We generate the constraint objects and call  IPNewton  with initial guess $x_0 = (0.25,0.25)$.  x0 = [0.25, 0.25]\nlc = [-Inf, 0.0]; uc = [0.5^2, 0.0]\ndfc = TwiceDifferentiableConstraints(con2_c!, con2_jacobian!, con2_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())  Results of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.25,0.25]\n * Minimizer: [-1.595442184049669e-19,-1.9465281383022617e-18, ...]\n * Minimum: 1.000000e+00\n * Iterations: 29\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 6.90e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.38e-09 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 2.00e+00\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 215\n * Gradient Calls: 215", 
            "title": "Multiple constraints"
        }, 
        {
            "location": "/examples/generated/ipnewton_basics/#plain-program", 
            "text": "Below follows a version of the program without any comments. The file is also available here:  ipnewton_basics.jl  using Optim, NLSolversBase #hide\nimport NLSolversBase: clear! #hide\n\nfun(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n\nfunction fun_grad!(g, x)\ng[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\ng[2] = 200.0 * (x[2] - x[1]^2)\nend\n\nfunction fun_hess!(h, x)\nh[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2\nh[1, 2] = -400.0 * x[1]\nh[2, 1] = -400.0 * x[1]\nh[2, 2] = 200.0\nend;\n\nx0 = [0.0, 0.0]\ndf = TwiceDifferentiable(fun, fun_grad!, fun_hess!, x0)\n\nlx = [-0.5, -0.5]; ux = [0.5, 0.5]\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nres = optimize(df, dfc, x0, IPNewton())\n\nux = fill(Inf, 2)\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\nlx = fill(-Inf, 2); ux = fill(Inf, 2)\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\nlx = Float64[]; ux = Float64[]\ndfc = TwiceDifferentiableConstraints(lx, ux)\n\nclear!(df)\nres = optimize(df, dfc, x0, IPNewton())\n\ncon_c!(c, x) = (c[1] = x[1]^2 + x[2]^2; c)\nfunction con_jacobian!(J, x)\n    J[1,1] = 2*x[1]\n    J[1,2] = 2*x[2]\n    J\nend\nfunction con_h!(h, x, \u03bb)\n    h[1,1] += \u03bb[1]*2\n    h[2,2] += \u03bb[1]*2\nend;\n\nlx = Float64[]; ux = Float64[]\nlc = [-Inf]; uc = [0.5^2]\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\nlc = [0.1^2]\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\nfunction con2_c!(c, x)\n    c[1] = x[1]^2 + x[2]^2     ## First constraint\n    c[2] = x[2]*sin(x[1])-x[1] ## Second constraint\n    c\nend\nfunction con2_jacobian!(J, x)\n    # First constraint\n    J[1,1] = 2*x[1]\n    J[1,2] = 2*x[2]\n    # Second constraint\n    J[2,1] = x[2]*cos(x[1])-1.0\n    J[2,2] = sin(x[1])\n    J\nend\nfunction con2_h!(h, x, \u03bb)\n    # First constraint\n    h[1,1] += \u03bb[1]*2\n    h[2,2] += \u03bb[1]*2\n    # Second constraint\n    h[1,1] += \u03bb[2]*x[2]*-sin(x[1])\n    h[1,2] += \u03bb[2]*cos(x[1])\n    # Symmetrize h\n    h[2,1]  = h[1,2]\n    h\nend;\n\nx0 = [0.25, 0.25]\nlc = [-Inf, 0.0]; uc = [0.5^2, 0.0]\ndfc = TwiceDifferentiableConstraints(con2_c!, con2_jacobian!, con2_h!,\n                                     lx, ux, lc, uc)\nres = optimize(df, dfc, x0, IPNewton())\n\n# This file was generated using Literate.jl, https://github.com/fredrikekre/Literate.jl  This page was generated using  Literate.jl .", 
            "title": "Plain Program"
        }, 
        {
            "location": "/examples/generated/maxlikenlm/", 
            "text": "Maximum Likelihood Estimation: The Normal Linear Model\n\n\n\n\nTip\n\n\nThis example is also available as a Jupyter notebook: \nmaxlikenlm.ipynb\n\n\n\n\nThe following tutorial will introduce maximum likelihood estimation in Julia for the normal linear model.\n\n\nThe normal linear model (sometimes referred to as the OLS model) is the workhorse of regression modeling and is utilized across a number of diverse fields.  In this tutorial, we will utilize simulated data to demonstrate how Julia can be used to recover the parameters of interest.\n\n\nThe first order of business is to use the \nOptim\n package and also include the \nNLSolversBase\n routine:\n\n\nusing Optim, NLSolversBase, Random\nusing LinearAlgebra: diag\nRandom.seed!(0);                            # Fix random seed generator for reproducibility\n\n\n\n\n\n\nTip\n\n\nAdd Optim with the following command at the Julia command prompt: \nPkg.add(\"Optim\")\n\n\n\n\nThe first item that needs to be addressed is the data generating process or DGP. The following code will produce data from a nomral linear model:\n\n\nn = 500                             # Number of observations\nnvar = 2                            # Number of variables\n\u03b2 = ones(nvar) * 3.0                # True coefficients\nx = [ones(n) randn(n, nvar - 1)]    # X matrix of explanatory variables plus constant\n\u03b5 = randn(n) * 0.5                  # Error variance\ny = x * \u03b2 + \u03b5;                      # Generate Data\n\n\n\n\nIn the above example, we have 500 observations, 2 explanatory variables plus an intercept, an error variance equal to 0.5, coefficients equal to 3.0, and all of these are subject to change by the user. Since we know the true value of these parameters, we should obtain these values when we maximize the likelihood function.\n\n\nThe next step in our tutorial is to define a Julia function for the likelihood function. The following function defines the likelihood function for the normal linear model:\n\n\nfunction Log_Likelihood(X, Y, \u03b2, log_\u03c3)\n    \u03c3 = exp(log_\u03c3)\n    llike = -n/2*log(2\u03c0) - n/2* log(\u03c3^2) - (sum((Y - X * \u03b2).^2) / (2\u03c3^2))\n    llike = -llike\nend\n\n\n\n\nLog_Likelihood (generic function with 1 method)\n\n\n\n\nThe log likelihood function accepts 4 inputs: the matrix of explanatory variables (X), the dependent variable (Y), the \u03b2's, and the error varicance. Note that we exponentiate the error variance in the second line of the code because the error variance cannot be negative and we want to avoid this situation when maximizing the likelihood.\n\n\nThe next step in our tutorial is to optimize our function. We first use the \nTwiceDifferentiable\n command in order to obtain the Hessian matrix later on, which will be used to help form t-statistics:\n\n\nfunc = TwiceDifferentiable(vars -\n Log_Likelihood(x, y, vars[1:nvar], vars[nvar + 1]),\n                           ones(nvar+1); autodiff=:forward);\n\n\n\n\nThe above statment accepts 4 inputs: the x matrix, the dependent variable y, and a vector of \u03b2's and the error variance.  The \nvars[1:nvar]\n is how we pass the vector of \u03b2's and the \nvars[nvar + 1]\n is how we pass the error variance. You can think of this as a vector of parameters with the first 2 being \u03b2's and the last one is the error variance.\n\n\nThe \nones(nvar+1)\n are the starting values for the parameters and the \nautodiff=:forward\n command performs forward mode automatic differentiation.\n\n\nThe actual optimization of the likelihood function is accomplished with the following command:\n\n\nopt = optimize(func, ones(nvar+1))\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: Newton's Method\n * Starting Point: [1.0,1.0,1.0]\n * Minimizer: [3.002788633849947,2.964549617572727, ...]\n * Minimum: 3.851229e+02\n * Iterations: 7\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 2.62e-08\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.33e-15 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 3.41e-13\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 31\n * Gradient Calls: 31\n * Hessian Calls: 7\n\n\n\n\nThe first input to the command is the function we wish to optimize and the second input are the starting values.\n\n\nAfter a brief period of time, you should see output of the optimization routine, with the parameter  estimates being very close to our simulated values.\n\n\nThe optimization routine stores several quantities and we can obtain the maximim likelihood estimates with the following command:\n\n\nparameters = Optim.minimizer(opt)\n\n\n\n\n3-element Array{Float64,1}:\n  3.002788633849947\n  2.964549617572727\n -0.648692780562844\n\n\n\n\n!!! Note     Fieldnames for all of the quantities can be obtained with the following command:     fieldnames(opt)\n\n\nSince we paramaterized our likelihood to use the exponentiated value, we need to exponentiate it to get back to our original log scale:\n\n\nparameters[nvar+1] = exp(parameters[nvar+1])\n\n\n\n\n0.5227286513837306\n\n\n\n\nIn order to obtain the correct Hessian matrix, we have to \"push\" the actual parameter values that maximizes the likelihood function since the \nTwiceDifferentiable\n command uses the next to last values to calculate the Hessian:\n\n\nnumerical_hessian = hessian!(func,parameters)\n\n\n\n\n3\u00d73 Array{Float64,2}:\n 175.766        -12.0877        5.67464e-14\n -12.0877       182.437         5.88344e-15\n   5.67464e-14    5.88344e-15  96.0542\n\n\n\n\nWe can now invert our Hessian matrix to obtain the variance-covariance matrix:\n\n\nvar_cov_matrix = inv(numerical_hessian)\n\n\n\n\n3\u00d73 Array{Float64,2}:\n  0.00571544    0.000378687  -3.39973e-18\n  0.000378687   0.00550643   -5.60994e-19\n -3.39973e-18  -5.60994e-19   0.0104108\n\n\n\n\nIn this example, we are only interested in the statistical significance of the coefficient estimates so we obtain those with the following command:\n\n\n\u03b2 = parameters[1:nvar]\n\n\n\n\n2-element Array{Float64,1}:\n 3.002788633849947\n 2.964549617572727\n\n\n\n\nWe now need to obtain those elements of the variance-covariance matrix needed to obtain our t-statistics, and we can do this with the following commands:\n\n\ntemp = diag(var_cov_matrix)\ntemp1 = temp[1:nvar]\n\n\n\n\n2-element Array{Float64,1}:\n 0.005715441191953887\n 0.005506430533329349\n\n\n\n\nThe t-statistics are formed by dividing element-by-element the coefficients by their standard errors, or the square root of the diagonal elements of the variance-covariance matrix:\n\n\nt_stats = \u03b2./sqrt.(temp1)\n\n\n\n\n2-element Array{Float64,1}:\n 39.719144251161474\n 39.95063081460274\n\n\n\n\nFrom here, one may examine other statistics of interest using the output from the optimization routine.\n\n\n\n\nPlain Program\n\n\nBelow follows a version of the program without any comments. The file is also available here: \nmaxlikenlm.jl\n\n\nusing Optim, NLSolversBase, Random\nusing LinearAlgebra: diag\nRandom.seed!(0);                            # Fix random seed generator for reproducibility\n\nn = 500                             # Number of observations\nnvar = 2                            # Number of variables\n\u03b2 = ones(nvar) * 3.0                # True coefficients\nx = [ones(n) randn(n, nvar - 1)]    # X matrix of explanatory variables plus constant\n\u03b5 = randn(n) * 0.5                  # Error variance\ny = x * \u03b2 + \u03b5;                      # Generate Data\n\nfunction Log_Likelihood(X, Y, \u03b2, log_\u03c3)\n    \u03c3 = exp(log_\u03c3)\n    llike = -n/2*log(2\u03c0) - n/2* log(\u03c3^2) - (sum((Y - X * \u03b2).^2) / (2\u03c3^2))\n    llike = -llike\nend\n\nfunc = TwiceDifferentiable(vars -\n Log_Likelihood(x, y, vars[1:nvar], vars[nvar + 1]),\n                           ones(nvar+1); autodiff=:forward);\n\nopt = optimize(func, ones(nvar+1))\n\nparameters = Optim.minimizer(opt)\n\nparameters[nvar+1] = exp(parameters[nvar+1])\n\nnumerical_hessian = hessian!(func,parameters)\n\nvar_cov_matrix = inv(numerical_hessian)\n\n\u03b2 = parameters[1:nvar]\n\ntemp = diag(var_cov_matrix)\ntemp1 = temp[1:nvar]\n\nt_stats = \u03b2./sqrt.(temp1)\n\n# This file was generated using Literate.jl, https://github.com/fredrikekre/Literate.jl\n\n\n\n\nThis page was generated using \nLiterate.jl\n.", 
            "title": "Maximum likelihood estimation"
        }, 
        {
            "location": "/examples/generated/maxlikenlm/#maximum-likelihood-estimation-the-normal-linear-model", 
            "text": "Tip  This example is also available as a Jupyter notebook:  maxlikenlm.ipynb   The following tutorial will introduce maximum likelihood estimation in Julia for the normal linear model.  The normal linear model (sometimes referred to as the OLS model) is the workhorse of regression modeling and is utilized across a number of diverse fields.  In this tutorial, we will utilize simulated data to demonstrate how Julia can be used to recover the parameters of interest.  The first order of business is to use the  Optim  package and also include the  NLSolversBase  routine:  using Optim, NLSolversBase, Random\nusing LinearAlgebra: diag\nRandom.seed!(0);                            # Fix random seed generator for reproducibility   Tip  Add Optim with the following command at the Julia command prompt:  Pkg.add(\"Optim\")   The first item that needs to be addressed is the data generating process or DGP. The following code will produce data from a nomral linear model:  n = 500                             # Number of observations\nnvar = 2                            # Number of variables\n\u03b2 = ones(nvar) * 3.0                # True coefficients\nx = [ones(n) randn(n, nvar - 1)]    # X matrix of explanatory variables plus constant\n\u03b5 = randn(n) * 0.5                  # Error variance\ny = x * \u03b2 + \u03b5;                      # Generate Data  In the above example, we have 500 observations, 2 explanatory variables plus an intercept, an error variance equal to 0.5, coefficients equal to 3.0, and all of these are subject to change by the user. Since we know the true value of these parameters, we should obtain these values when we maximize the likelihood function.  The next step in our tutorial is to define a Julia function for the likelihood function. The following function defines the likelihood function for the normal linear model:  function Log_Likelihood(X, Y, \u03b2, log_\u03c3)\n    \u03c3 = exp(log_\u03c3)\n    llike = -n/2*log(2\u03c0) - n/2* log(\u03c3^2) - (sum((Y - X * \u03b2).^2) / (2\u03c3^2))\n    llike = -llike\nend  Log_Likelihood (generic function with 1 method)  The log likelihood function accepts 4 inputs: the matrix of explanatory variables (X), the dependent variable (Y), the \u03b2's, and the error varicance. Note that we exponentiate the error variance in the second line of the code because the error variance cannot be negative and we want to avoid this situation when maximizing the likelihood.  The next step in our tutorial is to optimize our function. We first use the  TwiceDifferentiable  command in order to obtain the Hessian matrix later on, which will be used to help form t-statistics:  func = TwiceDifferentiable(vars -  Log_Likelihood(x, y, vars[1:nvar], vars[nvar + 1]),\n                           ones(nvar+1); autodiff=:forward);  The above statment accepts 4 inputs: the x matrix, the dependent variable y, and a vector of \u03b2's and the error variance.  The  vars[1:nvar]  is how we pass the vector of \u03b2's and the  vars[nvar + 1]  is how we pass the error variance. You can think of this as a vector of parameters with the first 2 being \u03b2's and the last one is the error variance.  The  ones(nvar+1)  are the starting values for the parameters and the  autodiff=:forward  command performs forward mode automatic differentiation.  The actual optimization of the likelihood function is accomplished with the following command:  opt = optimize(func, ones(nvar+1))  Results of Optimization Algorithm\n * Algorithm: Newton's Method\n * Starting Point: [1.0,1.0,1.0]\n * Minimizer: [3.002788633849947,2.964549617572727, ...]\n * Minimum: 3.851229e+02\n * Iterations: 7\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 2.62e-08\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.33e-15 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 3.41e-13\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 31\n * Gradient Calls: 31\n * Hessian Calls: 7  The first input to the command is the function we wish to optimize and the second input are the starting values.  After a brief period of time, you should see output of the optimization routine, with the parameter  estimates being very close to our simulated values.  The optimization routine stores several quantities and we can obtain the maximim likelihood estimates with the following command:  parameters = Optim.minimizer(opt)  3-element Array{Float64,1}:\n  3.002788633849947\n  2.964549617572727\n -0.648692780562844  !!! Note     Fieldnames for all of the quantities can be obtained with the following command:     fieldnames(opt)  Since we paramaterized our likelihood to use the exponentiated value, we need to exponentiate it to get back to our original log scale:  parameters[nvar+1] = exp(parameters[nvar+1])  0.5227286513837306  In order to obtain the correct Hessian matrix, we have to \"push\" the actual parameter values that maximizes the likelihood function since the  TwiceDifferentiable  command uses the next to last values to calculate the Hessian:  numerical_hessian = hessian!(func,parameters)  3\u00d73 Array{Float64,2}:\n 175.766        -12.0877        5.67464e-14\n -12.0877       182.437         5.88344e-15\n   5.67464e-14    5.88344e-15  96.0542  We can now invert our Hessian matrix to obtain the variance-covariance matrix:  var_cov_matrix = inv(numerical_hessian)  3\u00d73 Array{Float64,2}:\n  0.00571544    0.000378687  -3.39973e-18\n  0.000378687   0.00550643   -5.60994e-19\n -3.39973e-18  -5.60994e-19   0.0104108  In this example, we are only interested in the statistical significance of the coefficient estimates so we obtain those with the following command:  \u03b2 = parameters[1:nvar]  2-element Array{Float64,1}:\n 3.002788633849947\n 2.964549617572727  We now need to obtain those elements of the variance-covariance matrix needed to obtain our t-statistics, and we can do this with the following commands:  temp = diag(var_cov_matrix)\ntemp1 = temp[1:nvar]  2-element Array{Float64,1}:\n 0.005715441191953887\n 0.005506430533329349  The t-statistics are formed by dividing element-by-element the coefficients by their standard errors, or the square root of the diagonal elements of the variance-covariance matrix:  t_stats = \u03b2./sqrt.(temp1)  2-element Array{Float64,1}:\n 39.719144251161474\n 39.95063081460274  From here, one may examine other statistics of interest using the output from the optimization routine.", 
            "title": "Maximum Likelihood Estimation: The Normal Linear Model"
        }, 
        {
            "location": "/examples/generated/maxlikenlm/#plain-program", 
            "text": "Below follows a version of the program without any comments. The file is also available here:  maxlikenlm.jl  using Optim, NLSolversBase, Random\nusing LinearAlgebra: diag\nRandom.seed!(0);                            # Fix random seed generator for reproducibility\n\nn = 500                             # Number of observations\nnvar = 2                            # Number of variables\n\u03b2 = ones(nvar) * 3.0                # True coefficients\nx = [ones(n) randn(n, nvar - 1)]    # X matrix of explanatory variables plus constant\n\u03b5 = randn(n) * 0.5                  # Error variance\ny = x * \u03b2 + \u03b5;                      # Generate Data\n\nfunction Log_Likelihood(X, Y, \u03b2, log_\u03c3)\n    \u03c3 = exp(log_\u03c3)\n    llike = -n/2*log(2\u03c0) - n/2* log(\u03c3^2) - (sum((Y - X * \u03b2).^2) / (2\u03c3^2))\n    llike = -llike\nend\n\nfunc = TwiceDifferentiable(vars -  Log_Likelihood(x, y, vars[1:nvar], vars[nvar + 1]),\n                           ones(nvar+1); autodiff=:forward);\n\nopt = optimize(func, ones(nvar+1))\n\nparameters = Optim.minimizer(opt)\n\nparameters[nvar+1] = exp(parameters[nvar+1])\n\nnumerical_hessian = hessian!(func,parameters)\n\nvar_cov_matrix = inv(numerical_hessian)\n\n\u03b2 = parameters[1:nvar]\n\ntemp = diag(var_cov_matrix)\ntemp1 = temp[1:nvar]\n\nt_stats = \u03b2./sqrt.(temp1)\n\n# This file was generated using Literate.jl, https://github.com/fredrikekre/Literate.jl  This page was generated using  Literate.jl .", 
            "title": "Plain Program"
        }, 
        {
            "location": "/examples/generated/rasch/", 
            "text": "Conditional Maximum Likelihood for the Rasch Model\n\n\n\n\nTip\n\n\nThis example is also available as a Jupyter notebook: \n$rasch.ipynb$\n\n\n\n\nThe Rasch model is used in psychometrics as a model for assessment data such as student responses to a standardized test. Let $X_{pi}$ be the response accuracy of student $p$ to item $i$ where $X_{pi}=1$ if the item was answered correctly and $X_{pi}=0$ otherwise for $p=1,\\ldots,n$ and $i=1,\\ldots,m$. The model for this accuracy is\n\n\n\n\n\n  P(\\mathbf{X}_{p}=\\mathbf{x}_{p}|\\xi_p, \\mathbf\\epsilon) = \\prod_{i=1}^m \\dfrac{(\\xi_p \\epsilon_j)^{x_{pi}}}{1 + \\xi_p\\epsilon_i}\n\n\n\n\n\nwhere $\\xi_p \n 0$ the latent ability of person $p$ and $\\epsilon_i \n 0$ is the difficulty of item $i$.\n\n\nWe simulate data from this model:\n\n\nRandom.seed!(123)\nn = 1000\nm = 5\ntheta = randn(n)\ndelta = randn(m)\nr = zeros(n)\ns = zeros(m)\n\nfor i in 1:n\n  p = exp.(theta[i] .- delta) ./ (1.0 .+ exp.(theta[i] .- delta))\n  for j in 1:m\n    if rand() \n p[j] ##correct\n      r[i] += 1\n      s[j] += 1\n    end\n  end\nend\nf = [sum(r.==j) for j in 1:m];\n\n\n\n\nSince the number of parameters increases with sample size standard maximum likelihood will not provide us consistent estimates. Instead we consider the conditional likelihood. It can be shown that the Rasch model is an exponential family model and that the sum score $r_p = \\sum_{i} x_{pi}$ is the sufficient statistic for $\\xi_p$. If we condition on the sum score we should be able to eliminate $\\xi_p$. Indeed, with a bit of algebra we can show\n\n\n\n\n\nP(\\mathbf{X}_p = \\mathbf{x}_p | r_p, \\mathbf\\epsilon) = \\dfrac{\\prod_{i=1}^m \\epsilon_i^{x{ij}}}{\\gamma_{r_i}(\\mathbf\\epsilon)}\n\n\n\n\n\nwhere $\\gamma_r(\\mathbf\\epsilon)$ is the elementary symmetric function of order $r$\n\n\n\n\n\n\\gamma_r(\\mathbf\\epsilon) = \\sum_{\\mathbf{y} : \\mathbf{1}^\\intercal \\mathbf{y} = r} \\prod_{j=1}^m \\epsilon_j^{y_j}\n\n\n\n\n\nwhere the sum is over all possible answer configurations that give a sum score of $r$. Algorithms to efficiently compute $\\gamma$ and its derivatives are available in the literature (see eg Baker (1996) for a review and Biscarri (2018) for a more modern approach)\n\n\nfunction esf_sum!(S::AbstractArray{T,1}, x::AbstractArray{T,1}) where T \n: Real\n  n = length(x)\n  fill!(S,zero(T))\n  S[1] = one(T)\n  @inbounds for col in 1:n\n    for r in 1:col\n      row = col - r + 1\n      S[row+1] = S[row+1] + x[col] * S[row]\n    end\n  end\nend\n\nfunction esf_ext!(S::AbstractArray{T,1}, H::AbstractArray{T,3}, x::AbstractArray{T,1}) where T \n: Real\n  n = length(x)\n  esf_sum!(S, x)\n  H[:,:,1] .= zero(T)\n  H[:,:,2] .= one(T)\n\n  @inbounds for i in 3:n+1\n    for j in 1:n\n      H[j,j,i] = S[i-1] - x[j] * H[j,j,i-1]\n      for k in j+1:n\n        H[k,j,i] = S[i-1] - ((x[j]+x[k])*H[k,j,i-1] + x[j]*x[k]*H[k,j,i-2])\n        H[j,k,i] = H[k,j,i]\n      end\n    end\n  end\nend\n\n\n\n\nesf_ext! (generic function with 1 method)\n\n\n\n\nThe objective function we want to minimize is the negative log conditional likelihood\n\n\n\n\n\n\\begin{aligned}\n\\log{L_C(\\mathbf\\epsilon|\\mathbf{r})} &= \\sum_{p=1}^n \\sum_{i=1}^m x_{pi} \\log{\\epsilon_i} - \\log{\\gamma_{r_p}(\\mathbf\\epsilon)}\\\\\n  &= \\sum_{i=1}^m s_i \\log{\\epsilon_i} - \\sum_{r=1}^m f_r \\log{\\gamma_r(\\mathbf\\epsilon)}\n\\end{aligned}\n\n\n\n\n\n\u03f5 = ones(Float64, m)\n\u03b20 = zeros(Float64, m)\nlast_\u03b2 = fill(NaN, m)\nS = zeros(Float64, m+1)\nH = zeros(Float64, m, m, m+1)\n\nfunction calculate_common!(x, last_x)\n  if x != last_x\n    copyto!(last_x, x)\n    \u03f5 .= exp.(-x)\n    esf_ext!(S, H, \u03f5)\n  end\nend\nfunction neglogLC(\u03b2)\n  calculate_common!(\u03b2, last_\u03b2)\n  return -s'log.(\u03f5) + f'log.(S[2:end])\nend\n\n\n\n\nneglogLC (generic function with 1 method)\n\n\n\n\nParameter estimation is usually performed with respect to the unconstrained parameter $\\beta_i = -\\log{\\epsilon_i}$. Taking the derivative with respect to $\\beta_i$ (and applying the chain rule) one obtains\n\n\n\n\n\n  \\dfrac{\\partial\\log L_C(\\mathbf\\epsilon|\\mathbf{r})}{\\partial \\beta_i} = -s_i + \\epsilon_i\\sum_{r=1}^m \\dfrac{f_r \\gamma_{r-1}^{(j)}}{\\gamma_r}\n\n\n\n\n\nwhere $\\gamma_{r-1}^{(i)} = \\partial \\gamma_{r}(\\mathbf\\epsilon)/\\partial\\epsilon_i$.\n\n\nfunction g!(storage, \u03b2)\n  calculate_common!(\u03b2, last_\u03b2)\n  for j in 1:m\n    storage[j] = s[j]\n    for l in 1:m\n      storage[j] -= \u03f5[j] * f[l] * (H[j,j,l+1] / S[l+1])\n    end\n  end\nend\n\n\n\n\ng! (generic function with 1 method)\n\n\n\n\nSimilarly the Hessian matrix can be computed\n\n\n$$\n  \\dfrac{\\partial^2 \\log L_C(\\mathbf\\epsilon|\\mathbf{r})}{\\partial \\beta_i\\partial\\beta_j} = \n\\begin{cases} \\displaystyle  -\\epsilon_i \\sum_{r=1}^m \\dfrac{f_r\\gamma_{r-1}^{(i)}}{\\gamma_r}\\left(1 - \\dfrac{\\gamma_{r-1}^{(i)}}{\\gamma_r}\\right) & \\text{if $i=j$}\\\\\n    \\displaystyle -\\epsilon_i\\epsilon_j\\sum_{r=1}^m \\dfrac{f_r \\gamma_{r-2}^{(i,j)}}{\\gamma_r} - \\dfrac{f_r\\gamma_{r-1}^{(i)}\\gamma_{r-1}^{(j)}}{\\gamma_r^2} &\\text{if $i\\neq j$}\n   \\end{cases}\n\n$$\n\n\nwhere $\\gamma_{r-2}^{(i,j)} = \\partial^2 \\gamma_{r}(\\mathbf\\epsilon)/\\partial\\epsilon_i\\partial\\epsilon_j$.\n\n\nfunction h!(storage, \u03b2)\n  calculate_common!(\u03b2, last_\u03b2)\n  for j in 1:m\n    for k in 1:m\n      storage[k,j] = 0.0\n      for l in 1:m\n        if j == k\n          storage[j,j] += f[l] * (\u03f5[j]*H[j,j,l+1] / S[l+1]) *\n            (1 - \u03f5[j]*H[j,j,l+1] / S[l+1])\n        elseif k \n j\n          storage[k,j] += \u03f5[j] * \u03f5[k] * f[l] *\n            ((H[k,j,l] / S[l+1]) - (H[j,j,l+1] * H[k,k,l+1]) / S[l+1] ^ 2)\n        else #k \n j\n          storage[k,j] += \u03f5[j] * \u03f5[k] * f[l] *\n            ((H[j,k,l] / S[l+1]) - (H[j,j,l+1] * H[k,k,l+1]) / S[l+1] ^ 2)\n        end\n      end\n    end\n  end\nend\n\n\n\n\nh! (generic function with 1 method)\n\n\n\n\nThe estimates of the item parameters are then obtained via standard optimization algorithms (either Newton-Raphson or L-BFGS). One last issue is that the model is not identifiable (multiplying the $\\xi_p$ by a constant and dividing the $\\epsilon_i$ by the same constant results in the same likelihood). Therefore some kind of constraint must be imposed when estimating the parameters. Typically either $\\epsilon_1 = 0$ or $\\prod_{i=1}^m \\epsilon_i = 1$ (which is equivalent to $\\sum_{i=1}^m \\beta_i = 0$).\n\n\ncon_c!(c, x) = (c[1] = sum(x); c)\nfunction con_jacobian!(J, x)\n  J[1,:] .= ones(length(x))\nend\nfunction con_h!(h, x, \u03bb)\n    for i in 1:size(h)[1]\n        for j in 1:size(h)[2]\n            h[i,j] += (i == j) ? \u03bb[1] : 0.0\n        end\n    end\nend\nlx = Float64[]; ux = Float64[]\nlc = [0.0]; uc = [0.0]\ndf = TwiceDifferentiable(neglogLC, g!, h!, \u03b20)\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!, lx, ux, lc, uc)\nres = optimize(df, dfc, \u03b20, IPNewton())\n\n\n\n\nResults of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0,0.0,0.0,0.0]\n * Minimizer: [1.4801515639961125,0.88023072153977, ...]\n * Minimum: 1.302751e+03\n * Iterations: 26\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 2.67e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: true\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 4.67e-06\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 61\n * Gradient Calls: 61\n\n\n\n\nCompare the estimate to the truth\n\n\ndelta_hat = res.minimizer\n[delta delta_hat]\n\n\n\n\n5\u00d72 Array{Float64,2}:\n  1.14112    1.48015\n  0.597106   0.880231\n -1.30405   -0.981096\n -1.2566    -0.955468\n -0.706518  -0.423819\n\n\n\n\nThis page was generated using \nLiterate.jl\n.", 
            "title": "Conditional maximum likelihood estimation"
        }, 
        {
            "location": "/examples/generated/rasch/#conditional-maximum-likelihood-for-the-rasch-model", 
            "text": "Tip  This example is also available as a Jupyter notebook:  $rasch.ipynb$   The Rasch model is used in psychometrics as a model for assessment data such as student responses to a standardized test. Let $X_{pi}$ be the response accuracy of student $p$ to item $i$ where $X_{pi}=1$ if the item was answered correctly and $X_{pi}=0$ otherwise for $p=1,\\ldots,n$ and $i=1,\\ldots,m$. The model for this accuracy is   \n  P(\\mathbf{X}_{p}=\\mathbf{x}_{p}|\\xi_p, \\mathbf\\epsilon) = \\prod_{i=1}^m \\dfrac{(\\xi_p \\epsilon_j)^{x_{pi}}}{1 + \\xi_p\\epsilon_i}   where $\\xi_p   0$ the latent ability of person $p$ and $\\epsilon_i   0$ is the difficulty of item $i$.  We simulate data from this model:  Random.seed!(123)\nn = 1000\nm = 5\ntheta = randn(n)\ndelta = randn(m)\nr = zeros(n)\ns = zeros(m)\n\nfor i in 1:n\n  p = exp.(theta[i] .- delta) ./ (1.0 .+ exp.(theta[i] .- delta))\n  for j in 1:m\n    if rand()   p[j] ##correct\n      r[i] += 1\n      s[j] += 1\n    end\n  end\nend\nf = [sum(r.==j) for j in 1:m];  Since the number of parameters increases with sample size standard maximum likelihood will not provide us consistent estimates. Instead we consider the conditional likelihood. It can be shown that the Rasch model is an exponential family model and that the sum score $r_p = \\sum_{i} x_{pi}$ is the sufficient statistic for $\\xi_p$. If we condition on the sum score we should be able to eliminate $\\xi_p$. Indeed, with a bit of algebra we can show   \nP(\\mathbf{X}_p = \\mathbf{x}_p | r_p, \\mathbf\\epsilon) = \\dfrac{\\prod_{i=1}^m \\epsilon_i^{x{ij}}}{\\gamma_{r_i}(\\mathbf\\epsilon)}   where $\\gamma_r(\\mathbf\\epsilon)$ is the elementary symmetric function of order $r$   \n\\gamma_r(\\mathbf\\epsilon) = \\sum_{\\mathbf{y} : \\mathbf{1}^\\intercal \\mathbf{y} = r} \\prod_{j=1}^m \\epsilon_j^{y_j}   where the sum is over all possible answer configurations that give a sum score of $r$. Algorithms to efficiently compute $\\gamma$ and its derivatives are available in the literature (see eg Baker (1996) for a review and Biscarri (2018) for a more modern approach)  function esf_sum!(S::AbstractArray{T,1}, x::AbstractArray{T,1}) where T  : Real\n  n = length(x)\n  fill!(S,zero(T))\n  S[1] = one(T)\n  @inbounds for col in 1:n\n    for r in 1:col\n      row = col - r + 1\n      S[row+1] = S[row+1] + x[col] * S[row]\n    end\n  end\nend\n\nfunction esf_ext!(S::AbstractArray{T,1}, H::AbstractArray{T,3}, x::AbstractArray{T,1}) where T  : Real\n  n = length(x)\n  esf_sum!(S, x)\n  H[:,:,1] .= zero(T)\n  H[:,:,2] .= one(T)\n\n  @inbounds for i in 3:n+1\n    for j in 1:n\n      H[j,j,i] = S[i-1] - x[j] * H[j,j,i-1]\n      for k in j+1:n\n        H[k,j,i] = S[i-1] - ((x[j]+x[k])*H[k,j,i-1] + x[j]*x[k]*H[k,j,i-2])\n        H[j,k,i] = H[k,j,i]\n      end\n    end\n  end\nend  esf_ext! (generic function with 1 method)  The objective function we want to minimize is the negative log conditional likelihood   \n\\begin{aligned}\n\\log{L_C(\\mathbf\\epsilon|\\mathbf{r})} &= \\sum_{p=1}^n \\sum_{i=1}^m x_{pi} \\log{\\epsilon_i} - \\log{\\gamma_{r_p}(\\mathbf\\epsilon)}\\\\\n  &= \\sum_{i=1}^m s_i \\log{\\epsilon_i} - \\sum_{r=1}^m f_r \\log{\\gamma_r(\\mathbf\\epsilon)}\n\\end{aligned}   \u03f5 = ones(Float64, m)\n\u03b20 = zeros(Float64, m)\nlast_\u03b2 = fill(NaN, m)\nS = zeros(Float64, m+1)\nH = zeros(Float64, m, m, m+1)\n\nfunction calculate_common!(x, last_x)\n  if x != last_x\n    copyto!(last_x, x)\n    \u03f5 .= exp.(-x)\n    esf_ext!(S, H, \u03f5)\n  end\nend\nfunction neglogLC(\u03b2)\n  calculate_common!(\u03b2, last_\u03b2)\n  return -s'log.(\u03f5) + f'log.(S[2:end])\nend  neglogLC (generic function with 1 method)  Parameter estimation is usually performed with respect to the unconstrained parameter $\\beta_i = -\\log{\\epsilon_i}$. Taking the derivative with respect to $\\beta_i$ (and applying the chain rule) one obtains   \n  \\dfrac{\\partial\\log L_C(\\mathbf\\epsilon|\\mathbf{r})}{\\partial \\beta_i} = -s_i + \\epsilon_i\\sum_{r=1}^m \\dfrac{f_r \\gamma_{r-1}^{(j)}}{\\gamma_r}   where $\\gamma_{r-1}^{(i)} = \\partial \\gamma_{r}(\\mathbf\\epsilon)/\\partial\\epsilon_i$.  function g!(storage, \u03b2)\n  calculate_common!(\u03b2, last_\u03b2)\n  for j in 1:m\n    storage[j] = s[j]\n    for l in 1:m\n      storage[j] -= \u03f5[j] * f[l] * (H[j,j,l+1] / S[l+1])\n    end\n  end\nend  g! (generic function with 1 method)  Similarly the Hessian matrix can be computed  $$\n  \\dfrac{\\partial^2 \\log L_C(\\mathbf\\epsilon|\\mathbf{r})}{\\partial \\beta_i\\partial\\beta_j} =  \\begin{cases} \\displaystyle  -\\epsilon_i \\sum_{r=1}^m \\dfrac{f_r\\gamma_{r-1}^{(i)}}{\\gamma_r}\\left(1 - \\dfrac{\\gamma_{r-1}^{(i)}}{\\gamma_r}\\right) & \\text{if $i=j$}\\\\\n    \\displaystyle -\\epsilon_i\\epsilon_j\\sum_{r=1}^m \\dfrac{f_r \\gamma_{r-2}^{(i,j)}}{\\gamma_r} - \\dfrac{f_r\\gamma_{r-1}^{(i)}\\gamma_{r-1}^{(j)}}{\\gamma_r^2} &\\text{if $i\\neq j$}\n   \\end{cases} \n$$  where $\\gamma_{r-2}^{(i,j)} = \\partial^2 \\gamma_{r}(\\mathbf\\epsilon)/\\partial\\epsilon_i\\partial\\epsilon_j$.  function h!(storage, \u03b2)\n  calculate_common!(\u03b2, last_\u03b2)\n  for j in 1:m\n    for k in 1:m\n      storage[k,j] = 0.0\n      for l in 1:m\n        if j == k\n          storage[j,j] += f[l] * (\u03f5[j]*H[j,j,l+1] / S[l+1]) *\n            (1 - \u03f5[j]*H[j,j,l+1] / S[l+1])\n        elseif k   j\n          storage[k,j] += \u03f5[j] * \u03f5[k] * f[l] *\n            ((H[k,j,l] / S[l+1]) - (H[j,j,l+1] * H[k,k,l+1]) / S[l+1] ^ 2)\n        else #k   j\n          storage[k,j] += \u03f5[j] * \u03f5[k] * f[l] *\n            ((H[j,k,l] / S[l+1]) - (H[j,j,l+1] * H[k,k,l+1]) / S[l+1] ^ 2)\n        end\n      end\n    end\n  end\nend  h! (generic function with 1 method)  The estimates of the item parameters are then obtained via standard optimization algorithms (either Newton-Raphson or L-BFGS). One last issue is that the model is not identifiable (multiplying the $\\xi_p$ by a constant and dividing the $\\epsilon_i$ by the same constant results in the same likelihood). Therefore some kind of constraint must be imposed when estimating the parameters. Typically either $\\epsilon_1 = 0$ or $\\prod_{i=1}^m \\epsilon_i = 1$ (which is equivalent to $\\sum_{i=1}^m \\beta_i = 0$).  con_c!(c, x) = (c[1] = sum(x); c)\nfunction con_jacobian!(J, x)\n  J[1,:] .= ones(length(x))\nend\nfunction con_h!(h, x, \u03bb)\n    for i in 1:size(h)[1]\n        for j in 1:size(h)[2]\n            h[i,j] += (i == j) ? \u03bb[1] : 0.0\n        end\n    end\nend\nlx = Float64[]; ux = Float64[]\nlc = [0.0]; uc = [0.0]\ndf = TwiceDifferentiable(neglogLC, g!, h!, \u03b20)\ndfc = TwiceDifferentiableConstraints(con_c!, con_jacobian!, con_h!, lx, ux, lc, uc)\nres = optimize(df, dfc, \u03b20, IPNewton())  Results of Optimization Algorithm\n * Algorithm: Interior Point Newton\n * Starting Point: [0.0,0.0,0.0,0.0,0.0]\n * Minimizer: [1.4801515639961125,0.88023072153977, ...]\n * Minimum: 1.302751e+03\n * Iterations: 26\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 2.67e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: true\n     |f(x) - f(x')| = 0.00e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 4.67e-06\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 61\n * Gradient Calls: 61  Compare the estimate to the truth  delta_hat = res.minimizer\n[delta delta_hat]  5\u00d72 Array{Float64,2}:\n  1.14112    1.48015\n  0.597106   0.880231\n -1.30405   -0.981096\n -1.2566    -0.955468\n -0.706518  -0.423819  This page was generated using  Literate.jl .", 
            "title": "Conditional Maximum Likelihood for the Rasch Model"
        }, 
        {
            "location": "/algo/nelder_mead/", 
            "text": "Nelder-Mead\n\n\nNelder-Mead is currently the standard algorithm when no derivatives are provided.\n\n\n\n\nConstructor\n\n\nNelderMead(; parameters = AdaptiveParameters(),\n             initial_simplex = AffineSimplexer())\n\n\n\n\nThe keywords in the constructor are used to control the following parts of the solver:\n\n\n\n\nparameters\n is a an instance of either \nAdaptiveParameters\n or \nFixedParameters\n, and is\n\n\n\n\nused to generate parameters for the Nelder-Mead Algorithm.\n\n\n\n\ninitial_simplex\n is an instance of \nAffineSimplexer\n. See more\n\n\n\n\ndetails below.\n\n\n\n\nDescription\n\n\nOur current implementation of the Nelder-Mead algorithm is based on Nelder and Mead (1965) and Gao and Han (2010). Gradient free methods can be a bit sensitive to starting values and tuning parameters, so it is a good idea to be careful with the defaults provided in Optim.\n\n\nInstead of using gradient information, Nelder-Mead is a direct search method. It keeps track of the function value at a number of points in the search space. Together, the points form a simplex. Given a simplex, we can perform one of four actions: reflect, expand, contract, or shrink. Basically, the goal is to iteratively replace the worst point with a better point. More information can be found in Nelder and Mead (1965), Lagarias, et al (1998) or Gao and Han (2010).\n\n\nThe stopping rule is the same as in the original paper, and is the standard error of the function values at the vertices. To set the tolerance level for this convergence criterion, set the \ng_tol\n level as described in the Configurable Options section.\n\n\nWhen the solver finishes, we return a minimizer which is either the centroid or one of the vertices. The function value at the centroid adds a function evaluation, as we need to evaluate the objection at the centroid to choose the smallest function value. However, even if the function value at the centroid can be returned as the minimum, we do not trace it during the optimization iterations. This is to avoid too many evaluations of the objective function which can be computationally expensive. Typically, there should be no more than twice as many \nf_calls\n than \niterations\n.  Adding an evaluation at the centroid when tracing could considerably increase the total run-time of the algorithm.\n\n\n\n\nSpecifying the initial simplex\n\n\nThe default choice of \ninitial_simplex\n is \nAffineSimplexer()\n. A simplex is represented by an $(n+1)$-dimensional vector of $n$-dimensional vectors. It is used together  with the initial \nx\n to create the initial simplex. To construct the $i$th vertex, it simply multiplies entry $i$ in the initial vector with a constant \nb\n, and adds a constant \na\n. This means that the $i$th of the $n$ additional vertices is of the form\n\n\n\n\n\n(x_0^1, x_0^2, \\ldots, x_0^i, \\ldots, 0,0) + (0, 0, \\ldots, x_0^i\\cdot b+a,\\ldots, 0,0)\n\n\n\n\n\nIf an $x_0^i$ is zero, we need the $a$ to make sure all vertices are unique. Generally, it is advised to start with a relatively large simplex.\n\n\nIf a specific simplex is wanted, it is possible to construct the $(n+1)$-vector of $n$-dimensional vectors, and pass it to the solver using a new type definition and a new method for the function \nsimplexer\n. For example, let us minimize the two-dimensional Rosenbrock function, and choose three vertices that have elements that are simply standard uniform draws.\n\n\nusing Optim\nstruct MySimplexer \n: Optim.Simplexer end\nOptim.simplexer(S::MySimplexer, initial_x) = [rand(length(initial_x)) for i = 1:length(initial_x)+1]\nf(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\noptimize(f, [.0, .0], NelderMead(initial_simplex = MySimplexer()))\n\n\n\n\nSay we want to implement the initial simplex as in Matlab's \nfminsearch\n. This is very close to the \nAffineSimplexer\n above, but with a small twist. Instead of always adding the \na\n, a constant is only added to entries that are zero. If the entry is non-zero, five percent of the level is added. This might be implemented (by the user) as\n\n\nstruct MatlabSimplexer{T} \n: Optim.Simplexer\n    a::T\n    b::T\nend\nMatlabSimplexer(;a = 0.00025, b = 0.05) = MatlabSimplexer(a, b)\n\nfunction Optim.simplexer(A::MatlabSimplexer, initial_x::AbstractArray{T, N}) where {T, N}\n    n = length(initial_x)\n    initial_simplex = Array{T, N}[initial_x for i = 1:n+1]\n    for j = 1:n\n        initial_simplex[j+1][j] += initial_simplex[j+1][j] == zero(T) ? S.b * initial_simplex[j+1][j] : S.a\n    end\n    initial_simplex\nend\n\n\n\n\n\n\nThe parameters of Nelder-Mead\n\n\nThe different types of steps in the algorithm are governed by four parameters: $\\alpha$ for the reflection, $\\beta$ for the expansion, $\\gamma$ for the contraction, and $\\delta$ for the shrink step. We default to the adaptive parameters scheme in Gao and Han (2010). These are based on the dimensionality of the problem, and are given by\n\n\n\n\n\n\\alpha = 1, \\quad \\beta = 1+2/n,\\quad \\gamma =0.75 - 1/2n,\\quad \\delta = 1-1/n\n\n\n\n\n\nIt is also possible to specify the original parameters from Nelder and Mead (1965)\n\n\n\n\n\n\\alpha = 1,\\quad \\beta = 2, \\quad\\gamma = 1/2, \\quad\\delta = 1/2\n\n\n\n\n\nby specifying \nparameters  = Optim.FixedParameters()\n. For specifying custom values, \nparameters  = Optim.FixedParameters(\u03b1 = a, \u03b2 = b, \u03b3 = g, \u03b4 = d)\n is used, where a, b, g, d are the chosen values. If another parameter specification is wanted, it is possible to create a custom sub-type of\nOptim.NMParameters\n, and add a method to the \nparameters\n function. It should take the new type as the first positional argument, and the dimensionality of \nx\n as the second positional argument, and return a 4-tuple of parameters. However, it will often be easier to simply supply the wanted parameters to \nFixedParameters\n.\n\n\n\n\nReferences\n\n\nNelder, John A. and R. Mead (1965). \"A simplex method for function minimization\". Computer Journal 7: 308\u2013313. doi:10.1093/comjnl/7.4.308.\n\n\nLagarias, Jeffrey C., et al. \"Convergence properties of the Nelder\u2013Mead simplex method in low dimensions.\" SIAM Journal on optimization 9.1 (1998): 112-147.\n\n\nGao, Fuchang and Lixing Han (2010). \"Implementing the Nelder-Mead simplex algorithm with adaptive parameters\". Computational Optimization and Applications [DOI 10.1007/s10589-010-9329-3]", 
            "title": "Nelder Mead"
        }, 
        {
            "location": "/algo/nelder_mead/#nelder-mead", 
            "text": "Nelder-Mead is currently the standard algorithm when no derivatives are provided.", 
            "title": "Nelder-Mead"
        }, 
        {
            "location": "/algo/nelder_mead/#constructor", 
            "text": "NelderMead(; parameters = AdaptiveParameters(),\n             initial_simplex = AffineSimplexer())  The keywords in the constructor are used to control the following parts of the solver:   parameters  is a an instance of either  AdaptiveParameters  or  FixedParameters , and is   used to generate parameters for the Nelder-Mead Algorithm.   initial_simplex  is an instance of  AffineSimplexer . See more   details below.", 
            "title": "Constructor"
        }, 
        {
            "location": "/algo/nelder_mead/#description", 
            "text": "Our current implementation of the Nelder-Mead algorithm is based on Nelder and Mead (1965) and Gao and Han (2010). Gradient free methods can be a bit sensitive to starting values and tuning parameters, so it is a good idea to be careful with the defaults provided in Optim.  Instead of using gradient information, Nelder-Mead is a direct search method. It keeps track of the function value at a number of points in the search space. Together, the points form a simplex. Given a simplex, we can perform one of four actions: reflect, expand, contract, or shrink. Basically, the goal is to iteratively replace the worst point with a better point. More information can be found in Nelder and Mead (1965), Lagarias, et al (1998) or Gao and Han (2010).  The stopping rule is the same as in the original paper, and is the standard error of the function values at the vertices. To set the tolerance level for this convergence criterion, set the  g_tol  level as described in the Configurable Options section.  When the solver finishes, we return a minimizer which is either the centroid or one of the vertices. The function value at the centroid adds a function evaluation, as we need to evaluate the objection at the centroid to choose the smallest function value. However, even if the function value at the centroid can be returned as the minimum, we do not trace it during the optimization iterations. This is to avoid too many evaluations of the objective function which can be computationally expensive. Typically, there should be no more than twice as many  f_calls  than  iterations .  Adding an evaluation at the centroid when tracing could considerably increase the total run-time of the algorithm.", 
            "title": "Description"
        }, 
        {
            "location": "/algo/nelder_mead/#specifying-the-initial-simplex", 
            "text": "The default choice of  initial_simplex  is  AffineSimplexer() . A simplex is represented by an $(n+1)$-dimensional vector of $n$-dimensional vectors. It is used together  with the initial  x  to create the initial simplex. To construct the $i$th vertex, it simply multiplies entry $i$ in the initial vector with a constant  b , and adds a constant  a . This means that the $i$th of the $n$ additional vertices is of the form   \n(x_0^1, x_0^2, \\ldots, x_0^i, \\ldots, 0,0) + (0, 0, \\ldots, x_0^i\\cdot b+a,\\ldots, 0,0)   If an $x_0^i$ is zero, we need the $a$ to make sure all vertices are unique. Generally, it is advised to start with a relatively large simplex.  If a specific simplex is wanted, it is possible to construct the $(n+1)$-vector of $n$-dimensional vectors, and pass it to the solver using a new type definition and a new method for the function  simplexer . For example, let us minimize the two-dimensional Rosenbrock function, and choose three vertices that have elements that are simply standard uniform draws.  using Optim\nstruct MySimplexer  : Optim.Simplexer end\nOptim.simplexer(S::MySimplexer, initial_x) = [rand(length(initial_x)) for i = 1:length(initial_x)+1]\nf(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\noptimize(f, [.0, .0], NelderMead(initial_simplex = MySimplexer()))  Say we want to implement the initial simplex as in Matlab's  fminsearch . This is very close to the  AffineSimplexer  above, but with a small twist. Instead of always adding the  a , a constant is only added to entries that are zero. If the entry is non-zero, five percent of the level is added. This might be implemented (by the user) as  struct MatlabSimplexer{T}  : Optim.Simplexer\n    a::T\n    b::T\nend\nMatlabSimplexer(;a = 0.00025, b = 0.05) = MatlabSimplexer(a, b)\n\nfunction Optim.simplexer(A::MatlabSimplexer, initial_x::AbstractArray{T, N}) where {T, N}\n    n = length(initial_x)\n    initial_simplex = Array{T, N}[initial_x for i = 1:n+1]\n    for j = 1:n\n        initial_simplex[j+1][j] += initial_simplex[j+1][j] == zero(T) ? S.b * initial_simplex[j+1][j] : S.a\n    end\n    initial_simplex\nend", 
            "title": "Specifying the initial simplex"
        }, 
        {
            "location": "/algo/nelder_mead/#the-parameters-of-nelder-mead", 
            "text": "The different types of steps in the algorithm are governed by four parameters: $\\alpha$ for the reflection, $\\beta$ for the expansion, $\\gamma$ for the contraction, and $\\delta$ for the shrink step. We default to the adaptive parameters scheme in Gao and Han (2010). These are based on the dimensionality of the problem, and are given by   \n\\alpha = 1, \\quad \\beta = 1+2/n,\\quad \\gamma =0.75 - 1/2n,\\quad \\delta = 1-1/n   It is also possible to specify the original parameters from Nelder and Mead (1965)   \n\\alpha = 1,\\quad \\beta = 2, \\quad\\gamma = 1/2, \\quad\\delta = 1/2   by specifying  parameters  = Optim.FixedParameters() . For specifying custom values,  parameters  = Optim.FixedParameters(\u03b1 = a, \u03b2 = b, \u03b3 = g, \u03b4 = d)  is used, where a, b, g, d are the chosen values. If another parameter specification is wanted, it is possible to create a custom sub-type of Optim.NMParameters , and add a method to the  parameters  function. It should take the new type as the first positional argument, and the dimensionality of  x  as the second positional argument, and return a 4-tuple of parameters. However, it will often be easier to simply supply the wanted parameters to  FixedParameters .", 
            "title": "The parameters of Nelder-Mead"
        }, 
        {
            "location": "/algo/nelder_mead/#references", 
            "text": "Nelder, John A. and R. Mead (1965). \"A simplex method for function minimization\". Computer Journal 7: 308\u2013313. doi:10.1093/comjnl/7.4.308.  Lagarias, Jeffrey C., et al. \"Convergence properties of the Nelder\u2013Mead simplex method in low dimensions.\" SIAM Journal on optimization 9.1 (1998): 112-147.  Gao, Fuchang and Lixing Han (2010). \"Implementing the Nelder-Mead simplex algorithm with adaptive parameters\". Computational Optimization and Applications [DOI 10.1007/s10589-010-9329-3]", 
            "title": "References"
        }, 
        {
            "location": "/algo/simulated_annealing/", 
            "text": "Simulated Annealing\n\n\n\n\nConstructor\n\n\nSimulatedAnnealing(; neighbor = default_neighbor!,\n                    T = default_temperature,\n                    p = kirkpatrick)\n\n\n\n\nThe constructor takes three keywords:\n\n\n\n\nneighbor = a!(x_proposed, x_current)\n, a mutating function of the current x, and the proposed x\n\n\nT = b(iteration)\n, a function of the current iteration that returns a temperature\n\n\np = c(f_proposal, f_current, T)\n, a function of the current temperature, current function value and proposed function value that returns an acceptance probability\n\n\n\n\n\n\nDescription\n\n\nSimulated Annealing is a derivative free method for optimization. It is based on the Metropolis-Hastings algorithm that was originally used to generate samples from a thermodynamics system, and is often used to generate draws from a posterior when doing Bayesian inference. As such, it is a probabilistic method for finding the minimum of a function, often over a quite large domains. For the historical reasons given above, the algorithm uses terms such as cooling, temperature, and acceptance probabilities.\n\n\nAs the constructor shows, a simulated annealing implementation is characterized by a temperature, a neighbor function, and an acceptance probability. The temperature controls how volatile the changes in minimizer candidates are allowed to be, as it enters the acceptance probability. For example, the original Kirkpatrick et al. acceptance probability function can be written as follows\n\n\np(f_proposal, f_current, T) = exp(-(f_proposal - f_current)/T)\n\n\n\n\nA high temperature makes it more likely that a draw is accepted, by pushing acceptance probability to 1. As in the Metropolis-Hastings algorithm, we always accept a smaller function value, but we also sometimes accept a larger value. As the temperature decreases, we're more and more likely to only accept candidate \nx\n's that lowers the function value. To obtain a new \nf_proposal\n, we need a neighbor function. A simple neighbor function adds a standard normal draw to each dimension of \nx\n\n\nfunction neighbor!(x_proposal::Array, x::Array)\n    for i in eachindex(x)\n        x_proposal[i] = x[i]+randn()\n    end\nend\n\n\n\n\nAs we see, it is not really possible to disentangle the role of the different components of the algorithm. For example, both the functional form of the acceptance function, the temperature and (indirectly) the neighbor function determine if the next draw of \nx\n is accepted or not.\n\n\nThe current implementation of Simulated Annealing is very rough.  It lacks quite a few features which are normally part of a proper SA implementation. A better implementation is under way, see \nthis issue\n.\n\n\n\n\nExample\n\n\n\n\nReferences", 
            "title": "Simulated Annealing"
        }, 
        {
            "location": "/algo/simulated_annealing/#simulated-annealing", 
            "text": "", 
            "title": "Simulated Annealing"
        }, 
        {
            "location": "/algo/simulated_annealing/#constructor", 
            "text": "SimulatedAnnealing(; neighbor = default_neighbor!,\n                    T = default_temperature,\n                    p = kirkpatrick)  The constructor takes three keywords:   neighbor = a!(x_proposed, x_current) , a mutating function of the current x, and the proposed x  T = b(iteration) , a function of the current iteration that returns a temperature  p = c(f_proposal, f_current, T) , a function of the current temperature, current function value and proposed function value that returns an acceptance probability", 
            "title": "Constructor"
        }, 
        {
            "location": "/algo/simulated_annealing/#description", 
            "text": "Simulated Annealing is a derivative free method for optimization. It is based on the Metropolis-Hastings algorithm that was originally used to generate samples from a thermodynamics system, and is often used to generate draws from a posterior when doing Bayesian inference. As such, it is a probabilistic method for finding the minimum of a function, often over a quite large domains. For the historical reasons given above, the algorithm uses terms such as cooling, temperature, and acceptance probabilities.  As the constructor shows, a simulated annealing implementation is characterized by a temperature, a neighbor function, and an acceptance probability. The temperature controls how volatile the changes in minimizer candidates are allowed to be, as it enters the acceptance probability. For example, the original Kirkpatrick et al. acceptance probability function can be written as follows  p(f_proposal, f_current, T) = exp(-(f_proposal - f_current)/T)  A high temperature makes it more likely that a draw is accepted, by pushing acceptance probability to 1. As in the Metropolis-Hastings algorithm, we always accept a smaller function value, but we also sometimes accept a larger value. As the temperature decreases, we're more and more likely to only accept candidate  x 's that lowers the function value. To obtain a new  f_proposal , we need a neighbor function. A simple neighbor function adds a standard normal draw to each dimension of  x  function neighbor!(x_proposal::Array, x::Array)\n    for i in eachindex(x)\n        x_proposal[i] = x[i]+randn()\n    end\nend  As we see, it is not really possible to disentangle the role of the different components of the algorithm. For example, both the functional form of the acceptance function, the temperature and (indirectly) the neighbor function determine if the next draw of  x  is accepted or not.  The current implementation of Simulated Annealing is very rough.  It lacks quite a few features which are normally part of a proper SA implementation. A better implementation is under way, see  this issue .", 
            "title": "Description"
        }, 
        {
            "location": "/algo/simulated_annealing/#example", 
            "text": "", 
            "title": "Example"
        }, 
        {
            "location": "/algo/simulated_annealing/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/algo/samin/", 
            "text": "SAMIN\n\n\n\n\nConstructor\n\n\nSAMIN(; nt::Int = 5     # reduce temperature every nt*ns*dim(x_init) evaluations\n        ns::Int = 5     # adjust bounds every ns*dim(x_init) evaluations\n        rt::T = 0.9     # geometric temperature reduction factor: when temp changes, new temp is t=rt*t\n        neps::Int = 5   # number of previous best values the final result is compared to\n        f_tol::T = 1e-12 # the required tolerance level for function value comparisons\n        x_tol::T = 1e-6 # the required tolerance level for x\n        coverage_ok::Bool = false, # if false, increase temperature until initial parameter space is covered\n        verbosity::Int = 0) # scalar: 0, 1, 2 or 3 (default = 0).\n\n\n\n\n\n\nDescription\n\n\nThe \nSAMIN\n method implements the Simulated Annealing algorithm for problems with bounds constraints as described in Goffe et. al. (1994) and Goffe (1996). A key control parameter is rt, the geometric temperature reduction rate, which should be between zero and one. Setting rt lower will cause the algorithm to contract the search space more quickly, reducing the run time. Setting rt too low will cause the algorithm to narrow the search too quickly, and the true minimizer may be skipped over. If possible, run the algorithm multiple times to verify that the same solution is found each time. If this is not the case, increase rt. When in doubt, start with a conservative rt, for example, rt=0.95, and allow for a generous iteration limit. The algorithm requires lower and upper bounds on the parameters, although these bounds are often set rather wide, and are not necessarily meant to reflect constraints in the model, but rather bounds that enclose the parameter space. If the final \nx\ns are very close to the boundary (which can be checked by setting verbosity=1), it is a good idea to restart the optimizer with wider bounds, unless the bounds actually reflect hard constraints on \nx\n.\n\n\n\n\nExample\n\n\nThis example shows a successful minimization:\n\n\njulia\n using Optim, OptimTestProblems\n\njulia\n prob = OptimTestProblems.UnconstrainedProblems.examples[\nRosenbrock\n];\n\njulia\n res = Optim.optimize(prob.f, fill(-100.0, 2), fill(100.0, 2), prob.initial_x, SAMIN(), Optim.Options(iterations=10^6))\n================================================================================\nSAMIN results\n==\n Normal convergence \n==\ntotal number of objective function evaluations: 23701\n\n     Obj. value:      0.0000000000\n\n       parameter      search width\n         1.00000           0.00000\n         1.00000           0.00000\n================================================================================\n\nResults of Optimization Algorithm\n * Algorithm: SAMIN\n * Starting Point: [-1.2,1.0]\n * Minimizer: [0.9999999893140956,0.9999999765350857]\n * Minimum: 5.522977e-16\n * Iterations: 23701\n * Convergence: false\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = NaN\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = NaN |f(x)|\n   * |g(x)| \u2264 0.0e+00: false\n     |g(x)| = NaN\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 23701\n * Gradient Calls: 0\n\n\n\n\n\n\nExample\n\n\nThis example shows an unsuccessful minimization, because the cooling rate, rt=0.5, is too rapid:\n\n\njulia\n using Optim, OptimTestProblems\n\njulia\n prob = OptimTestProblems.UnconstrainedProblems.examples[\nRosenbrock\n];\njulia\n res = Optim.optimize(prob.f, fill(-100.0, 2), fill(100.0, 2), prob.initial_x, SAMIN(rt=0.5), Optim.Options(iterations=10^6))\n================================================================================\nSAMIN results\n==\n Normal convergence \n==\ntotal number of objective function evaluations: 12051\n\n     Obj. value:      0.0011613045\n\n       parameter      search width\n         0.96592           0.00000\n         0.93301           0.00000\n================================================================================\n\nResults of Optimization Algorithm\n * Algorithm: SAMIN\n * Starting Point: [-1.2,1.0]\n * Minimizer: [0.9659220825756248,0.9330054696322896]\n * Minimum: 1.161304e-03\n * Iterations: 12051\n * Convergence: false\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = NaN\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = NaN |f(x)|\n   * |g(x)| \u2264 0.0e+00: false\n     |g(x)| = NaN\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 12051\n * Gradient Calls: 0\n\n\n\n\n\n\n\nReferences\n\n\n\n\nGoffe, et. al. (1994) \"Global Optimization of Statistical Functions with Simulated Annealing\", Journal of Econometrics, V. 60, N. 1/2.\n\n\nGoffe, William L. (1996) \"SIMANN: A Global Optimization Algorithm using Simulated Annealing \" Studies in Nonlinear Dynamics \n Econometrics, Oct96, Vol. 1 Issue 3.", 
            "title": "Simulated Annealing w/ bounds"
        }, 
        {
            "location": "/algo/samin/#samin", 
            "text": "", 
            "title": "SAMIN"
        }, 
        {
            "location": "/algo/samin/#constructor", 
            "text": "SAMIN(; nt::Int = 5     # reduce temperature every nt*ns*dim(x_init) evaluations\n        ns::Int = 5     # adjust bounds every ns*dim(x_init) evaluations\n        rt::T = 0.9     # geometric temperature reduction factor: when temp changes, new temp is t=rt*t\n        neps::Int = 5   # number of previous best values the final result is compared to\n        f_tol::T = 1e-12 # the required tolerance level for function value comparisons\n        x_tol::T = 1e-6 # the required tolerance level for x\n        coverage_ok::Bool = false, # if false, increase temperature until initial parameter space is covered\n        verbosity::Int = 0) # scalar: 0, 1, 2 or 3 (default = 0).", 
            "title": "Constructor"
        }, 
        {
            "location": "/algo/samin/#description", 
            "text": "The  SAMIN  method implements the Simulated Annealing algorithm for problems with bounds constraints as described in Goffe et. al. (1994) and Goffe (1996). A key control parameter is rt, the geometric temperature reduction rate, which should be between zero and one. Setting rt lower will cause the algorithm to contract the search space more quickly, reducing the run time. Setting rt too low will cause the algorithm to narrow the search too quickly, and the true minimizer may be skipped over. If possible, run the algorithm multiple times to verify that the same solution is found each time. If this is not the case, increase rt. When in doubt, start with a conservative rt, for example, rt=0.95, and allow for a generous iteration limit. The algorithm requires lower and upper bounds on the parameters, although these bounds are often set rather wide, and are not necessarily meant to reflect constraints in the model, but rather bounds that enclose the parameter space. If the final  x s are very close to the boundary (which can be checked by setting verbosity=1), it is a good idea to restart the optimizer with wider bounds, unless the bounds actually reflect hard constraints on  x .", 
            "title": "Description"
        }, 
        {
            "location": "/algo/samin/#example", 
            "text": "This example shows a successful minimization:  julia  using Optim, OptimTestProblems\n\njulia  prob = OptimTestProblems.UnconstrainedProblems.examples[ Rosenbrock ];\n\njulia  res = Optim.optimize(prob.f, fill(-100.0, 2), fill(100.0, 2), prob.initial_x, SAMIN(), Optim.Options(iterations=10^6))\n================================================================================\nSAMIN results\n==  Normal convergence  ==\ntotal number of objective function evaluations: 23701\n\n     Obj. value:      0.0000000000\n\n       parameter      search width\n         1.00000           0.00000\n         1.00000           0.00000\n================================================================================\n\nResults of Optimization Algorithm\n * Algorithm: SAMIN\n * Starting Point: [-1.2,1.0]\n * Minimizer: [0.9999999893140956,0.9999999765350857]\n * Minimum: 5.522977e-16\n * Iterations: 23701\n * Convergence: false\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = NaN\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = NaN |f(x)|\n   * |g(x)| \u2264 0.0e+00: false\n     |g(x)| = NaN\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 23701\n * Gradient Calls: 0", 
            "title": "Example"
        }, 
        {
            "location": "/algo/samin/#example_1", 
            "text": "This example shows an unsuccessful minimization, because the cooling rate, rt=0.5, is too rapid:  julia  using Optim, OptimTestProblems\n\njulia  prob = OptimTestProblems.UnconstrainedProblems.examples[ Rosenbrock ];\njulia  res = Optim.optimize(prob.f, fill(-100.0, 2), fill(100.0, 2), prob.initial_x, SAMIN(rt=0.5), Optim.Options(iterations=10^6))\n================================================================================\nSAMIN results\n==  Normal convergence  ==\ntotal number of objective function evaluations: 12051\n\n     Obj. value:      0.0011613045\n\n       parameter      search width\n         0.96592           0.00000\n         0.93301           0.00000\n================================================================================\n\nResults of Optimization Algorithm\n * Algorithm: SAMIN\n * Starting Point: [-1.2,1.0]\n * Minimizer: [0.9659220825756248,0.9330054696322896]\n * Minimum: 1.161304e-03\n * Iterations: 12051\n * Convergence: false\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = NaN\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = NaN |f(x)|\n   * |g(x)| \u2264 0.0e+00: false\n     |g(x)| = NaN\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 12051\n * Gradient Calls: 0", 
            "title": "Example"
        }, 
        {
            "location": "/algo/samin/#references", 
            "text": "Goffe, et. al. (1994) \"Global Optimization of Statistical Functions with Simulated Annealing\", Journal of Econometrics, V. 60, N. 1/2.  Goffe, William L. (1996) \"SIMANN: A Global Optimization Algorithm using Simulated Annealing \" Studies in Nonlinear Dynamics   Econometrics, Oct96, Vol. 1 Issue 3.", 
            "title": "References"
        }, 
        {
            "location": "/algo/particle_swarm/", 
            "text": "Particle Swarm\n\n\n\n\nConstructor\n\n\nParticleSwarm(; lower = [],\n                upper = [],\n                n_particles = 0)\n\n\n\n\nThe constructor takes three keywords:\n\n\n\n\nlower = []\n, a vector of lower bounds, unbounded below if empty or \nInf\n's\n\n\nupper = []\n, a vector of upper bounds, unbounded above if empty or \nInf\n's\n\n\nn_particles = 0\n, number of particles in the swarm, defaults to least three\n\n\n\n\n\n\nDescription\n\n\nThe Particle Swarm implementation in Optim.jl is the so-called Adaptive Particle Swarm algorithm in [1]. It attempts to improve global coverage and convergence by switching between four evolutionary states: exploration, exploitation, convergence, and jumping out. In the jumping out state it intentially tries to take the best particle and move it away from its (potentially and probably) local optimum, to improve the ability to find a global optimum. Of course, this comes a the cost of slower convergence, but hopefully converges to the global optimum as a result.\n\n\n\n\nReferences\n\n\n[1] Zhan, Zhang, and Chung. Adaptive particle swarm optimization, IEEE Transactions on Systems, Man, and Cybernetics, Part B: CyberneticsVolume 39, Issue 6, 2009, Pages 1362-1381 (2009)", 
            "title": "Particle Swarm"
        }, 
        {
            "location": "/algo/particle_swarm/#particle-swarm", 
            "text": "", 
            "title": "Particle Swarm"
        }, 
        {
            "location": "/algo/particle_swarm/#constructor", 
            "text": "ParticleSwarm(; lower = [],\n                upper = [],\n                n_particles = 0)  The constructor takes three keywords:   lower = [] , a vector of lower bounds, unbounded below if empty or  Inf 's  upper = [] , a vector of upper bounds, unbounded above if empty or  Inf 's  n_particles = 0 , number of particles in the swarm, defaults to least three", 
            "title": "Constructor"
        }, 
        {
            "location": "/algo/particle_swarm/#description", 
            "text": "The Particle Swarm implementation in Optim.jl is the so-called Adaptive Particle Swarm algorithm in [1]. It attempts to improve global coverage and convergence by switching between four evolutionary states: exploration, exploitation, convergence, and jumping out. In the jumping out state it intentially tries to take the best particle and move it away from its (potentially and probably) local optimum, to improve the ability to find a global optimum. Of course, this comes a the cost of slower convergence, but hopefully converges to the global optimum as a result.", 
            "title": "Description"
        }, 
        {
            "location": "/algo/particle_swarm/#references", 
            "text": "[1] Zhan, Zhang, and Chung. Adaptive particle swarm optimization, IEEE Transactions on Systems, Man, and Cybernetics, Part B: CyberneticsVolume 39, Issue 6, 2009, Pages 1362-1381 (2009)", 
            "title": "References"
        }, 
        {
            "location": "/algo/cg/", 
            "text": "Conjugate Gradient Descent\n\n\n\n\nConstructor\n\n\nConjugateGradient(; alphaguess = LineSearches.InitialHagerZhang(),\n                    linesearch = LineSearches.HagerZhang(),\n                    eta = 0.4,\n                    P = nothing,\n                    precondprep = (P, x) -\n nothing)\n\n\n\n\n\n\nDescription\n\n\nThe \nConjugateGradient\n method implements Hager and Zhang (2006) and elements from Hager and Zhang (2013). Notice, that the default \nlinesearch\n is \nHagerZhang\n from LineSearches.jl. This line search is exactly the one proposed in Hager and Zhang (2006). The constant $eta$ is used in determining the next step direction, and the default here deviates from the one used in the original paper ($0.01$). It needs to be a strictly positive number.\n\n\n\n\nExample\n\n\nLet's optimize the 2D Rosenbrock function. The function and gradient are given by\n\n\nf(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\nfunction g!(storage, x)\n    storage[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n    storage[2] = 200.0 * (x[2] - x[1]^2)\nend\n\n\n\n\nwe can then try to optimize this function from \nx=[0.0, 0.0]\n\n\njulia\n optimize(f, g!, zeros(2), ConjugateGradient())\nResults of Optimization Algorithm\n * Algorithm: Conjugate Gradient\n * Starting Point: [0.0,0.0]\n * Minimizer: [1.000000002262018,1.0000000045408348]\n * Minimum: 5.144946e-18\n * Iterations: 21\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 2.09e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.55e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 3.36e-09\n   * stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 54\n * Gradient Calls: 39\n\n\n\n\nWe can compare this to the default first order solver in Optim.jl\n\n\n julia\n optimize(f, g!, zeros(2))\n\n Results of Optimization Algorithm\n  * Algorithm: L-BFGS\n  * Starting Point: [0.0,0.0]\n  * Minimizer: [0.9999999999373614,0.999999999868622]\n  * Minimum: 7.645684e-21\n  * Iterations: 16\n  * Convergence: true\n    * |x - x'| \u2264 0.0e+00: false\n      |x - x'| = 3.48e-07\n    * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n      |f(x) - f(x')| = 9.03e+06 |f(x)|\n    * |g(x)| \u2264 1.0e-08: true\n      |g(x)| = 2.32e-09\n    * stopped by an increasing objective: false\n    * Reached Maximum Number of Iterations: false\n  * Objective Calls: 53\n  * Gradient Calls: 53\n\n\n\n\nWe see that for this objective and starting point, \nConjugateGradient()\n requires fewer gradient evaluations to reach convergence.\n\n\n\n\nReferences\n\n\n\n\nW. W. Hager and H. Zhang (2006) Algorithm 851: CG_DESCENT, a conjugate gradient method with guaranteed descent. ACM Transactions on Mathematical Software 32: 113-137.\n\n\nW. W. Hager and H. Zhang (2013), The Limited Memory Conjugate Gradient Method. SIAM Journal on Optimization, 23, pp. 2150-2168.", 
            "title": "Conjugate Gradient"
        }, 
        {
            "location": "/algo/cg/#conjugate-gradient-descent", 
            "text": "", 
            "title": "Conjugate Gradient Descent"
        }, 
        {
            "location": "/algo/cg/#constructor", 
            "text": "ConjugateGradient(; alphaguess = LineSearches.InitialHagerZhang(),\n                    linesearch = LineSearches.HagerZhang(),\n                    eta = 0.4,\n                    P = nothing,\n                    precondprep = (P, x) -  nothing)", 
            "title": "Constructor"
        }, 
        {
            "location": "/algo/cg/#description", 
            "text": "The  ConjugateGradient  method implements Hager and Zhang (2006) and elements from Hager and Zhang (2013). Notice, that the default  linesearch  is  HagerZhang  from LineSearches.jl. This line search is exactly the one proposed in Hager and Zhang (2006). The constant $eta$ is used in determining the next step direction, and the default here deviates from the one used in the original paper ($0.01$). It needs to be a strictly positive number.", 
            "title": "Description"
        }, 
        {
            "location": "/algo/cg/#example", 
            "text": "Let's optimize the 2D Rosenbrock function. The function and gradient are given by  f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\nfunction g!(storage, x)\n    storage[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n    storage[2] = 200.0 * (x[2] - x[1]^2)\nend  we can then try to optimize this function from  x=[0.0, 0.0]  julia  optimize(f, g!, zeros(2), ConjugateGradient())\nResults of Optimization Algorithm\n * Algorithm: Conjugate Gradient\n * Starting Point: [0.0,0.0]\n * Minimizer: [1.000000002262018,1.0000000045408348]\n * Minimum: 5.144946e-18\n * Iterations: 21\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 2.09e-10\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.55e+00 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 3.36e-09\n   * stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 54\n * Gradient Calls: 39  We can compare this to the default first order solver in Optim.jl   julia  optimize(f, g!, zeros(2))\n\n Results of Optimization Algorithm\n  * Algorithm: L-BFGS\n  * Starting Point: [0.0,0.0]\n  * Minimizer: [0.9999999999373614,0.999999999868622]\n  * Minimum: 7.645684e-21\n  * Iterations: 16\n  * Convergence: true\n    * |x - x'| \u2264 0.0e+00: false\n      |x - x'| = 3.48e-07\n    * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n      |f(x) - f(x')| = 9.03e+06 |f(x)|\n    * |g(x)| \u2264 1.0e-08: true\n      |g(x)| = 2.32e-09\n    * stopped by an increasing objective: false\n    * Reached Maximum Number of Iterations: false\n  * Objective Calls: 53\n  * Gradient Calls: 53  We see that for this objective and starting point,  ConjugateGradient()  requires fewer gradient evaluations to reach convergence.", 
            "title": "Example"
        }, 
        {
            "location": "/algo/cg/#references", 
            "text": "W. W. Hager and H. Zhang (2006) Algorithm 851: CG_DESCENT, a conjugate gradient method with guaranteed descent. ACM Transactions on Mathematical Software 32: 113-137.  W. W. Hager and H. Zhang (2013), The Limited Memory Conjugate Gradient Method. SIAM Journal on Optimization, 23, pp. 2150-2168.", 
            "title": "References"
        }, 
        {
            "location": "/algo/gradientdescent/", 
            "text": "Gradient Descent\n\n\n\n\nConstructor\n\n\nGradientDescent(; alphaguess = LineSearches.InitialPrevious(),\n                  linesearch = LineSearches.HagerZhang(),\n                  P = nothing,\n                  precondprep = (P, x) -\n nothing)\n\n\n\n\n\n\nDescription\n\n\nGradient Descent a common name for a quasi-Newton solver. This means that it takes steps according to\n\n\n\n\n\nx_{n+1} = x_n - P^{-1}\\nabla f(x_n)\n\n\n\n\n\nwhere $P$ is a positive definite matrix. If $P$ is the Hessian, we get Newton's method. In Gradient Descent, $P$ is simply an appropriately dimensioned identity matrix, such that we go in the exact opposite direction of the gradient. This means that we do not use the curvature information from the Hessian, or an approximation of it. While it does seem quite logical to go in the opposite direction of the fastest increase in objective value, the procedure can be very slow if the problem is ill-conditioned. See the section on preconditioners for ways to remedy this when using Gradient Descent.\n\n\nAs with the other quasi-Newton solvers in this package, a scalar $\\alpha$ is introduced as follows\n\n\n\n\n\nx_{n+1} = x_n - \\alpha P^{-1}\\nabla f(x_n)\n\n\n\n\n\nand is chosen by a linesearch algorithm such that each step gives sufficient descent.\n\n\n\n\nExample\n\n\n\n\nReferences", 
            "title": "Gradient Descent"
        }, 
        {
            "location": "/algo/gradientdescent/#gradient-descent", 
            "text": "", 
            "title": "Gradient Descent"
        }, 
        {
            "location": "/algo/gradientdescent/#constructor", 
            "text": "GradientDescent(; alphaguess = LineSearches.InitialPrevious(),\n                  linesearch = LineSearches.HagerZhang(),\n                  P = nothing,\n                  precondprep = (P, x) -  nothing)", 
            "title": "Constructor"
        }, 
        {
            "location": "/algo/gradientdescent/#description", 
            "text": "Gradient Descent a common name for a quasi-Newton solver. This means that it takes steps according to   \nx_{n+1} = x_n - P^{-1}\\nabla f(x_n)   where $P$ is a positive definite matrix. If $P$ is the Hessian, we get Newton's method. In Gradient Descent, $P$ is simply an appropriately dimensioned identity matrix, such that we go in the exact opposite direction of the gradient. This means that we do not use the curvature information from the Hessian, or an approximation of it. While it does seem quite logical to go in the opposite direction of the fastest increase in objective value, the procedure can be very slow if the problem is ill-conditioned. See the section on preconditioners for ways to remedy this when using Gradient Descent.  As with the other quasi-Newton solvers in this package, a scalar $\\alpha$ is introduced as follows   \nx_{n+1} = x_n - \\alpha P^{-1}\\nabla f(x_n)   and is chosen by a linesearch algorithm such that each step gives sufficient descent.", 
            "title": "Description"
        }, 
        {
            "location": "/algo/gradientdescent/#example", 
            "text": "", 
            "title": "Example"
        }, 
        {
            "location": "/algo/gradientdescent/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/algo/lbfgs/", 
            "text": "(L-)BFGS\n\n\nThis page contains information about BFGS and its limited memory version L-BFGS.\n\n\n\n\nConstructors\n\n\nBFGS(; alphaguess = LineSearches.InitialStatic(),\n       linesearch = LineSearches.HagerZhang(),\n       initial_invH = nothing,\n       initial_stepnorm = nothing,\n       manifold = Flat())\n\n\n\n\ninitial_invH\n has a default value of \nnothing\n. If the user has a specific initial matrix they want to supply, it should be supplied as a function of an array similar to the initial point \nx0\n.\n\n\nIf \ninitial_stepnorm\n is set to a number \nz\n, the initial matrix will be the identity matrix scaled by \nz\n times the sup-norm of the gradient at the initial point \nx0\n.\n\n\nLBFGS(; m = 10,\n        alphaguess = LineSearches.InitialStatic(),\n        linesearch = LineSearches.HagerZhang(),\n        P = nothing,\n        precondprep = (P, x) -\n nothing,\n        manifold = Flat(),\n        scaleinvH0::Bool = true \n (typeof(P) \n: Nothing))\n\n\n\n\n\n\nDescription\n\n\nThis means that it takes steps according to\n\n\n\n\n\nx_{n+1} = x_n - P^{-1}\\nabla f(x_n)\n\n\n\n\n\nwhere $P$ is a positive definite matrix. If $P$ is the Hessian, we get Newton's method. In (L-)BFGS, the matrix is an approximation to the Hessian built using differences in the gradient across iterations. As long as the initial matrix is positive definite  it is possible to show that all the follow matrices will be as well. The starting matrix could simply be the identity matrix, such that the first step is identical to the Gradient Descent algorithm, or even the actual Hessian.\n\n\nThere are two versions of BFGS in the package: BFGS, and L-BFGS. The latter is different from the former because it doesn't use a complete history of the iterative procedure to construct $P$, but rather only the latest $m$ steps. It doesn't actually build the Hessian approximation matrix either, but computes the direction directly. This makes more suitable for large scale problems, as the memory requirement to store the relevant vectors will grow quickly in large problems.\n\n\nAs with the other quasi-Newton solvers in this package, a scalar $\\alpha$ is introduced as follows\n\n\n\n\n\nx_{n+1} = x_n - \\alpha P^{-1}\\nabla f(x_n)\n\n\n\n\n\nand is chosen by a linesearch algorithm such that each step gives sufficient descent.\n\n\n\n\nExample\n\n\n\n\nReferences\n\n\nWright, Stephen, and Jorge Nocedal (2006) \"Numerical optimization.\" Springer", 
            "title": "(L-)BFGS"
        }, 
        {
            "location": "/algo/lbfgs/#l-bfgs", 
            "text": "This page contains information about BFGS and its limited memory version L-BFGS.", 
            "title": "(L-)BFGS"
        }, 
        {
            "location": "/algo/lbfgs/#constructors", 
            "text": "BFGS(; alphaguess = LineSearches.InitialStatic(),\n       linesearch = LineSearches.HagerZhang(),\n       initial_invH = nothing,\n       initial_stepnorm = nothing,\n       manifold = Flat())  initial_invH  has a default value of  nothing . If the user has a specific initial matrix they want to supply, it should be supplied as a function of an array similar to the initial point  x0 .  If  initial_stepnorm  is set to a number  z , the initial matrix will be the identity matrix scaled by  z  times the sup-norm of the gradient at the initial point  x0 .  LBFGS(; m = 10,\n        alphaguess = LineSearches.InitialStatic(),\n        linesearch = LineSearches.HagerZhang(),\n        P = nothing,\n        precondprep = (P, x) -  nothing,\n        manifold = Flat(),\n        scaleinvH0::Bool = true   (typeof(P)  : Nothing))", 
            "title": "Constructors"
        }, 
        {
            "location": "/algo/lbfgs/#description", 
            "text": "This means that it takes steps according to   \nx_{n+1} = x_n - P^{-1}\\nabla f(x_n)   where $P$ is a positive definite matrix. If $P$ is the Hessian, we get Newton's method. In (L-)BFGS, the matrix is an approximation to the Hessian built using differences in the gradient across iterations. As long as the initial matrix is positive definite  it is possible to show that all the follow matrices will be as well. The starting matrix could simply be the identity matrix, such that the first step is identical to the Gradient Descent algorithm, or even the actual Hessian.  There are two versions of BFGS in the package: BFGS, and L-BFGS. The latter is different from the former because it doesn't use a complete history of the iterative procedure to construct $P$, but rather only the latest $m$ steps. It doesn't actually build the Hessian approximation matrix either, but computes the direction directly. This makes more suitable for large scale problems, as the memory requirement to store the relevant vectors will grow quickly in large problems.  As with the other quasi-Newton solvers in this package, a scalar $\\alpha$ is introduced as follows   \nx_{n+1} = x_n - \\alpha P^{-1}\\nabla f(x_n)   and is chosen by a linesearch algorithm such that each step gives sufficient descent.", 
            "title": "Description"
        }, 
        {
            "location": "/algo/lbfgs/#example", 
            "text": "", 
            "title": "Example"
        }, 
        {
            "location": "/algo/lbfgs/#references", 
            "text": "Wright, Stephen, and Jorge Nocedal (2006) \"Numerical optimization.\" Springer", 
            "title": "References"
        }, 
        {
            "location": "/algo/ngmres/", 
            "text": "Acceleration methods: N-GMRES and O-ACCEL\n\n\n\n\nConstructors\n\n\nNGMRES(;\n        alphaguess = LineSearches.InitialStatic(),\n        linesearch = LineSearches.HagerZhang(),\n        manifold = Flat(),\n        wmax::Int = 10,\n        \u03f50 = 1e-12,\n        nlprecon = GradientDescent(\n            alphaguess = LineSearches.InitialStatic(alpha=1e-4,scaled=true),\n            linesearch = LineSearches.Static(),\n            manifold = manifold),\n        nlpreconopts = Options(iterations = 1, allow_f_increases = true),\n      )\n\n\n\n\nOACCEL(;manifold::Manifold = Flat(),\n       alphaguess = LineSearches.InitialStatic(),\n       linesearch = LineSearches.HagerZhang(),\n       nlprecon = GradientDescent(\n           alphaguess = LineSearches.InitialStatic(alpha=1e-4,scaled=true),\n           linesearch = LineSearches.Static(),\n           manifold = manifold),\n       nlpreconopts = Options(iterations = 1, allow_f_increases = true),\n       \u03f50 = 1e-12,\n       wmax::Int = 10)\n\n\n\n\n\n\nDescription\n\n\nThese algorithms take a step given by the nonlinear preconditioner \nnlprecon\n and proposes an accelerated step on a subspace spanned by the previous \nwmax\n iterates.\n\n\n\n\nN-GMRES accelerates based on a minimization of an approximation to the $\\ell_2$ norm of the\n\n\n\n\ngradient.\n\n\n\n\nO-ACCEL accelerates based on a minimization of a n approximation to the objective.\n\n\n\n\nN-GMRES was originally developed for solving nonlinear systems [1], and reduces to GMRES for linear problems. Application of the algorithm to optimization is covered, for example, in [2]. A description of O-ACCEL and its connection to N-GMRES can be found in [3].\n\n\nWe recommend trying \nLBFGS\n on your problem before N-GMRES or O-ACCEL. All three algorithms have similar computational cost and memory requirements, however, L-BFGS is more efficient for many problems.\n\n\n\n\nExample\n\n\nThis example shows how to accelerate \nGradientDescent\n on the Extended Rosenbrock problem. First, we try to optimize using \nGradientDescent\n.\n\n\nusing Optim, OptimTestProblems\nUP = OptimTestProblems.UnconstrainedProblems\nprob = UP.examples[\nExtended Rosenbrock\n]\noptimize(UP.objective(prob), UP.gradient(prob), prob.initial_x, GradientDescent())\n\n\n\n\nThe algorithm does not converge within 1000 iterations.\n\n\nResults of Optimization Algorithm\n * Algorithm: Gradient Descent\n * Starting Point: [-1.2,1.0, ...]\n * Minimizer: [0.8923389282461412,0.7961268644300445, ...]\n * Minimum: 2.898230e-01\n * Iterations: 1000\n * Convergence: false\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 4.02e-04\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 2.38e-03 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 8.23e-02\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: true\n * Objective Calls: 2525\n * Gradient Calls: 2525\n\n\n\n\nNow, we use \nOACCEL\n to accelerate \nGradientDescent\n.\n\n\n# Default nonlinear procenditioner for `OACCEL`\nnlprecon = GradientDescent(alphaguess=LineSearches.InitialStatic(alpha=1e-4,scaled=true),\n                           linesearch=LineSearches.Static())\n# Default size of subspace that OACCEL accelerates over is `wmax = 10`\noacc10 = OACCEL(nlprecon=nlprecon, wmax=10)\noptimize(UP.objective(prob), UP.gradient(prob), prob.initial_x, oacc10)\n\n\n\n\nThis drastically improves the \nGradientDescent\n algorithm, converging in 87 iterations.\n\n\nResults of Optimization Algorithm\n * Algorithm: O-ACCEL preconditioned with Gradient Descent\n * Starting Point: [-1.2,1.0, ...]\n * Minimizer: [1.0000000011361219,1.0000000022828495, ...]\n * Minimum: 3.255053e-17\n * Iterations: 87\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 6.51e-08\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 7.56e+02 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 1.06e-09\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 285\n * Gradient Calls: 285\n\n\n\n\nWe can improve the acceleration further by changing the acceleration subspace size \nwmax\n.\n\n\noacc5 = OACCEL(nlprecon=nlprecon, wmax=5)\noptimize(UP.objective(prob), UP.gradient(prob), prob.initial_x, oacc5)\n\n\n\n\nNow, the O-ACCEL algorithm has accelerated \nGradientDescent\n to converge in 50 iterations.\n\n\nResults of Optimization Algorithm\n * Algorithm: O-ACCEL preconditioned with Gradient Descent\n * Starting Point: [-1.2,1.0, ...]\n * Minimizer: [0.9999999999392858,0.9999999998784691, ...]\n * Minimum: 9.218164e-20\n * Iterations: 50\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 2.76e-07\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 5.18e+06 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 4.02e-11\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 181\n * Gradient Calls: 181\n\n\n\n\nAs a final comparison, we can do the same with N-GMRES.\n\n\nngmres5 = NGMRES(nlprecon=nlprecon, wmax=5)\noptimize(UP.objective(prob), UP.gradient(prob), prob.initial_x, ngmres5)\n\n\n\n\nAgain, this significantly improves the \nGradientDescent\n algorithm, and converges in 63 iterations.\n\n\nResults of Optimization Algorithm\n * Algorithm: Nonlinear GMRES preconditioned with Gradient Descent\n * Starting Point: [-1.2,1.0, ...]\n * Minimizer: [0.9999999998534468,0.9999999997063993, ...]\n * Minimum: 5.375569e-19\n * Iterations: 63\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 9.94e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.29e+03 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 4.94e-11\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 222\n * Gradient Calls: 222\n\n\n\n\n\n\nReferences\n\n\n[1] De Sterck. Steepest descent preconditioning for nonlinear GMRES optimization. NLAA, 2013. [2] Washio and Oosterlee. Krylov subspace acceleration for nonlinear multigrid schemes. ETNA, 1997. [3] Riseth. Objective acceleration for unconstrained optimization. 2018.", 
            "title": "Acceleration"
        }, 
        {
            "location": "/algo/ngmres/#acceleration-methods-n-gmres-and-o-accel", 
            "text": "", 
            "title": "Acceleration methods: N-GMRES and O-ACCEL"
        }, 
        {
            "location": "/algo/ngmres/#constructors", 
            "text": "NGMRES(;\n        alphaguess = LineSearches.InitialStatic(),\n        linesearch = LineSearches.HagerZhang(),\n        manifold = Flat(),\n        wmax::Int = 10,\n        \u03f50 = 1e-12,\n        nlprecon = GradientDescent(\n            alphaguess = LineSearches.InitialStatic(alpha=1e-4,scaled=true),\n            linesearch = LineSearches.Static(),\n            manifold = manifold),\n        nlpreconopts = Options(iterations = 1, allow_f_increases = true),\n      )  OACCEL(;manifold::Manifold = Flat(),\n       alphaguess = LineSearches.InitialStatic(),\n       linesearch = LineSearches.HagerZhang(),\n       nlprecon = GradientDescent(\n           alphaguess = LineSearches.InitialStatic(alpha=1e-4,scaled=true),\n           linesearch = LineSearches.Static(),\n           manifold = manifold),\n       nlpreconopts = Options(iterations = 1, allow_f_increases = true),\n       \u03f50 = 1e-12,\n       wmax::Int = 10)", 
            "title": "Constructors"
        }, 
        {
            "location": "/algo/ngmres/#description", 
            "text": "These algorithms take a step given by the nonlinear preconditioner  nlprecon  and proposes an accelerated step on a subspace spanned by the previous  wmax  iterates.   N-GMRES accelerates based on a minimization of an approximation to the $\\ell_2$ norm of the   gradient.   O-ACCEL accelerates based on a minimization of a n approximation to the objective.   N-GMRES was originally developed for solving nonlinear systems [1], and reduces to GMRES for linear problems. Application of the algorithm to optimization is covered, for example, in [2]. A description of O-ACCEL and its connection to N-GMRES can be found in [3].  We recommend trying  LBFGS  on your problem before N-GMRES or O-ACCEL. All three algorithms have similar computational cost and memory requirements, however, L-BFGS is more efficient for many problems.", 
            "title": "Description"
        }, 
        {
            "location": "/algo/ngmres/#example", 
            "text": "This example shows how to accelerate  GradientDescent  on the Extended Rosenbrock problem. First, we try to optimize using  GradientDescent .  using Optim, OptimTestProblems\nUP = OptimTestProblems.UnconstrainedProblems\nprob = UP.examples[ Extended Rosenbrock ]\noptimize(UP.objective(prob), UP.gradient(prob), prob.initial_x, GradientDescent())  The algorithm does not converge within 1000 iterations.  Results of Optimization Algorithm\n * Algorithm: Gradient Descent\n * Starting Point: [-1.2,1.0, ...]\n * Minimizer: [0.8923389282461412,0.7961268644300445, ...]\n * Minimum: 2.898230e-01\n * Iterations: 1000\n * Convergence: false\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 4.02e-04\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 2.38e-03 |f(x)|\n   * |g(x)| \u2264 1.0e-08: false\n     |g(x)| = 8.23e-02\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: true\n * Objective Calls: 2525\n * Gradient Calls: 2525  Now, we use  OACCEL  to accelerate  GradientDescent .  # Default nonlinear procenditioner for `OACCEL`\nnlprecon = GradientDescent(alphaguess=LineSearches.InitialStatic(alpha=1e-4,scaled=true),\n                           linesearch=LineSearches.Static())\n# Default size of subspace that OACCEL accelerates over is `wmax = 10`\noacc10 = OACCEL(nlprecon=nlprecon, wmax=10)\noptimize(UP.objective(prob), UP.gradient(prob), prob.initial_x, oacc10)  This drastically improves the  GradientDescent  algorithm, converging in 87 iterations.  Results of Optimization Algorithm\n * Algorithm: O-ACCEL preconditioned with Gradient Descent\n * Starting Point: [-1.2,1.0, ...]\n * Minimizer: [1.0000000011361219,1.0000000022828495, ...]\n * Minimum: 3.255053e-17\n * Iterations: 87\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 6.51e-08\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 7.56e+02 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 1.06e-09\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 285\n * Gradient Calls: 285  We can improve the acceleration further by changing the acceleration subspace size  wmax .  oacc5 = OACCEL(nlprecon=nlprecon, wmax=5)\noptimize(UP.objective(prob), UP.gradient(prob), prob.initial_x, oacc5)  Now, the O-ACCEL algorithm has accelerated  GradientDescent  to converge in 50 iterations.  Results of Optimization Algorithm\n * Algorithm: O-ACCEL preconditioned with Gradient Descent\n * Starting Point: [-1.2,1.0, ...]\n * Minimizer: [0.9999999999392858,0.9999999998784691, ...]\n * Minimum: 9.218164e-20\n * Iterations: 50\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 2.76e-07\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 5.18e+06 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 4.02e-11\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 181\n * Gradient Calls: 181  As a final comparison, we can do the same with N-GMRES.  ngmres5 = NGMRES(nlprecon=nlprecon, wmax=5)\noptimize(UP.objective(prob), UP.gradient(prob), prob.initial_x, ngmres5)  Again, this significantly improves the  GradientDescent  algorithm, and converges in 63 iterations.  Results of Optimization Algorithm\n * Algorithm: Nonlinear GMRES preconditioned with Gradient Descent\n * Starting Point: [-1.2,1.0, ...]\n * Minimizer: [0.9999999998534468,0.9999999997063993, ...]\n * Minimum: 5.375569e-19\n * Iterations: 63\n * Convergence: true\n   * |x - x'| \u2264 0.0e+00: false\n     |x - x'| = 9.94e-09\n   * |f(x) - f(x')| \u2264 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.29e+03 |f(x)|\n   * |g(x)| \u2264 1.0e-08: true\n     |g(x)| = 4.94e-11\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 222\n * Gradient Calls: 222", 
            "title": "Example"
        }, 
        {
            "location": "/algo/ngmres/#references", 
            "text": "[1] De Sterck. Steepest descent preconditioning for nonlinear GMRES optimization. NLAA, 2013. [2] Washio and Oosterlee. Krylov subspace acceleration for nonlinear multigrid schemes. ETNA, 1997. [3] Riseth. Objective acceleration for unconstrained optimization. 2018.", 
            "title": "References"
        }, 
        {
            "location": "/algo/newton/", 
            "text": "Newton's Method\n\n\n\n\nConstructor\n\n\nNewton(; alphaguess = LineSearches.InitialStatic(),\n         linesearch = LineSearches.HagerZhang())\n\n\n\n\nThe constructor takes two keywords:\n\n\n\n\nlinesearch = a(d, x, p, x_new, g_new, phi0, dphi0, c)\n, a function performing line search, see the line search section.\n\n\nalphaguess = a(state, dphi0, d)\n, a function for setting the initial guess for the line search algorithm, see the line search section.\n\n\n\n\n\n\nDescription\n\n\nNewton's method for optimization has a long history, and is in some sense the gold standard in unconstrained optimization of smooth functions, at least from a theoretical viewpoint. The main benefit is that it has a quadratic rate of convergence near a local optimum. The main disadvantage is that the user has to provide a Hessian. This can be difficult, complicated, or simply annoying. It can also be computationally expensive to calculate it.\n\n\nNewton's method for optimization consists of applying Newton's method for solving systems of equations, where the equations are the first order conditions, saying that the gradient should equal the zero vector.\n\n\n\n\n\n\\nabla f(x) = 0\n\n\n\n\n\nA second order Taylor expansion of the left-hand side leads to the iterative scheme\n\n\n\n\n\nx_{n+1} = x_n - H(x_n)^{-1}\\nabla f(x_n)\n\n\n\n\n\nwhere the inverse is not calculated directly, but the step size is instead calculated by solving\n\n\n\n\n\nH(x) \\textbf{s} = \\nabla f(x_n).\n\n\n\n\n\nThis is equivalent to minimizing a quadratic model, $m_k$ around the current $x_n$\n\n\n\n\n\nm_k(s) = f(x_n) + \\nabla f(x_n)^\\top \\textbf{s} + \\frac{1}{2} \\textbf{s}^\\top H(x_n) \\textbf{s}\n\n\n\n\n\nFor functions where $H(x_n)$ is difficult, or computationally expensive to obtain, we might replace the Hessian with another positive definite matrix that approximates it. Such methods are called Quasi-Newton methods; see (L-)BFGS and Gradient Descent.\n\n\nIn a sufficiently small neighborhood around the minimizer, Newton's method has quadratic convergence, but globally it might have slower convergence, or it might even diverge. To ensure convergence, a line search is performed for each $\\textbf{s}$. This amounts to replacing the step formula above with\n\n\n\n\n\nx_{n+1} = x_n - \\alpha \\textbf{s}\n\n\n\n\n\nand finding a scalar $\\alpha$ such that we get sufficient descent; see the line search section for more information.\n\n\nAdditionally, if the function is locally concave, the step taken in the formulas above will go in a direction of ascent,  as the Hessian will not be positive (semi)definite. To avoid this, we use a specialized method to calculate the step direction. If the Hessian is positive semidefinite then the method used is standard, but if it is not, a correction is made using the functionality in \nPositiveFactorizations.jl\n.\n\n\n\n\nExample\n\n\nshow the example from the issue\n\n\n\n\nReferences", 
            "title": "Newton"
        }, 
        {
            "location": "/algo/newton/#newtons-method", 
            "text": "", 
            "title": "Newton's Method"
        }, 
        {
            "location": "/algo/newton/#constructor", 
            "text": "Newton(; alphaguess = LineSearches.InitialStatic(),\n         linesearch = LineSearches.HagerZhang())  The constructor takes two keywords:   linesearch = a(d, x, p, x_new, g_new, phi0, dphi0, c) , a function performing line search, see the line search section.  alphaguess = a(state, dphi0, d) , a function for setting the initial guess for the line search algorithm, see the line search section.", 
            "title": "Constructor"
        }, 
        {
            "location": "/algo/newton/#description", 
            "text": "Newton's method for optimization has a long history, and is in some sense the gold standard in unconstrained optimization of smooth functions, at least from a theoretical viewpoint. The main benefit is that it has a quadratic rate of convergence near a local optimum. The main disadvantage is that the user has to provide a Hessian. This can be difficult, complicated, or simply annoying. It can also be computationally expensive to calculate it.  Newton's method for optimization consists of applying Newton's method for solving systems of equations, where the equations are the first order conditions, saying that the gradient should equal the zero vector.   \n\\nabla f(x) = 0   A second order Taylor expansion of the left-hand side leads to the iterative scheme   \nx_{n+1} = x_n - H(x_n)^{-1}\\nabla f(x_n)   where the inverse is not calculated directly, but the step size is instead calculated by solving   \nH(x) \\textbf{s} = \\nabla f(x_n).   This is equivalent to minimizing a quadratic model, $m_k$ around the current $x_n$   \nm_k(s) = f(x_n) + \\nabla f(x_n)^\\top \\textbf{s} + \\frac{1}{2} \\textbf{s}^\\top H(x_n) \\textbf{s}   For functions where $H(x_n)$ is difficult, or computationally expensive to obtain, we might replace the Hessian with another positive definite matrix that approximates it. Such methods are called Quasi-Newton methods; see (L-)BFGS and Gradient Descent.  In a sufficiently small neighborhood around the minimizer, Newton's method has quadratic convergence, but globally it might have slower convergence, or it might even diverge. To ensure convergence, a line search is performed for each $\\textbf{s}$. This amounts to replacing the step formula above with   \nx_{n+1} = x_n - \\alpha \\textbf{s}   and finding a scalar $\\alpha$ such that we get sufficient descent; see the line search section for more information.  Additionally, if the function is locally concave, the step taken in the formulas above will go in a direction of ascent,  as the Hessian will not be positive (semi)definite. To avoid this, we use a specialized method to calculate the step direction. If the Hessian is positive semidefinite then the method used is standard, but if it is not, a correction is made using the functionality in  PositiveFactorizations.jl .", 
            "title": "Description"
        }, 
        {
            "location": "/algo/newton/#example", 
            "text": "show the example from the issue", 
            "title": "Example"
        }, 
        {
            "location": "/algo/newton/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/algo/newton_trust_region/", 
            "text": "Newton's Method With a Trust Region\n\n\n\n\nConstructor\n\n\nNewtonTrustRegion(; initial_delta = 1.0,\n                    delta_hat = 100.0,\n                    eta = 0.1,\n                    rho_lower = 0.25,\n                    rho_upper = 0.75)\n\n\n\n\nThe constructor takes keywords that determine the initial and maximal size of the trust region, when to grow and shrink the region, and how close the function should be to the quadratic approximation.  The notation follows chapter four of Numerical Optimization.  Below, \nrho\n $=\\rho$ refers to the ratio of the actual function change to the change in the quadratic approximation for a given step.\n\n\n\n\ninitial_delta:\nThe starting trust region radius\n\n\ndelta_hat:\n The largest allowable trust region radius\n\n\neta:\n When \nrho\n is at least \neta\n, accept the step.\n\n\nrho_lower:\n When \nrho\n is less than \nrho_lower\n, shrink the trust region.\n\n\nrho_upper:\n When \nrho\n is greater than \nrho_upper\n, grow the trust region (though no greater than \ndelta_hat\n).\n\n\n\n\n\n\nDescription\n\n\nNewton's method with a trust region is designed to take advantage of the second-order information in a function's Hessian, but with more stability than Newton's method when functions are not globally well-approximated by a quadratic.  This is achieved by repeatedly minimizing quadratic approximations within a dynamically-sized \"trust region\" in which the function is assumed to be locally quadratic [1].\n\n\nNewton's method optimizes a quadratic approximation to a function.  When a function is well approximated by a quadratic (for example, near an optimum), Newton's method converges very quickly by exploiting the second-order information in the Hessian matrix.  However, when the function is not well-approximated by a quadratic, either because the starting point is far from the optimum or the function has a more irregular shape, Newton steps can be erratically large, leading to distant, irrelevant areas of the space.\n\n\nTrust region methods use second-order information but restrict the steps to be within a \"trust region\" where the function is believed to be approximately quadratic.  At iteration $k$, a trust region method chooses a step $p$ to minimize a quadratic approximation to the objective such that the step size is no larger than a given trust region size, $\\Delta_k$.\n\n\n\n\n\n\\underset{p\\in\\mathbb{R}^n}\\min m_k(p) = f_k + g_k^T p + \\frac{1}{2}p^T B_k p \\quad\\textrm{such that } ||p||\\le \\Delta_k\n\n\n\n\n\nHere, $p$ is the step to take at iteration $k$, so that $x_{k+1} = x_k + p$.   In the definition of $m_k(p)$, $f_k = f(x_k)$ is the value at the previous location, $g_k=\\nabla f(x_k)$ is the gradient at the previous location, $B_k = \\nabla^2 f(x_k)$ is the Hessian matrix at the previous iterate, and $||\\cdot||$ is the Euclidian norm.\n\n\nIf the trust region size, $\\Delta_k$, is large enough that the minimizer of the quadratic approximation $m_k(p)$ has $||p|| \\le \\Delta_k$, then the step is the same as an ordinary Newton step.  However, if the unconstrained quadratic minimizer lies outside the trust region, then the minimizer to the constrained problem will occur on the boundary, i.e. we will have $||p|| = \\Delta_k$.  It turns out that when the Cholesky decomposition of $B_k$ can be computed, the optimal $p$ can be found numerically with relative ease.  ([1], section 4.3)  This is the method currently used in Optim.\n\n\nIt makes sense to adapt the trust region size, $\\Delta_k$, as one moves through the space and assesses the quality of the quadratic fit.  This adaptation is controlled by the parameters $\\eta$, $\\rho_{lower}$, and $\\rho_{upper}$, which are parameters to the \nNewtonTrustRegion\n optimization method.  For each step, we calculate\n\n\n\n\n\n\\rho_k := \\frac{f(x_{k+1}) - f(x_k)}{m_k(p) - m_k(0)}\n\n\n\n\n\nIntuitively, $\\rho_k$ measures the quality of the quadratic approximation: if $\\rho_k \\approx 1$, then our quadratic approximation is reasonable.  If  $p$ was on the boundary and $\\rho_k \n \\rho_{upper}$, then perhaps we can benefit from larger steps.  In this case, for the next iteration we grow the trust region geometrically up to a maximum of $\\hat\\Delta$:\n\n\n\n\n\n\\rho_k > \\rho_{upper} \\Rightarrow \\Delta_{k+1} = \\min(2 \\Delta_k, \\hat\\Delta).\n\n\n\n\n\nConversely, if $\\rho_k \n \\rho_{lower}$, then we shrink the trust region geometrically:\n\n\n$\\rho_k \n \\rho_{lower} \\Rightarrow \\Delta_{k+1} = 0.25 \\Delta_k$. Finally, we only accept a point if its decrease is appreciable compared to the quadratic approximation.  Specifically, a step is only accepted $\\rho_k \n \\eta$.  As long as we choose $\\eta$ to be less than $\\rho_{lower}$, we will shrink the trust region whenever we reject a step.  Eventually, if the objective function is locally quadratic, $\\Delta_k$ will become small enough that a quadratic approximation will be accurate enough to make progress again.\n\n\n\n\nExample\n\n\nusing Optim, OptimTestProblems\nprob = OptimTestProblems.UnconstrainedProblems.examples[\nRosenbrock\n];\nres = Optim.optimize(prob.f, prob.g!, prob.h!, prob.initial_x, NewtonTrustRegion())\n\n\n\n\n\n\nReferences\n\n\n[1] Nocedal, Jorge, and Stephen Wright. Numerical optimization. Springer Science \n Business Media, 2006.", 
            "title": "Newton with Trust Region"
        }, 
        {
            "location": "/algo/newton_trust_region/#newtons-method-with-a-trust-region", 
            "text": "", 
            "title": "Newton's Method With a Trust Region"
        }, 
        {
            "location": "/algo/newton_trust_region/#constructor", 
            "text": "NewtonTrustRegion(; initial_delta = 1.0,\n                    delta_hat = 100.0,\n                    eta = 0.1,\n                    rho_lower = 0.25,\n                    rho_upper = 0.75)  The constructor takes keywords that determine the initial and maximal size of the trust region, when to grow and shrink the region, and how close the function should be to the quadratic approximation.  The notation follows chapter four of Numerical Optimization.  Below,  rho  $=\\rho$ refers to the ratio of the actual function change to the change in the quadratic approximation for a given step.   initial_delta: The starting trust region radius  delta_hat:  The largest allowable trust region radius  eta:  When  rho  is at least  eta , accept the step.  rho_lower:  When  rho  is less than  rho_lower , shrink the trust region.  rho_upper:  When  rho  is greater than  rho_upper , grow the trust region (though no greater than  delta_hat ).", 
            "title": "Constructor"
        }, 
        {
            "location": "/algo/newton_trust_region/#description", 
            "text": "Newton's method with a trust region is designed to take advantage of the second-order information in a function's Hessian, but with more stability than Newton's method when functions are not globally well-approximated by a quadratic.  This is achieved by repeatedly minimizing quadratic approximations within a dynamically-sized \"trust region\" in which the function is assumed to be locally quadratic [1].  Newton's method optimizes a quadratic approximation to a function.  When a function is well approximated by a quadratic (for example, near an optimum), Newton's method converges very quickly by exploiting the second-order information in the Hessian matrix.  However, when the function is not well-approximated by a quadratic, either because the starting point is far from the optimum or the function has a more irregular shape, Newton steps can be erratically large, leading to distant, irrelevant areas of the space.  Trust region methods use second-order information but restrict the steps to be within a \"trust region\" where the function is believed to be approximately quadratic.  At iteration $k$, a trust region method chooses a step $p$ to minimize a quadratic approximation to the objective such that the step size is no larger than a given trust region size, $\\Delta_k$.   \n\\underset{p\\in\\mathbb{R}^n}\\min m_k(p) = f_k + g_k^T p + \\frac{1}{2}p^T B_k p \\quad\\textrm{such that } ||p||\\le \\Delta_k   Here, $p$ is the step to take at iteration $k$, so that $x_{k+1} = x_k + p$.   In the definition of $m_k(p)$, $f_k = f(x_k)$ is the value at the previous location, $g_k=\\nabla f(x_k)$ is the gradient at the previous location, $B_k = \\nabla^2 f(x_k)$ is the Hessian matrix at the previous iterate, and $||\\cdot||$ is the Euclidian norm.  If the trust region size, $\\Delta_k$, is large enough that the minimizer of the quadratic approximation $m_k(p)$ has $||p|| \\le \\Delta_k$, then the step is the same as an ordinary Newton step.  However, if the unconstrained quadratic minimizer lies outside the trust region, then the minimizer to the constrained problem will occur on the boundary, i.e. we will have $||p|| = \\Delta_k$.  It turns out that when the Cholesky decomposition of $B_k$ can be computed, the optimal $p$ can be found numerically with relative ease.  ([1], section 4.3)  This is the method currently used in Optim.  It makes sense to adapt the trust region size, $\\Delta_k$, as one moves through the space and assesses the quality of the quadratic fit.  This adaptation is controlled by the parameters $\\eta$, $\\rho_{lower}$, and $\\rho_{upper}$, which are parameters to the  NewtonTrustRegion  optimization method.  For each step, we calculate   \n\\rho_k := \\frac{f(x_{k+1}) - f(x_k)}{m_k(p) - m_k(0)}   Intuitively, $\\rho_k$ measures the quality of the quadratic approximation: if $\\rho_k \\approx 1$, then our quadratic approximation is reasonable.  If  $p$ was on the boundary and $\\rho_k   \\rho_{upper}$, then perhaps we can benefit from larger steps.  In this case, for the next iteration we grow the trust region geometrically up to a maximum of $\\hat\\Delta$:   \n\\rho_k > \\rho_{upper} \\Rightarrow \\Delta_{k+1} = \\min(2 \\Delta_k, \\hat\\Delta).   Conversely, if $\\rho_k   \\rho_{lower}$, then we shrink the trust region geometrically:  $\\rho_k   \\rho_{lower} \\Rightarrow \\Delta_{k+1} = 0.25 \\Delta_k$. Finally, we only accept a point if its decrease is appreciable compared to the quadratic approximation.  Specifically, a step is only accepted $\\rho_k   \\eta$.  As long as we choose $\\eta$ to be less than $\\rho_{lower}$, we will shrink the trust region whenever we reject a step.  Eventually, if the objective function is locally quadratic, $\\Delta_k$ will become small enough that a quadratic approximation will be accurate enough to make progress again.", 
            "title": "Description"
        }, 
        {
            "location": "/algo/newton_trust_region/#example", 
            "text": "using Optim, OptimTestProblems\nprob = OptimTestProblems.UnconstrainedProblems.examples[ Rosenbrock ];\nres = Optim.optimize(prob.f, prob.g!, prob.h!, prob.initial_x, NewtonTrustRegion())", 
            "title": "Example"
        }, 
        {
            "location": "/algo/newton_trust_region/#references", 
            "text": "[1] Nocedal, Jorge, and Stephen Wright. Numerical optimization. Springer Science   Business Media, 2006.", 
            "title": "References"
        }, 
        {
            "location": "/algo/ipnewton/", 
            "text": "Interior point Newton method\n\n\n#\n\n\nOptim.IPNewton\n \n \nType\n.\n\n\nInterior-point Newton\n\n\nConstructor\n\n\nIPNewton(; linesearch::Function = Optim.backtrack_constrained_grad,\n         \u03bc0::Union{Symbol,Number} = :auto,\n         show_linesearch::Bool = false)\n\n\n\n\nThe initial barrier penalty coefficient \n\u03bc0\n can be chosen as a number, or set to \n:auto\n to let the algorithm decide its value, see \ninitialize_\u03bc_\u03bb!\n.\n\n\nNote\n: For constrained optimization problems, we recommend always enabling \nallow_f_increases\n and \nsuccessive_f_tol\n in the options passed to \noptimize\n. The default is set to \nOptim.Options(allow_f_increases = true, successive_f_tol = 2)\n.\n\n\nAs of February 2018, the line search algorithm is specialised for constrained interior-point methods. In future we hope to support more algorithms from \nLineSearches.jl\n.\n\n\nDescription\n\n\nThe \nIPNewton\n method implements an interior-point primal-dual Newton algorithm for solving nonlinear, constrained optimization problems. See Nocedal and Wright (Ch. 19, 2006) for a discussion of interior-point methods for constrained optimization.\n\n\nReferences\n\n\nThe algorithm was \noriginally written by Tim Holy\n (@timholy, tim.holy@gmail.com).\n\n\n\n\nJ Nocedal, SJ Wright (2006), Numerical optimization, second edition. Springer.\n\n\nA W\u00e4chter, LT Biegler (2006), On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Mathematical Programming 106 (1), 25-57.\n\n\n\n\n\n\nExamples\n\n\n\n\nNonlinear constrained optimization in Optim", 
            "title": "Interior point Newton"
        }, 
        {
            "location": "/algo/ipnewton/#interior-point-newton-method", 
            "text": "#  Optim.IPNewton     Type .  Interior-point Newton  Constructor  IPNewton(; linesearch::Function = Optim.backtrack_constrained_grad,\n         \u03bc0::Union{Symbol,Number} = :auto,\n         show_linesearch::Bool = false)  The initial barrier penalty coefficient  \u03bc0  can be chosen as a number, or set to  :auto  to let the algorithm decide its value, see  initialize_\u03bc_\u03bb! .  Note : For constrained optimization problems, we recommend always enabling  allow_f_increases  and  successive_f_tol  in the options passed to  optimize . The default is set to  Optim.Options(allow_f_increases = true, successive_f_tol = 2) .  As of February 2018, the line search algorithm is specialised for constrained interior-point methods. In future we hope to support more algorithms from  LineSearches.jl .  Description  The  IPNewton  method implements an interior-point primal-dual Newton algorithm for solving nonlinear, constrained optimization problems. See Nocedal and Wright (Ch. 19, 2006) for a discussion of interior-point methods for constrained optimization.  References  The algorithm was  originally written by Tim Holy  (@timholy, tim.holy@gmail.com).   J Nocedal, SJ Wright (2006), Numerical optimization, second edition. Springer.  A W\u00e4chter, LT Biegler (2006), On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Mathematical Programming 106 (1), 25-57.", 
            "title": "Interior point Newton method"
        }, 
        {
            "location": "/algo/ipnewton/#examples", 
            "text": "Nonlinear constrained optimization in Optim", 
            "title": "Examples"
        }, 
        {
            "location": "/dev/contributing/", 
            "text": "Notes for contributing\n\n\nWe are always happy to get help from people who normally do not contribute to the package. However, to make the process run smoothly, we ask you to read this page before creating your pull request. That way it is more probable that your changes will be incorporated, and in the end it will mean less work for everyone.\n\n\n\n\nThings to consider\n\n\nWhen proposing a change to \nOptim.jl\n, there are a few things to consider. If you're in doubt feel free to reach out. A simple way to get in touch, is to join our \ngitter channel\n.\n\n\nBefore submitting a pull request, please consider the following bullets:\n\n\n\n\nDid you remember to provide tests for your changes? If not, please do so, or ask for help.\n\n\nDid your change add new functionality? Remember to add a section in the documentation.\n\n\nDid you change existing code in a breaking way? Then remember to use Julia's deprecation tools to help users migrate to the new syntax.\n\n\nAdd a note in the NEWS.md file, so we can keep track of changes between versions.\n\n\n\n\n\n\nAdding a solver\n\n\nIf you're contributing a new solver, you shouldn't need to touch any of the code in \nsrc/optimize.jl\n. You should rather add a file named (\nsolver\n is the name of the solver) \nsolver.jl\n in \nsrc\n, and make sure that you define an \nOptimizer\n subtype \nstruct Solver \n: Optimizer end\n with appropriate fields, a default constructor with a keyword for each field, a state type that holds all variables that are (re)used throughout the iterative procedure, an \ninitial_state\n that initializes such a state, and  an \nupdate!\n method that does the actual work. Say you want to contribute a solver called \nMinim\n, then your \nsrc/minim.jl\n file would look something like\n\n\nstruct Minim{IF, F\n:Function, T} \n: Optimizer\n    alphaguess!::IF\n    linesearch!::F\n    minim_parameter::T\nend\n\nMinim(; alphaguess = LineSearches.InitialStatic(), linesearch = LineSearches.HagerZhang(), minim_parameter = 1.0) =\n  Minim(linesearch, minim_parameter)\n\ntype MinimState{T,N,G}\n  x::AbstractArray{T,N}\n  x_previous::AbstractArray{T,N}\n  f_x_previous::T\n  s::AbstractArray{T,N}\n  @add_linesearch_fields()\nend\n\nfunction initial_state(method::Minim, options, d, initial_x)\n# prepare cache variables etc here\n\nend\n\nfunction update!{T}(d, state::MinimState{T}, method::Minim)\n    # code for Minim here\n    false # should the procedure force quit?\nend", 
            "title": "Contributing"
        }, 
        {
            "location": "/dev/contributing/#notes-for-contributing", 
            "text": "We are always happy to get help from people who normally do not contribute to the package. However, to make the process run smoothly, we ask you to read this page before creating your pull request. That way it is more probable that your changes will be incorporated, and in the end it will mean less work for everyone.", 
            "title": "Notes for contributing"
        }, 
        {
            "location": "/dev/contributing/#things-to-consider", 
            "text": "When proposing a change to  Optim.jl , there are a few things to consider. If you're in doubt feel free to reach out. A simple way to get in touch, is to join our  gitter channel .  Before submitting a pull request, please consider the following bullets:   Did you remember to provide tests for your changes? If not, please do so, or ask for help.  Did your change add new functionality? Remember to add a section in the documentation.  Did you change existing code in a breaking way? Then remember to use Julia's deprecation tools to help users migrate to the new syntax.  Add a note in the NEWS.md file, so we can keep track of changes between versions.", 
            "title": "Things to consider"
        }, 
        {
            "location": "/dev/contributing/#adding-a-solver", 
            "text": "If you're contributing a new solver, you shouldn't need to touch any of the code in  src/optimize.jl . You should rather add a file named ( solver  is the name of the solver)  solver.jl  in  src , and make sure that you define an  Optimizer  subtype  struct Solver  : Optimizer end  with appropriate fields, a default constructor with a keyword for each field, a state type that holds all variables that are (re)used throughout the iterative procedure, an  initial_state  that initializes such a state, and  an  update!  method that does the actual work. Say you want to contribute a solver called  Minim , then your  src/minim.jl  file would look something like  struct Minim{IF, F :Function, T}  : Optimizer\n    alphaguess!::IF\n    linesearch!::F\n    minim_parameter::T\nend\n\nMinim(; alphaguess = LineSearches.InitialStatic(), linesearch = LineSearches.HagerZhang(), minim_parameter = 1.0) =\n  Minim(linesearch, minim_parameter)\n\ntype MinimState{T,N,G}\n  x::AbstractArray{T,N}\n  x_previous::AbstractArray{T,N}\n  f_x_previous::T\n  s::AbstractArray{T,N}\n  @add_linesearch_fields()\nend\n\nfunction initial_state(method::Minim, options, d, initial_x)\n# prepare cache variables etc here\n\nend\n\nfunction update!{T}(d, state::MinimState{T}, method::Minim)\n    # code for Minim here\n    false # should the procedure force quit?\nend", 
            "title": "Adding a solver"
        }, 
        {
            "location": "/LICENSE/", 
            "text": "Optim.jl is licensed under the MIT License:\n\n\n\n\nCopyright (c) 2012: John Myles White, Tim Holy, and other contributors. Copyright (c) 2016: Patrick Kofod Mogensen, John Myles White, Tim Holy,                     and other contributors. Copyright (c) 2017: Patrick Kofod Mogensen, Asbj\u00f8rn Nilsen Riseth,                     John Myles White, Tim Holy, and other contributors.\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", 
            "title": "License"
        }
    ]
}