<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="JuliaNLSolvers">
    
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Gradients and Hessians - Optim.jl</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <link href="../../assets/Documenter.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Gradients and Hessians", url: "#_top", children: [
          ]},
          {title: "Analytic", url: "#analytic", children: [
          ]},
          {title: "Finite differences", url: "#finite-differences", children: [
          ]},
          {title: "Automatic differentiation", url: "#automatic-differentiation", children: [
          ]},
          {title: "Example", url: "#example", children: [
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="../../assets/mathjaxhelper.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../config/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../config/" class="btn btn-xs btn-link">
        Configurable Options
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../minimization/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../minimization/" class="btn btn-xs btn-link">
        Minimizing a function
      </a>
    </div>
    
  </div>

    

    <p><a id='Gradients-and-Hessians-1'></a></p>
<h2 id="gradients-and-hessians">Gradients and Hessians</h2>
<p>To use first- and second-order methods, you need to provide gradients and Hessians, either in-place or out-of-place. There are three main ways of specifying derivatives: analytic, finite-difference and automatic differentiation.</p>
<p><a id='Analytic-1'></a></p>
<h2 id="analytic">Analytic</h2>
<p>This results in the fastest run times, but requires the user to perform the often tedious task of computing the derivatives by hand. The gradient of complicated objective functions (e.g. involving the solution of algebraic equations, differential equations, eigendecompositions, etc.) can be computed efficiently using the adjoint method (see e.g. <a href="https://math.mit.edu/~stevenj/18.336/adjoint.pdf">these lecture notes</a>). In particular, assuming infinite memory, the gradient of a $\mathbb{R}^N \to \mathbb{R}$ function $f$ can always be computed with a runtime comparable with only one evaluation of $f$, no matter how large $N$.</p>
<p>To use analytic derivatives, simply pass <code>g!</code> and <code>h!</code> functions to <code>optimize</code>.</p>
<p><a id='Finite-differences-1'></a></p>
<h2 id="finite-differences">Finite differences</h2>
<p>This uses the functionality in <a href="https://github.com/JuliaDiffEq/DiffEqDiffTools.jl">DiffEqDiffTools.jl</a> to compute gradients and Hessians through central finite differences: $f'(x) \approx \frac{f(x+h)-f(x-h)}{2h}$. For a $\mathbb{R}^N \to \mathbb{R}$ objective function $f$, this requires $2N$ evaluations of $f$. It is therefore efficient in low dimensions but slow when $N$ is large. It is also inaccurate: $h$ is chosen equal to $\epsilon^{1/3}$ where $\epsilon$ is the machine epsilon (about $10^{-16}$ for <code>Float64</code>) to balance the truncation and rounding errors, resulting in an error of $\epsilon^{2/3}$ (about $10^{-11}$ for <code>Float64</code>) for the derivative.</p>
<p>Finite differences are on by default if gradients and Hessians are not supplied to the <code>optimize</code> call.</p>
<p><a id='Automatic-differentiation-1'></a></p>
<h2 id="automatic-differentiation">Automatic differentiation</h2>
<p>Automatic differentiation techniques are a middle ground between finite differences and analytic computations. They are exact up to machine precision, and do not require intervention from the user. They come in two main flavors: <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">forward and reverse mode</a>. Forward-mode automatic differentiation is relatively straightforward to implement by propagating the sensitivities of the input variables, and is often faster than finite differences. The disadvantage is that the objective function has to be written using only Julia code. Forward-mode automatic differentiation still requires a runtime comparable to $N$ evaluations of $f$, and is therefore costly in large dimensions, like finite differences.</p>
<p>Reverse-mode automatic differentiation can be seen as an automatic implementation of the adjoint method mentioned above, and requires a runtime comparable to only one evaluation of $f$. It is however considerably more complex to implement, requiring to record the execution of the program to then run it backwards, and incurs a larger overhead.</p>
<p>Forward-mode automatic differentiation is supported through the <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> package by providing the <code>autodiff=:forward</code> keyword to <code>optimize</code>. Reverse-mode automatic differentiation is not supported explicitly yet (although you can use it by writing your own <code>g!</code> function). There are a number of implementations in Julia, such as <a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff.jl</a>.</p>
<p><a id='Example-1'></a></p>
<h2 id="example">Example</h2>
<p>Let us consider the Rosenbrock example again.</p>
<pre><code class="julia">function f(x)
    return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2
end

function g!(G, x)
    G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]
    G[2] = 200.0 * (x[2] - x[1]^2)
end

function h!(H, x)
    H[1, 1] = 2.0 - 400.0 * x[2] + 1200.0 * x[1]^2
    H[1, 2] = -400.0 * x[1]
    H[2, 1] = -400.0 * x[1]
    H[2, 2] = 200.0
end

initial_x = zeros(2)
</code></pre>

<p>Let us see if BFGS and Newton's Method can solve this problem with the functions provided.</p>
<pre><code class="jlcon">julia&gt; Optim.minimizer(optimize(f, g!, h!, initial_x, BFGS()))
2-element Array{Float64,1}:
 1.0
 1.0

julia&gt; Optim.minimizer(optimize(f, g!, h!, initial_x, Newton()))

2-element Array{Float64,1}:
 1.0
 1.0
</code></pre>

<p>This is indeed the case. Now let us use finite differences for BFGS.</p>
<pre><code class="jlcon">julia&gt; Optim.minimizer(optimize(f, initial_x, BFGS()))
2-element Array{Float64,1}:
 1.0
 1.0
</code></pre>

<p>Still looks good. Returning to automatic differentiation, let us try both solvers using this method.  We enable <a href="https://github.com/JuliaDiff/ForwardDiff.jl">forward mode</a> automatic differentiation by using the <code>autodiff = :forward</code> keyword.</p>
<pre><code class="jlcon">julia&gt; Optim.minimizer(optimize(f, initial_x, BFGS(); autodiff = :forward))
2-element Array{Float64,1}:
 1.0
 1.0

julia&gt; Optim.minimizer(optimize(f, initial_x, Newton(); autodiff = :forward))
2-element Array{Float64,1}:
 1.0
 1.0
</code></pre>

<p>Indeed, the minimizer was found, without providing any gradients or Hessians.</p>

  <br>
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../config/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../config/" class="btn btn-xs btn-link">
        Configurable Options
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../minimization/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../minimization/" class="btn btn-xs btn-link">
        Minimizing a function
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
      <p>
        <a href="https://github.com/JuliaNLSolvers/Optim.jl/edit/master/docs/user/gradientsandhessians.md"><i class="fa fa-github"></i>
Edit on GitHub</a>
      </p>
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>