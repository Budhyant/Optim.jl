<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="JuliaNLSolvers">
    
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Acceleration - Optim.jl</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Acceleration methods: N-GMRES and O-ACCEL", url: "#acceleration-methods-n-gmres-and-o-accel", children: [
              {title: "Constructors", url: "#constructors" },
              {title: "Description", url: "#description" },
              {title: "Example", url: "#example" },
              {title: "References", url: "#references" },
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="../../assets/mathjaxhelper.js"></script>
      <script src="../../search/require.js"></script>
      <script src="../../search/search.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../newton/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../newton/" class="btn btn-xs btn-link">
        Newton
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../lbfgs/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../lbfgs/" class="btn btn-xs btn-link">
        (L-)BFGS
      </a>
    </div>
    
  </div>

    

    <p><a id='Acceleration-methods:-N-GMRES-and-O-ACCEL-1'></a></p>
<h1 id="acceleration-methods-n-gmres-and-o-accel">Acceleration methods: N-GMRES and O-ACCEL</h1>
<p><a id='Constructors-1'></a></p>
<h2 id="constructors">Constructors</h2>
<div class="codehilite"><pre><span></span><span class="n">NGMRES</span><span class="p">(;</span>
        <span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialStatic</span><span class="p">(),</span>
        <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">HagerZhang</span><span class="p">(),</span>
        <span class="n">manifold</span> <span class="o">=</span> <span class="n">Flat</span><span class="p">(),</span>
        <span class="n">wmax</span><span class="o">::</span><span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">ϵ0</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">,</span>
        <span class="n">nlprecon</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span>
            <span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialPrevious</span><span class="p">(),</span>
            <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">Static</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span><span class="n">scaled</span><span class="o">=</span><span class="kc">true</span><span class="p">),</span>
            <span class="n">manifold</span> <span class="o">=</span> <span class="n">manifold</span><span class="p">),</span>
        <span class="n">nlpreconopts</span> <span class="o">=</span> <span class="n">Options</span><span class="p">(</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">allow_f_increases</span> <span class="o">=</span> <span class="kc">true</span><span class="p">),</span>
      <span class="p">)</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">OACCEL</span><span class="p">(;</span><span class="n">manifold</span><span class="o">::</span><span class="n">Manifold</span> <span class="o">=</span> <span class="n">Flat</span><span class="p">(),</span>
       <span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialStatic</span><span class="p">(),</span>
       <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">HagerZhang</span><span class="p">(),</span>
       <span class="n">nlprecon</span> <span class="o">=</span> <span class="n">GradientDescent</span><span class="p">(</span>
           <span class="n">alphaguess</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">InitialPrevious</span><span class="p">(),</span>
           <span class="n">linesearch</span> <span class="o">=</span> <span class="n">LineSearches</span><span class="o">.</span><span class="n">Static</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span><span class="n">scaled</span><span class="o">=</span><span class="kc">true</span><span class="p">),</span>
           <span class="n">manifold</span> <span class="o">=</span> <span class="n">manifold</span><span class="p">),</span>
       <span class="n">nlpreconopts</span> <span class="o">=</span> <span class="n">Options</span><span class="p">(</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">allow_f_increases</span> <span class="o">=</span> <span class="kc">true</span><span class="p">),</span>
       <span class="n">ϵ0</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">,</span>
       <span class="n">wmax</span><span class="o">::</span><span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>


<p><a id='Description-1'></a></p>
<h2 id="description">Description</h2>
<p>These algorithms take a step given by the nonlinear preconditioner <code>nlprecon</code> and proposes an accelerated step on a subspace spanned by the previous <code>wmax</code> iterates.</p>
<ul>
<li>N-GMRES accelerates based on a minimization of an approximation to the $\ell_2$ norm of the</li>
</ul>
<p>gradient.</p>
<ul>
<li>O-ACCEL accelerates based on a minimization of a n approximation to the objective.</li>
</ul>
<p>N-GMRES was originally developed for solving nonlinear systems [1], and reduces to GMRES for linear problems. Application of the algorithm to optimization is covered, for example, in [2]. A description of O-ACCEL and its connection to N-GMRES can be found in [3].</p>
<p><em>We recommend trying <a href="../lbfgs/">LBFGS</a> on your problem before N-GMRES or O-ACCEL. All three algorithms have similar computational cost and memory requirements, however, L-BFGS is more efficient for many problems.</em></p>
<p><a id='Example-1'></a></p>
<h2 id="example">Example</h2>
<p>This example shows how to accelerate <code>GradientDescent</code> on the Extended Rosenbrock problem. First, we try to optimize using <code>GradientDescent</code>.</p>
<div class="codehilite"><pre><span></span><span class="k">using</span> <span class="n">Optim</span><span class="p">,</span> <span class="n">OptimTestProblems</span>
<span class="n">UP</span> <span class="o">=</span> <span class="n">OptimTestProblems</span><span class="o">.</span><span class="n">UnconstrainedProblems</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">UP</span><span class="o">.</span><span class="n">examples</span><span class="p">[</span><span class="s">&quot;Extended Rosenbrock&quot;</span><span class="p">]</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">UP</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">UP</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">prob</span><span class="o">.</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">GradientDescent</span><span class="p">())</span>
</pre></div>


<p>The algorithm does not converge within 1000 iterations.</p>
<div class="codehilite"><pre><span></span>Results of Optimization Algorithm
 * Algorithm: Gradient Descent
 * Starting Point: [-1.2,1.0, ...]
 * Minimizer: [0.8923389282461412,0.7961268644300445, ...]
 * Minimum: 2.898230e-01
 * Iterations: 1000
 * Convergence: false
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 4.02e-04 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = 2.38e-03 |f(x)|
   * |g(x)| ≤ 1.0e-08: false 
     |g(x)| = 8.23e-02 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: true
 * Objective Calls: 2525
 * Gradient Calls: 2525
</pre></div>


<p>Now, we use <code>OACCEL</code> to accelerate <code>GradientDescent</code>.</p>
<div class="codehilite"><pre><span></span><span class="c"># Default nonlinear procenditioner for `OACCEL`</span>
<span class="n">nlprecon</span> <span class="o">=</span>  <span class="n">GradientDescent</span><span class="p">(</span><span class="n">linesearch</span><span class="o">=</span><span class="n">LineSearches</span><span class="o">.</span><span class="n">Static</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span><span class="n">scaled</span><span class="o">=</span><span class="kc">true</span><span class="p">))</span>
<span class="c"># Default size of subspace that OACCEL accelerates over is `wmax = 10`</span>
<span class="n">oacc10</span> <span class="o">=</span> <span class="n">OACCEL</span><span class="p">(</span><span class="n">nlprecon</span><span class="o">=</span><span class="n">nlprecon</span><span class="p">,</span> <span class="n">wmax</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">UP</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">UP</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">prob</span><span class="o">.</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">oacc10</span><span class="p">)</span>
</pre></div>


<p>This drastically improves the <code>GradientDescent</code> algorithm, converging in 87 iterations.</p>
<div class="codehilite"><pre><span></span>Results of Optimization Algorithm
 * Algorithm: O-ACCEL preconditioned with Gradient Descent
 * Starting Point: [-1.2,1.0, ...]
 * Minimizer: [1.0000000011361219,1.0000000022828495, ...]
 * Minimum: 3.255053e-17
 * Iterations: 87
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 6.51e-08 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = 7.56e+02 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 1.06e-09 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 285
 * Gradient Calls: 285
</pre></div>


<p>We can improve the acceleration further by changing the acceleration subspace size <code>wmax</code>.</p>
<div class="codehilite"><pre><span></span><span class="n">oacc5</span> <span class="o">=</span> <span class="n">OACCEL</span><span class="p">(</span><span class="n">nlprecon</span><span class="o">=</span><span class="n">nlprecon</span><span class="p">,</span> <span class="n">wmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">UP</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">UP</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">prob</span><span class="o">.</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">oacc5</span><span class="p">)</span>
</pre></div>


<p>Now, the O-ACCEL algorithm has accelerated <code>GradientDescent</code> to converge in 50 iterations.</p>
<div class="codehilite"><pre><span></span>Results of Optimization Algorithm
 * Algorithm: O-ACCEL preconditioned with Gradient Descent
 * Starting Point: [-1.2,1.0, ...]
 * Minimizer: [0.9999999999392858,0.9999999998784691, ...]
 * Minimum: 9.218164e-20
 * Iterations: 50
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 2.76e-07 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = 5.18e+06 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 4.02e-11 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 181
 * Gradient Calls: 181
</pre></div>


<p>As a final comparison, we can do the same with N-GMRES.</p>
<div class="codehilite"><pre><span></span><span class="n">ngmres5</span> <span class="o">=</span> <span class="n">NGMRES</span><span class="p">(</span><span class="n">nlprecon</span><span class="o">=</span><span class="n">nlprecon</span><span class="p">,</span> <span class="n">wmax</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">optimize</span><span class="p">(</span><span class="n">UP</span><span class="o">.</span><span class="n">objective</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">UP</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">prob</span><span class="p">),</span> <span class="n">prob</span><span class="o">.</span><span class="n">initial_x</span><span class="p">,</span> <span class="n">ngmres5</span><span class="p">)</span>
</pre></div>


<p>Again, this significantly improves the <code>GradientDescent</code> algorithm, and converges in 63 iterations.</p>
<div class="codehilite"><pre><span></span>Results of Optimization Algorithm
 * Algorithm: Nonlinear GMRES preconditioned with Gradient Descent
 * Starting Point: [-1.2,1.0, ...]
 * Minimizer: [0.9999999998534468,0.9999999997063993, ...]
 * Minimum: 5.375569e-19
 * Iterations: 63
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false 
     |x - x&#39;| = 9.94e-09 
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = 1.29e+03 |f(x)|
   * |g(x)| ≤ 1.0e-08: true 
     |g(x)| = 4.94e-11 
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 222
 * Gradient Calls: 222
</pre></div>


<p><a id='References-1'></a></p>
<h2 id="references">References</h2>
<p>[1] De Sterck. Steepest descent preconditioning for nonlinear GMRES optimization. NLAA, 2013. [2] Washio and Oosterlee. Krylov subspace acceleration for nonlinear multigrid schemes. ETNA, 1997. [3] Riseth. Objective acceleration for unconstrained optimization. 2018.</p>

  <br>
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../newton/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../newton/" class="btn btn-xs btn-link">
        Newton
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../lbfgs/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../lbfgs/" class="btn btn-xs btn-link">
        (L-)BFGS
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>